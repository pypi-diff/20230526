# Comparing `tmp/uwsift-1.2.3.tar.gz` & `tmp/uwsift-2.0.0b0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "uwsift-1.2.3.tar", last modified: Fri Feb  4 17:00:00 2022, max compression
+gzip compressed data, was "uwsift-2.0.0b0.tar", last modified: Fri May 26 01:29:53 2023, max compression
```

## Comparing `uwsift-1.2.3.tar` & `uwsift-2.0.0b0.tar`

### file list

```diff
@@ -1,201 +1,270 @@
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.407389 uwsift-1.2.3/
--rw-r--r--   0 runner    (1001) docker     (121)    35148 2022-02-04 16:59:59.000000 uwsift-1.2.3/LICENSE.txt
--rw-r--r--   0 runner    (1001) docker     (121)      377 2022-02-04 16:59:59.000000 uwsift-1.2.3/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (121)     4082 2022-02-04 17:00:00.407389 uwsift-1.2.3/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (121)     2783 2022-02-04 16:59:59.000000 uwsift-1.2.3/README.md
--rw-r--r--   0 runner    (1001) docker     (121)      298 2022-02-04 17:00:00.407389 uwsift-1.2.3/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (121)     8588 2022-02-04 16:59:59.000000 uwsift-1.2.3/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.367387 uwsift-1.2.3/uwsift/
--rw-r--r--   0 runner    (1001) docker     (121)     1629 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    53956 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/__main__.py
--rw-r--r--   0 runner    (1001) docker     (121)    15047 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/common.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.371387 uwsift-1.2.3/uwsift/control/
--rw-r--r--   0 runner    (1001) docker     (121)      372 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/control/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    14997 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/control/doc_ws_as_timeline_scene.py
--rw-r--r--   0 runner    (1001) docker     (121)     1378 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/control/file_behaviors.py
--rw-r--r--   0 runner    (1001) docker     (121)    31430 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/control/layer_tree.py
--rw-r--r--   0 runner    (1001) docker     (121)     4899 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/control/rgb_behaviors.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.371387 uwsift-1.2.3/uwsift/data/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.363386 uwsift-1.2.3/uwsift/data/colormaps/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.379387 uwsift-1.2.3/uwsift/data/colormaps/OAX/
--rw-r--r--   0 runner    (1001) docker     (121)    17977 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/Cloud Amount Default.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    21297 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/Cloud Top Height.cmap
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.363386 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.383388 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/
--rw-r--r--   0 runner    (1001) docker     (121)     1554 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/ACTP.cmap
--rw-r--r--   0 runner    (1001) docker     (121)     2375 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/ADP.cmap
--rw-r--r--   0 runner    (1001) docker     (121)     1373 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/CSM.cmap
--rw-r--r--   0 runner    (1001) docker     (121)     1824 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/Dust.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    27392 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/FSC.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    15814 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/GOES-SST-35.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    33638 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/RRQPE.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    17050 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/RRQPE1.cmap
--rw-r--r--   0 runner    (1001) docker     (121)     1856 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/Smoke.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    67536 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/VTRSB.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    69943 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-cape-10.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    75542 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-li-10.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    78072 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-pw10-10.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    78298 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-pw8-10.cmap
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.387388 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/
--rw-r--r--   0 runner    (1001) docker     (121)    20342 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/CIRA (IR Default).cmap
--rw-r--r--   0 runner    (1001) docker     (121)    18921 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/IR WV.cmap
--rw-r--r--   0 runner    (1001) docker     (121)   126974 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/IR_Color_Clouds_Summer.cmap
--rw-r--r--   0 runner    (1001) docker     (121)   122179 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/IR_Color_Clouds_Winter.cmap
--rw-r--r--   0 runner    (1001) docker     (121)   141263 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/Rainbow_11_bit.cmap
--rw-r--r--   0 runner    (1001) docker     (121)   131406 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/WV_Dry_Yellow.cmap
--rw-r--r--   0 runner    (1001) docker     (121)   120378 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/dust_and_moisture_split_window.cmap
--rw-r--r--   0 runner    (1001) docker     (121)   134876 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/enhanced-rainbow-11.cmap
--rw-r--r--   0 runner    (1001) docker     (121)   170568 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/enhanced-rainbow_warmer_yellow.cmap
--rw-r--r--   0 runner    (1001) docker     (121)   123139 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/fire_detection_3.9.cmap
--rw-r--r--   0 runner    (1001) docker     (121)   122625 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/fogdiff_blue.cmap
--rw-r--r--   0 runner    (1001) docker     (121)   259032 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/ramsdis_IR_12bit.cmap
--rw-r--r--   0 runner    (1001) docker     (121)   259043 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/ramsdis_WV_12bit.cmap
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.387388 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Lifted Index/
--rw-r--r--   0 runner    (1001) docker     (121)    21274 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Lifted Index/Lifted Index - New CIMSS Table.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    21455 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Lifted Index/Lifted Index Default.cmap
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.387388 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Precip/
--rw-r--r--   0 runner    (1001) docker     (121)    10497 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Precip/Blended Total Precip Water.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    16455 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Precip/Percent of Normal TPW.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    20988 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Precip/Precip Water - New CIMSS Table.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    18907 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Precip/Precip Water - Polar.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    20837 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Precip/Precip Water Default.cmap
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.387388 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Skin Temp/
--rw-r--r--   0 runner    (1001) docker     (121)    20569 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Skin Temp/Skin Temp - New CIMSS Table.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    19847 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Skin Temp/Skin Temp Default.cmap
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.387388 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/VIS/
--rw-r--r--   0 runner    (1001) docker     (121)    15146 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/VIS/CA (Low Light Vis).cmap
--rw-r--r--   0 runner    (1001) docker     (121)    21931 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/VIS/Linear.cmap
--rw-r--r--   0 runner    (1001) docker     (121)   318665 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/VIS/VIS_gray_sq-root-12.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    17452 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/VIS/ZA (Vis Default).cmap
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.391388 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/WV/
--rw-r--r--   0 runner    (1001) docker     (121)    21793 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/WV/Gray Scale Water Vapor.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    16252 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/WV/NSSL VAS (WV Alternate).cmap
--rw-r--r--   0 runner    (1001) docker     (121)    20542 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/WV/SLC WV.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    15975 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/Low Cloud Base.cmap
--rw-r--r--   0 runner    (1001) docker     (121)    13724 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/Rain Rate.cmap
--rwxr-xr-x   0 runner    (1001) docker     (121)    16449 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/colormaps/OAX/prob_severe.cmap
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.391388 uwsift-1.2.3/uwsift/data/fonts/
--rw-r--r--   0 runner    (1001) docker     (121)   109700 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/fonts/Andale Mono.ttf
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.363386 uwsift-1.2.3/uwsift/data/grib_definitions/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.363386 uwsift-1.2.3/uwsift/data/grib_definitions/grib2/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.363386 uwsift-1.2.3/uwsift/data/grib_definitions/grib2/localConcepts/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.391388 uwsift-1.2.3/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/
--rw-r--r--   0 runner    (1001) docker     (121)     4652 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/cfName.def
--rw-r--r--   0 runner    (1001) docker     (121)     8874 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/modelName.def
--rw-r--r--   0 runner    (1001) docker     (121)     4175 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/name.def
--rw-r--r--   0 runner    (1001) docker     (121)     4087 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/paramId.def
--rw-r--r--   0 runner    (1001) docker     (121)     4087 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/shortName.def
--rw-r--r--   0 runner    (1001) docker     (121)     3802 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/units.def
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.391388 uwsift-1.2.3/uwsift/data/ne_110m_admin_0_countries/
--rw-r--r--   0 runner    (1001) docker     (121)    29581 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.README.html
--rw-r--r--   0 runner    (1001) docker     (121)        5 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.VERSION.txt
--rw-r--r--   0 runner    (1001) docker     (121)  1739659 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.dbf
--rw-r--r--   0 runner    (1001) docker     (121)      145 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.prj
--rw-r--r--   0 runner    (1001) docker     (121)   179828 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp
--rw-r--r--   0 runner    (1001) docker     (121)     1516 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shx
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.399389 uwsift-1.2.3/uwsift/data/ne_50m_admin_0_countries/
--rw-r--r--   0 runner    (1001) docker     (121)    29640 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.README.html
--rw-r--r--   0 runner    (1001) docker     (121)        5 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.VERSION.txt
--rw-r--r--   0 runner    (1001) docker     (121)  2367947 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.dbf
--rw-r--r--   0 runner    (1001) docker     (121)      145 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.prj
--rw-r--r--   0 runner    (1001) docker     (121)  1612204 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp
--rw-r--r--   0 runner    (1001) docker     (121)     2028 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shx
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.399389 uwsift-1.2.3/uwsift/data/ne_50m_admin_1_states_provinces_lakes/
--rw-r--r--   0 runner    (1001) docker     (121)    22074 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.README.html
--rw-r--r--   0 runner    (1001) docker     (121)        5 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.VERSION.txt
--rwxr-xr-x   0 runner    (1001) docker     (121)   217114 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.dbf
--rwxr-xr-x   0 runner    (1001) docker     (121)      145 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.prj
--rwxr-xr-x   0 runner    (1001) docker     (121)   559984 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.shp
--rwxr-xr-x   0 runner    (1001) docker     (121)      900 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.shx
--rw-r--r--   0 runner    (1001) docker     (121)  5653052 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/data/shadedrelief.jpg
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.399389 uwsift-1.2.3/uwsift/model/
--rw-r--r--   0 runner    (1001) docker     (121)      565 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     6612 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/model/composite_recipes.py
--rw-r--r--   0 runner    (1001) docker     (121)   114310 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/model/document.py
--rw-r--r--   0 runner    (1001) docker     (121)    14441 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/model/layer.py
--rw-r--r--   0 runner    (1001) docker     (121)     3501 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/model/shapes.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.399389 uwsift-1.2.3/uwsift/project/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/project/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    10190 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/project/ahi2gtiff.py
--rw-r--r--   0 runner    (1001) docker     (121)     7899 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/project/ahi2merc.py
--rw-r--r--   0 runner    (1001) docker     (121)    15728 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/project/geocat2merc.py
--rw-r--r--   0 runner    (1001) docker     (121)     3230 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/project/organize_data_bands.py
--rw-r--r--   0 runner    (1001) docker     (121)     9532 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/project/organize_data_topics.py
--rw-r--r--   0 runner    (1001) docker     (121)     7437 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/queue.py
--rw-r--r--   0 runner    (1001) docker     (121)      834 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/satpy_compat.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.399389 uwsift-1.2.3/uwsift/tests/
--rw-r--r--   0 runner    (1001) docker     (121)      266 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      566 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/tests/conftest.py
--rw-r--r--   0 runner    (1001) docker     (121)     7240 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/tests/timeline.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.403389 uwsift-1.2.3/uwsift/tests/view/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/tests/view/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     1283 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/tests/view/test_colormap_dialogs.py
--rw-r--r--   0 runner    (1001) docker     (121)    10315 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/tests/view/test_export_image.py
--rw-r--r--   0 runner    (1001) docker     (121)     4312 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/tests/view/test_open_file_wizard.py
--rw-r--r--   0 runner    (1001) docker     (121)     1803 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/tests/view/test_rgb_config.py
--rw-r--r--   0 runner    (1001) docker     (121)     9721 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/tests/view/test_tile_calculator.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.403389 uwsift-1.2.3/uwsift/tests/workspace/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/tests/workspace/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2282 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/tests/workspace/test_algebraic.py
--rw-r--r--   0 runner    (1001) docker     (121)     7872 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/tests/workspace/test_importer.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.403389 uwsift-1.2.3/uwsift/ui/
--rw-r--r--   0 runner    (1001) docker     (121)     1401 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/ui/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     4469 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/ui/change_colormap_dialog.ui
--rw-r--r--   0 runner    (1001) docker     (121)     4249 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/ui/change_colormap_dialog_ui.py
--rw-r--r--   0 runner    (1001) docker     (121)     6959 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/ui/config_rgb_layer.ui
--rw-r--r--   0 runner    (1001) docker     (121)     6782 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/ui/config_rgb_layer_ui.py
--rw-r--r--   0 runner    (1001) docker     (121)     4667 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/ui/create_algebraic_dialog.ui
--rw-r--r--   0 runner    (1001) docker     (121)     5795 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/ui/create_algebraic_dialog_ui.py
--rw-r--r--   0 runner    (1001) docker     (121)     3325 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/ui/custom_widgets.py
--rw-r--r--   0 runner    (1001) docker     (121)    12520 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/ui/export_image_dialog.ui
--rw-r--r--   0 runner    (1001) docker     (121)    11658 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/ui/export_image_dialog_ui.py
--rw-r--r--   0 runner    (1001) docker     (121)     2528 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/ui/open_cache_dialog.ui
--rw-r--r--   0 runner    (1001) docker     (121)     2904 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/ui/open_cache_dialog_ui.py
--rw-r--r--   0 runner    (1001) docker     (121)     7181 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/ui/open_file_wizard.ui
--rw-r--r--   0 runner    (1001) docker     (121)     7412 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/ui/open_file_wizard_ui.py
--rw-r--r--   0 runner    (1001) docker     (121)    32420 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/ui/pov_main.ui
--rw-r--r--   0 runner    (1001) docker     (121)    36243 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/ui/pov_main_ui.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.403389 uwsift-1.2.3/uwsift/util/
--rw-r--r--   0 runner    (1001) docker     (121)     1978 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/util/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     1439 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/util/default_paths.py
--rw-r--r--   0 runner    (1001) docker     (121)       22 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/version.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.407389 uwsift-1.2.3/uwsift/view/
--rw-r--r--   0 runner    (1001) docker     (121)      139 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     6940 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/cameras.py
--rw-r--r--   0 runner    (1001) docker     (121)    47048 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/colormap.py
--rw-r--r--   0 runner    (1001) docker     (121)     6249 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/colormap_dialogs.py
--rw-r--r--   0 runner    (1001) docker     (121)    14682 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/colormap_editor.py
--rw-r--r--   0 runner    (1001) docker     (121)     8259 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/create_algebraic.py
--rw-r--r--   0 runner    (1001) docker     (121)    16643 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/export_image.py
--rw-r--r--   0 runner    (1001) docker     (121)    10807 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/layer_details.py
--rw-r--r--   0 runner    (1001) docker     (121)    16387 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/open_file_wizard.py
--rw-r--r--   0 runner    (1001) docker     (121)    29244 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/probes.py
--rw-r--r--   0 runner    (1001) docker     (121)    17909 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/rgb_config.py
--rw-r--r--   0 runner    (1001) docker     (121)    56921 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/scene_graph.py
--rw-r--r--   0 runner    (1001) docker     (121)     1652 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/test_visuals.py
--rw-r--r--   0 runner    (1001) docker     (121)     9412 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/texture_atlas.py
--rw-r--r--   0 runner    (1001) docker     (121)    26683 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/tile_calculator.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.407389 uwsift-1.2.3/uwsift/view/timeline/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/timeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     7398 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/timeline/common.py
--rw-r--r--   0 runner    (1001) docker     (121)    23123 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/timeline/items.py
--rw-r--r--   0 runner    (1001) docker     (121)    25722 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/timeline/scene.py
--rw-r--r--   0 runner    (1001) docker     (121)    28820 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/transform.py
--rw-r--r--   0 runner    (1001) docker     (121)    40457 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/view/visuals.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.407389 uwsift-1.2.3/uwsift/workspace/
--rw-r--r--   0 runner    (1001) docker     (121)      500 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/workspace/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     9240 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/workspace/collector.py
--rw-r--r--   0 runner    (1001) docker     (121)      157 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/workspace/goesr_pug.py
--rw-r--r--   0 runner    (1001) docker     (121)     8967 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/workspace/guidebook.py
--rw-r--r--   0 runner    (1001) docker     (121)    57284 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/workspace/importer.py
--rw-r--r--   0 runner    (1001) docker     (121)     3471 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/workspace/matrix.py
--rw-r--r--   0 runner    (1001) docker     (121)    36022 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/workspace/metadatabase.py
--rw-r--r--   0 runner    (1001) docker     (121)    59341 2022-02-04 16:59:59.000000 uwsift-1.2.3/uwsift/workspace/workspace.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 17:00:00.371387 uwsift-1.2.3/uwsift.egg-info/
--rw-r--r--   0 runner    (1001) docker     (121)     4082 2022-02-04 17:00:00.000000 uwsift-1.2.3/uwsift.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (121)     7364 2022-02-04 17:00:00.000000 uwsift-1.2.3/uwsift.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (121)        1 2022-02-04 17:00:00.000000 uwsift-1.2.3/uwsift.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (121)       47 2022-02-04 17:00:00.000000 uwsift-1.2.3/uwsift.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (121)        1 2022-02-04 17:00:00.000000 uwsift-1.2.3/uwsift.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (121)      318 2022-02-04 17:00:00.000000 uwsift-1.2.3/uwsift.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (121)        7 2022-02-04 17:00:00.000000 uwsift-1.2.3/uwsift.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.300020 uwsift-2.0.0b0/
+-rw-r--r--   0 runner    (1001) docker     (123)     1147 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/AUTHORS.md
+-rw-r--r--   0 runner    (1001) docker     (123)    35149 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/LICENSE.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      497 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (123)     5122 2023-05-26 01:29:53.300020 uwsift-2.0.0b0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     4348 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)      491 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (123)      180 2023-05-26 01:29:53.300020 uwsift-2.0.0b0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     8715 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.256019 uwsift-2.0.0b0/uwsift/
+-rw-r--r--   0 runner    (1001) docker     (123)     4689 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    65160 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12129 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/common.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.256019 uwsift-2.0.0b0/uwsift/control/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/control/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5300 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/control/auto_update.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12847 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/control/qml_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      518 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/control/time_matcher.py
+-rw-r--r--   0 runner    (1001) docker     (123)      691 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/control/time_matcher_policies.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2147 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/control/time_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6538 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/control/time_transformer_policies.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.256019 uwsift-2.0.0b0/uwsift/data/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.252019 uwsift-2.0.0b0/uwsift/data/colormaps/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.264019 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/
+-rw-r--r--   0 runner    (1001) docker     (123)    17978 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/Cloud Amount Default.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    21297 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/Cloud Top Height.cmap
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.252019 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.268020 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/
+-rw-r--r--   0 runner    (1001) docker     (123)     1530 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/ACTP.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)     2367 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/ADP.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)     1349 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/CSM.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)     1800 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/Dust.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    27368 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/FSC.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    15814 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/GOES-SST-35.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    33614 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/RRQPE.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    17038 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/RRQPE1.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)     1832 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/Smoke.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    67512 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/VTRSB.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    69940 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-cape-10.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    75540 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-li-10.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    78071 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-pw10-10.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    78297 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-pw8-10.cmap
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.268020 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/
+-rw-r--r--   0 runner    (1001) docker     (123)    20342 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/CIRA (IR Default).cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    18921 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/IR WV.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)   126967 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/IR_Color_Clouds_Summer.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)   122170 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/IR_Color_Clouds_Winter.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)   141257 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/Rainbow_11_bit.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)   131401 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/WV_Dry_Yellow.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)   120372 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/dust_and_moisture_split_window.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)   134867 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/enhanced-rainbow-11.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)   170563 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/enhanced-rainbow_warmer_yellow.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)   123139 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/fire_detection_3.9.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)   122625 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/fogdiff_blue.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)   259026 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/ramsdis_IR_12bit.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)   259035 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/ramsdis_WV_12bit.cmap
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.272019 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Lifted Index/
+-rw-r--r--   0 runner    (1001) docker     (123)    21274 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Lifted Index/Lifted Index - New CIMSS Table.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    21455 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Lifted Index/Lifted Index Default.cmap
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.272019 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Precip/
+-rw-r--r--   0 runner    (1001) docker     (123)    10497 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Precip/Blended Total Precip Water.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    16455 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Precip/Percent of Normal TPW.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    20988 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Precip/Precip Water - New CIMSS Table.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    18908 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Precip/Precip Water - Polar.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    20837 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Precip/Precip Water Default.cmap
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.272019 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Skin Temp/
+-rw-r--r--   0 runner    (1001) docker     (123)    20569 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Skin Temp/Skin Temp - New CIMSS Table.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    19847 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Skin Temp/Skin Temp Default.cmap
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.272019 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/VIS/
+-rw-r--r--   0 runner    (1001) docker     (123)    15146 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/VIS/CA (Low Light Vis).cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    21931 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/VIS/Linear.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)   318651 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/VIS/VIS_gray_sq-root-12.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    17452 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/VIS/ZA (Vis Default).cmap
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.272019 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/WV/
+-rw-r--r--   0 runner    (1001) docker     (123)    21793 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/WV/Gray Scale Water Vapor.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    16252 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/WV/NSSL VAS (WV Alternate).cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    20542 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/WV/SLC WV.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    15976 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/Low Cloud Base.cmap
+-rw-r--r--   0 runner    (1001) docker     (123)    13725 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/Rain Rate.cmap
+-rwxr-xr-x   0 runner    (1001) docker     (123)    16449 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/colormaps/OAX/prob_severe.cmap
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.272019 uwsift-2.0.0b0/uwsift/data/fonts/
+-rw-r--r--   0 runner    (1001) docker     (123)   109700 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/fonts/Andale Mono.ttf
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.252019 uwsift-2.0.0b0/uwsift/data/grib_definitions/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.252019 uwsift-2.0.0b0/uwsift/data/grib_definitions/grib2/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.252019 uwsift-2.0.0b0/uwsift/data/grib_definitions/grib2/localConcepts/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.272019 uwsift-2.0.0b0/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/
+-rw-r--r--   0 runner    (1001) docker     (123)     4652 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/cfName.def
+-rw-r--r--   0 runner    (1001) docker     (123)     8874 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/modelName.def
+-rw-r--r--   0 runner    (1001) docker     (123)     4175 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/name.def
+-rw-r--r--   0 runner    (1001) docker     (123)     4087 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/paramId.def
+-rw-r--r--   0 runner    (1001) docker     (123)     4087 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/shortName.def
+-rw-r--r--   0 runner    (1001) docker     (123)     3802 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/units.def
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.272019 uwsift-2.0.0b0/uwsift/data/icons/
+-rw-r--r--   0 runner    (1001) docker     (123)     3204 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/icons/menu.svg
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.276019 uwsift-2.0.0b0/uwsift/data/ne_110m_admin_0_countries/
+-rw-r--r--   0 runner    (1001) docker     (123)    29479 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.README.html
+-rw-r--r--   0 runner    (1001) docker     (123)        6 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.VERSION.txt
+-rw-r--r--   0 runner    (1001) docker     (123)  1739659 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.dbf
+-rw-r--r--   0 runner    (1001) docker     (123)      146 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.prj
+-rw-r--r--   0 runner    (1001) docker     (123)   179828 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp
+-rw-r--r--   0 runner    (1001) docker     (123)     1516 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shx
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.284020 uwsift-2.0.0b0/uwsift/data/ne_50m_admin_0_countries/
+-rw-r--r--   0 runner    (1001) docker     (123)    29538 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.README.html
+-rw-r--r--   0 runner    (1001) docker     (123)        6 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.VERSION.txt
+-rw-r--r--   0 runner    (1001) docker     (123)  2367947 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.dbf
+-rw-r--r--   0 runner    (1001) docker     (123)      146 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.prj
+-rw-r--r--   0 runner    (1001) docker     (123)  1612204 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp
+-rw-r--r--   0 runner    (1001) docker     (123)     2028 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shx
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.284020 uwsift-2.0.0b0/uwsift/data/ne_50m_admin_1_states_provinces_lakes/
+-rw-r--r--   0 runner    (1001) docker     (123)    21821 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.README.html
+-rw-r--r--   0 runner    (1001) docker     (123)        6 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.VERSION.txt
+-rwxr-xr-x   0 runner    (1001) docker     (123)   217114 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.dbf
+-rwxr-xr-x   0 runner    (1001) docker     (123)      146 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.prj
+-rwxr-xr-x   0 runner    (1001) docker     (123)   559984 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.shp
+-rwxr-xr-x   0 runner    (1001) docker     (123)      900 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.shx
+-rw-r--r--   0 runner    (1001) docker     (123)  5653052 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/data/shadedrelief.jpg
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.252019 uwsift-2.0.0b0/uwsift/etc/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.252019 uwsift-2.0.0b0/uwsift/etc/SIFT/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.284020 uwsift-2.0.0b0/uwsift/etc/SIFT/config/
+-rw-r--r--   0 runner    (1001) docker     (123)      757 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/area_definitions.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      263 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/auto_update.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     3681 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/catalogue.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      406 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/default_colormaps.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1344 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/default_points_styles.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)       47 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/default_reader.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      485 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/default_standard_names.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/display.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      403 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/external_satpy_and_readers.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      555 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/limit_available_readers.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      573 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/logging.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      350 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/open_file_wizard.yaml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.288020 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/
+-rw-r--r--   0 runner    (1001) docker     (123)      310 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/abi_l1b.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      441 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/avhrr_l1b_eps.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      283 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/fci_l1_cat_lmk_loc.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      577 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/fci_l1_geoobs_lmk_loc.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      647 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/fci_l1_geoobs_lmk_nav_err.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      594 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/fci_l1c_iqt_fdhsi.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      587 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/fci_l1c_nc.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      532 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/fci_l2_nc.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      357 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/gld360_ualf2.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      577 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/li_l1b_nc.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      625 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/li_l2_nc.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      234 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/lsasaf_l2_frppixel_hdf.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)       68 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/modis_l1b.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      277 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/modis_l2_fire_hdf.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      338 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/seviri_l1b_hrit.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      678 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/seviri_l1b_native.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      202 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/seviri_l2_binary.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      714 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/seviri_l2_bufr.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      712 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/seviri_l2_grib.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      342 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/vii_l1b_nc.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      278 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/readers/viirs_l2_fire_nc.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      446 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/storage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      242 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/units.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1083 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/etc/SIFT/config/watchdog.yaml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.288020 uwsift-2.0.0b0/uwsift/model/
+-rw-r--r--   0 runner    (1001) docker     (123)      565 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3919 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/model/area_definitions_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32831 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/model/catalogue.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15360 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/model/composite_recipes.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15623 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/model/document.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16490 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/model/layer_item.py
+-rw-r--r--   0 runner    (1001) docker     (123)    46247 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/model/layer_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5306 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/model/product_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2296 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/model/shapes.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11189 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/model/time_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.288020 uwsift-2.0.0b0/uwsift/project/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/project/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3165 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/project/organize_data_bands.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7245 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/project/organize_data_topics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7195 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/queue.py
+-rw-r--r--   0 runner    (1001) docker     (123)      863 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/satpy_compat.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.288020 uwsift-2.0.0b0/uwsift/tests/
+-rw-r--r--   0 runner    (1001) docker     (123)      266 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      567 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.288020 uwsift-2.0.0b0/uwsift/tests/control/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/control/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8244 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/control/fill_dir_periodically.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.292020 uwsift-2.0.0b0/uwsift/tests/model/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1192 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/model/conftest.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3217 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/model/interactive_test_globbing_creator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6571 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/model/test_globbing_creator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6108 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/test_satpy_import.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.292020 uwsift-2.0.0b0/uwsift/tests/view/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/view/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10535 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/view/test_export_image.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2435 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/view/test_rgb_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10335 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/view/test_tile_calculator.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.292020 uwsift-2.0.0b0/uwsift/tests/workspace/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/workspace/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2261 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/workspace/test_algebraic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5852 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/workspace/test_importer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4325 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/tests/workspace/test_statistics.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.292020 uwsift-2.0.0b0/uwsift/ui/
+-rw-r--r--   0 runner    (1001) docker     (123)    15773 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/TimelineRuler.qml
+-rw-r--r--   0 runner    (1001) docker     (123)     1481 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      164 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/build.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     4063 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/change_colormap_dialog_ui.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3740 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/custom_widgets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2273 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/dataset_statistics_widget.ui
+-rw-r--r--   0 runner    (1001) docker     (123)     3481 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/dataset_statistics_widget_ui.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12522 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/export_image_dialog.ui
+-rw-r--r--   0 runner    (1001) docker     (123)    11782 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/export_image_dialog_ui.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11327 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/layer_details_widget.ui
+-rw-r--r--   0 runner    (1001) docker     (123)    13865 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/layer_details_widget_ui.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2534 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/open_cache_dialog.ui
+-rw-r--r--   0 runner    (1001) docker     (123)     3012 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/open_cache_dialog_ui.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11437 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/open_file_wizard.ui
+-rw-r--r--   0 runner    (1001) docker     (123)    14077 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/open_file_wizard_ui.py
+-rw-r--r--   0 runner    (1001) docker     (123)    60975 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/pov_main.ui
+-rw-r--r--   0 runner    (1001) docker     (123)    57885 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/pov_main_ui.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8481 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/ui/timeline.qml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.296020 uwsift-2.0.0b0/uwsift/util/
+-rw-r--r--   0 runner    (1001) docker     (123)     2571 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/util/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12990 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/util/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2025 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/util/default_paths.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6164 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/util/disk_management.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8944 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/util/heap_analyzer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3027 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/util/heap_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4512 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/util/logger.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1646 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/util/ps_analyzer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3750 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/util/ps_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11106 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/util/storage_agent.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13322 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/util/watchdog.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.296020 uwsift-2.0.0b0/uwsift/util/widgets/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/util/widgets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12349 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/util/widgets/pie_dial.py
+-rw-r--r--   0 runner    (1001) docker     (123)       24 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/version.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.296020 uwsift-2.0.0b0/uwsift/view/
+-rw-r--r--   0 runner    (1001) docker     (123)      139 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8548 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/algebraic_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4858 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/cameras.py
+-rw-r--r--   0 runner    (1001) docker     (123)    45367 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/colormap.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14769 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/colormap_editor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8209 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/dataset_statistics_pane.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17488 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/export_image.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18984 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/layer_details.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7774 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/layer_tree_view.py
+-rw-r--r--   0 runner    (1001) docker     (123)    37091 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/open_file_wizard.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35184 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/probes.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19068 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/rgb_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    62042 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/scene_graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1697 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/test_visuals.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9539 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/texture_atlas.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24852 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/tile_calculator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31336 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)    55023 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/view/visuals.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.296020 uwsift-2.0.0b0/uwsift/workspace/
+-rw-r--r--   0 runner    (1001) docker     (123)      632 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/workspace/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31601 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/workspace/caching_workspace.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9206 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/workspace/collector.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8728 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/workspace/guidebook.py
+-rw-r--r--   0 runner    (1001) docker     (123)    61222 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/workspace/importer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31664 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/workspace/metadatabase.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20395 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/workspace/simple_workspace.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5835 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/workspace/statistics.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.300020 uwsift-2.0.0b0/uwsift/workspace/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/workspace/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6978 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/workspace/utils/metadata_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32577 2023-05-26 01:29:52.000000 uwsift-2.0.0b0/uwsift/workspace/workspace.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-26 01:29:53.256019 uwsift-2.0.0b0/uwsift.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     5122 2023-05-26 01:29:53.000000 uwsift-2.0.0b0/uwsift.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     9879 2023-05-26 01:29:53.000000 uwsift-2.0.0b0/uwsift.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-26 01:29:53.000000 uwsift-2.0.0b0/uwsift.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       47 2023-05-26 01:29:53.000000 uwsift-2.0.0b0/uwsift.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-26 01:29:53.000000 uwsift-2.0.0b0/uwsift.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (123)      396 2023-05-26 01:29:53.000000 uwsift-2.0.0b0/uwsift.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        7 2023-05-26 01:29:53.000000 uwsift-2.0.0b0/uwsift.egg-info/top_level.txt
```

### Comparing `uwsift-1.2.3/LICENSE.txt` & `uwsift-2.0.0b0/LICENSE.txt`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -667,8 +667,8 @@
 <https://www.gnu.org/licenses/>.
 
   The GNU General Public License does not permit incorporating your program
 into proprietary programs.  If your program is a subroutine library, you
 may consider it more useful to permit linking proprietary applications with
 the library.  If this is what you want to do, use the GNU Lesser General
 Public License instead of this License.  But first, please read
-<https://www.gnu.org/licenses/why-not-lgpl.html>.
+<https://www.gnu.org/licenses/why-not-lgpl.html>.
```

### Comparing `uwsift-1.2.3/PKG-INFO` & `uwsift-2.0.0b0/README.md`

 * *Files 22% similar despite different names*

```diff
@@ -1,86 +1,95 @@
-Metadata-Version: 2.1
-Name: uwsift
-Version: 1.2.3
-Summary: Satellite Information Familiarization Tool
-Home-page: https://github.com/ssec/sift
-Author: R.K.Garcia, University of Wisconsin - Madison Space Science & Engineering Center
-Author-email: rkgarcia@wisc.edu
-License: UNKNOWN
-Description: # SIFT
-        
-        [![Coverage Status](https://coveralls.io/repos/github/ssec/sift/badge.svg)](https://coveralls.io/github/ssec/sift)
-        [![PyPI version](https://badge.fury.io/py/uwsift.svg)](https://badge.fury.io/py/uwsift)
-        ![CI](https://github.com/ssec/sift/actions/workflows/ci.yaml/badge.svg)
-        [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2587907.svg)](https://doi.org/10.5281/zenodo.2587907)
-        [![Gitter chat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im/gitterHQ/gitter)
-        
-        
-        Satellite Information Familiarization Tool (SIFT) was designed by the Space
-        Science and Engineering Center (SSEC) at the University of Wisconsin - Madison
-        to support scientists during forecaster training events. It provides a
-        graphical interface for visualization and basic analysis of geostationary
-        satellite data.
-        
-        SIFT is built on open source technologies like Python, OpenGL, and PyQt5. It
-        can be run from Mac, Windows, and Linux. The SIFT application is provided as
-        a python library called "uwsift". It can also be installed as a standalone
-        application.
-        
-        SIFT's main website is http://sift.ssec.wisc.edu/.
-        
-        The Git repository where you can find SIFT's source code, issue tracker, and
-        other documentation is on GitHub: https://github.com/ssec/sift
-        
-        The project wiki with some in-depth usage and installation instructions can
-        also be found on GitHub: https://github.com/ssec/sift/wiki
-        
-        Developer documentation can be found on https://sift.readthedocs.io/en/latest/.
-        
-        ## Data Access and Reading
-        
-        SIFT uses the open source python library Satpy to read input data. By using
-        Satpy SIFT is able to read many satellite instrument file formats, but may not
-        be able to display or understand all data formats that Satpy can read. SIFT
-        defaults to a limited set of readers for loading satellite instrument data.
-        This set of readers includes but is not limited to:
-        
-        * GOES-R ABI Level 1b
-        * Himawari AHI HRIT
-        * Himawari AHI HSD
-        * GEO-KOMPSAT-2 AMI Level 1b
-        
-        Other readers can be accessed from SIFT but this is considered an advanced
-        usage right now.
-        
-        ## Installation
-        
-        SIFT can be installed as an all-in-one bundled application or the python
-        library "uwsift" can be installed in a traditional python environment.
-        
-        Detailed installation instructions can be found on the
-        [GitHub Wiki](https://github.com/ssec/sift/wiki/Installation-Guide).
-        
-        ## Contributors
-        
-        SIFT is an open source project welcoming all contributions. See the
-        [Contributing Guide](https://github.com/ssec/sift/wiki/Contributing)
-        for more information on how you can help.
-        
-        ### Building and releasing
-        
-        For instructions on how SIFT is built and packaged see the
-        [releasing instructions](RELEASING.md). Note that these instructions
-        are mainly for SIFT developers and may require technical understanding of
-        SIFT and the libraries it depends on.
-        
-Platform: UNKNOWN
-Classifier: Development Status :: 5 - Production/Stable
-Classifier: Intended Audience :: Science/Research
-Classifier: License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)
-Classifier: Operating System :: OS Independent
-Classifier: Programming Language :: Python
-Classifier: Programming Language :: Python :: 3
-Classifier: Topic :: Scientific/Engineering
-Requires-Python: >=3.7
-Description-Content-Type: text/markdown
-Provides-Extra: docs
+# SIFT
+
+[![Coverage Status](https://coveralls.io/repos/github/ssec/sift/badge.svg)](https://coveralls.io/github/ssec/sift)
+[![PyPI version](https://badge.fury.io/py/uwsift.svg)](https://badge.fury.io/py/uwsift)
+![CI](https://github.com/ssec/sift/actions/workflows/ci.yaml/badge.svg)
+[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2587907.svg)](https://doi.org/10.5281/zenodo.2587907)
+[![Gitter chat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im/gitterHQ/gitter)
+[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/ssec/sift/master.svg)](https://results.pre-commit.ci/latest/github/ssec/sift/master)
+
+
+SIFT (Satellite Information Familiarization Tool) is a visualization tool
+for satellite data. It provides a graphical interface that can be used for
+e.g. fast visualization, scientific data analysis, training, cal/val activities
+and operations.
+
+SIFT is built on open source technologies like Python, OpenGL, PyQt5, and
+makes use of the [Pytroll framework](https://pytroll.github.io/) for reading
+and processing the input data.
+It can be run from Mac, Windows, and Linux. The SIFT application is provided as
+a python library called "uwsift". It can also be installed as a standalone
+application.
+
+SIFT's main website is http://sift.ssec.wisc.edu/.
+
+The Git repository where you can find SIFT's source code, issue tracker, and
+other documentation is on GitHub: https://github.com/ssec/sift
+
+The project wiki with some in-depth usage and installation instructions can
+also be found on GitHub: https://github.com/ssec/sift/wiki
+
+Developer and configuration documentation can be found on
+https://sift.readthedocs.io/en/latest/.
+
+## What's new in SIFT 2.0
+
+Many new features have been added starting from the version 2.0 of SIFT, including:
+- reading of data from both geostationary (GEO) as well as low-Earth-orbit (LEO)
+  satellite instruments
+- visualization of point data (e.g. lightning)
+- support for composite (RGB) visualization
+- an improved timeline manager
+- integration of a statistics module
+- full resampling functionalities using Pyresample
+- an automatic update/monitoring mode
+- partial redesign of the UI/UX
+- ... many more small but useful features!
+
+## History
+
+SIFT was originally created and designed at [SSEC/CIMSS at the University of
+Wisconsin - Madison](https://cimss.ssec.wisc.edu/) as a training tool for US
+NWS forecasters. Later, [EUMETSAT, European Organization for the Exploitation
+of Meteorological Satellites](https://www.eumetsat.int/),
+joined the project contributing many new features and refactoring various
+portions of the project to support instrument calibration/validation workflows
+as well as additional scientific analysis. CIMSS and EUMETSAT now work on the
+project together as well as accepting contributions from users outside these
+groups.
+
+EUMETSAT contributions, leading up to SIFT 2.0, were carried out by
+[ask  Innovative Visualisierungslsungen GmbH](https://askvisual.de/).
+
+## Data Access and Reading
+
+SIFT uses the open source python library Satpy to read input data. By using
+Satpy, SIFT is able to read many satellite instrument file formats,
+especially in the meteorology domain. The full list of available Satpy readers
+can be found in
+[Satpy's documentation](https://satpy.readthedocs.io/en/stable/index.html#id1).
+Note however that SIFT may not be able to display or understand all data formats
+that Satpy can read.
+SIFT defaults to a limited set of readers; head to the
+[configuration documentation](https://sift.readthedocs.io/en/latest/configuration/index.html)
+for customizing your SIFT.
+
+## Installation
+
+SIFT can be installed as an all-in-one bundled application or the python
+library "uwsift" can be installed in a traditional python environment.
+
+Detailed installation instructions can be found on the
+[GitHub Wiki](https://github.com/ssec/sift/wiki/Installation-Guide).
+
+## Contributors
+
+SIFT is an open source project welcoming all contributions. See the
+[Contributing Guide](https://github.com/ssec/sift/wiki/Contributing)
+for more information on how you can help.
+
+### Building and releasing
+
+For instructions on how SIFT is built and packaged see the
+[releasing instructions](RELEASING.md). Note that these instructions
+are mainly for SIFT developers and may require technical understanding of
+SIFT and the libraries it depends on.
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `uwsift-1.2.3/setup.py` & `uwsift-2.0.0b0/setup.py`

 * *Files 9% similar despite different names*

```diff
@@ -21,15 +21,15 @@
     python setup.py install
 
 To install for development replace 'install' with 'develop' in the above
 command.
 
 .. note::
 
-    PyQt4 is required for GUI operations, but must be install manually
+    PyQt5 is required for GUI operations, but must be install manually
     since it is not 'pip' installable.
 
 For Developers
 --------------
 
 To bump the version run:
 
@@ -41,40 +41,44 @@
 
 See the `-h` options for more info.
 
 """
 
 import os
 import re
-from setuptools import setup, find_packages, Command
+
+from setuptools import Command, find_packages, setup
 
 script_dir = os.path.dirname(os.path.realpath(__file__))
 version_pathname = os.path.join(script_dir, "uwsift", "version.py")
-version_str = open(version_pathname).readlines()[-1].split()[-1].strip("\"\'")
-version_regex = re.compile('^(?P<major>\d+)\.(?P<minor>\d+)\.(?P<micro>\d+)(?:(?P<dev_level>(a|b|rc))(?P<dev_version>\d))?$')
-version_info = version_regex.match(version_str).groupdict()
-assert version_info is not None, "Invalid version in version.py: {}".format(version_str)
+version_str = open(version_pathname).readlines()[-1].split()[-1].strip("\"'")
+version_regex = re.compile(
+    r"^(?P<major>\d+)\.(?P<minor>\d+)\.(?P<micro>\d+)(?:(?P<dev_level>(a|b|rc))(?P<dev_version>\d))?$"
+)
+version_match = version_regex.match(version_str)
+assert version_match is not None, "Invalid version in version.py: {}".format(version_str)  # nosec B101
+version_info = version_match.groupdict()
 version_info["major"] = int(version_info["major"])
 version_info["minor"] = int(version_info["minor"])
 version_info["micro"] = int(version_info["micro"])
 version_info["dev_version"] = int(version_info["dev_version"] or 0)
 
 extras_require = {
-    "docs": ['blockdiag', 'sphinx', 'sphinx_rtd_theme',
-             'sphinxcontrib-seqdiag', 'sphinxcontrib-blockdiag'],
+    "docs": ["blockdiag", "sphinx", "sphinx_rtd_theme", "sphinxcontrib-seqdiag", "sphinxcontrib-blockdiag", "psutil"],
+    "profiling": ["psutil"],
 }
 
 
 class BumpCommand(Command):
     description = "bump package version by one micro, minor, or major version number (major.minor.micro[a/b])"
     user_options = [
-        ("bump-level=", 'b', "major, minor, micro (default: None)"),
-        ("dev-level=", 'd', "alpha, beta, rc (default: None)"),
-        ("new-version=", 'v', "specify exact new version number (default: None"),
-        ("dry-run", 'n', "dry run, don't change anything"),
+        ("bump-level=", "b", "major, minor, micro (default: None)"),
+        ("dev-level=", "d", "alpha, beta, rc (default: None)"),
+        ("new-version=", "v", "specify exact new version number (default: None"),
+        ("dry-run", "n", "dry run, don't change anything"),
         ("tag", "t", "add a git tag for this version (default: False)"),
         ("commit", "c", "Run the git commit command but do not push (default: False)"),
     ]
     boolean_options = ["dry_run", "tag", "commit"]
 
     def initialize_options(self):
         self.bump_level = None
@@ -91,29 +95,29 @@
             raise ValueError("Dev level must be one of ['alpha', 'beta', <unspecified>]")
 
     def run(self):
         current_version = version_info
         new_version = current_version.copy()
         if self.new_version is not None:
             new_version_str = self.new_version
-            assert version_regex.match(new_version_str) is not None
+            assert version_regex.match(new_version_str) is not None  # nosec B101
         else:
             if self.bump_level == "micro":
                 new_version["micro"] += 1
             elif self.bump_level == "minor":
                 new_version["minor"] += 1
                 new_version["micro"] = 0
             elif self.bump_level == "major":
                 new_version["major"] += 1
                 new_version["minor"] = 0
                 new_version["micro"] = 0
             new_version_str = "{major:d}.{minor:d}.{micro:d}".format(**new_version)
 
             if self.dev_level:
-                short_level = {'alpha': 'a', 'beta': 'b', 'rc': 'rc'}[self.dev_level]
+                short_level = {"alpha": "a", "beta": "b", "rc": "rc"}[self.dev_level]
                 if current_version["dev_level"] == short_level and self.bump_level is None:
                     new_dev_version = current_version["dev_version"] + 1
                 else:
                     new_dev_version = 0
                 new_version_str += "{:s}{:d}".format(short_level, new_dev_version)
 
         # Update the version test in the version.py file
@@ -123,16 +127,17 @@
         if self.dry_run:
             print("### Dry Run: No modifications ###")
             return
 
         # Update the version.py
         print("Updating version.py...")
         version_data = open(version_pathname, "r").read()
-        version_data = version_data.replace("__version__ = \"{}\"".format(version_str),
-                                            "__version__ = \"{}\"".format(new_version_str))
+        version_data = version_data.replace(
+            '__version__ = "{}"'.format(version_str), '__version__ = "{}"'.format(new_version_str)
+        )
         open(version_pathname, "w").write(version_data)
 
         # Updating Windows Inno Setup file
         # XXX: Once PyInstaller executable is properly encoded with version this may be removed after proper fixes
         print("Updating Inno Setup Version number...")
         iss_pathname = os.path.join(script_dir, "sift.iss")
         file_data = open(iss_pathname, "rb").read()
@@ -141,72 +146,95 @@
         file_data = file_data.replace(_old, _new)
         open(iss_pathname, "wb").write(file_data)
 
         # Tag git repository commit
         add_args = ["git", "add", version_pathname, iss_pathname]
         commit_args = ["git", "commit", "-m", "Bump version from {} to {}".format(version_str, new_version_str)]
         if self.commit:
-            import subprocess
+            import subprocess  # nosec
+
             print("Adding files to git staging area...")
-            subprocess.check_call(add_args)
+            subprocess.check_call(add_args)  # nosec
             print("Committing changes...")
-            subprocess.check_call(commit_args)
+            subprocess.check_call(commit_args)  # nosec
         else:
-            commit_args[-1] = "\"" + commit_args[-1] + "\""
+            commit_args[-1] = '"' + commit_args[-1] + '"'
             print("To appropriate files:")
             print("    ", " ".join(add_args))
             print("To commit after run:")
             print("    ", " ".join(commit_args))
 
         tag_args = ["git", "tag", "-a", new_version_str, "-m", "Version {}".format(new_version_str)]
         if self.tag:
             print("Tagging commit...")
-            subprocess.check_call(tag_args)
+            subprocess.check_call(tag_args)  # nosec
         else:
-            tag_args[-1] = "\"" + tag_args[-1] + "\""
+            tag_args[-1] = '"' + tag_args[-1] + '"'
             print("To tag:")
             print("    ", " ".join(tag_args))
 
         print("To push git changes to remote, run:\n    git push --follow-tags")
 
 
-readme = open(os.path.join(script_dir, 'README.md')).read()
+readme = open(os.path.join(script_dir, "README.md")).read()
 
 setup(
-    name='uwsift',
+    name="uwsift",
     version=version_str,
     description="Satellite Information Familiarization Tool",
     long_description=readme,
-    long_description_content_type='text/markdown',
-    author='R.K.Garcia, University of Wisconsin - Madison Space Science & Engineering Center',
-    author_email='rkgarcia@wisc.edu',
-    url='https://github.com/ssec/sift',
-    classifiers=["Development Status :: 5 - Production/Stable",
-                 "Intended Audience :: Science/Research",
-                 "License :: OSI Approved :: GNU General Public License v3 " +
-                 "or later (GPLv3+)",
-                 "Operating System :: OS Independent",
-                 "Programming Language :: Python",
-                 "Programming Language :: Python :: 3",
-                 "Topic :: Scientific/Engineering"],
+    long_description_content_type="text/markdown",
+    author="SIFT Developers",
+    author_email="rkgarcia@wisc.edu",
+    url="https://github.com/ssec/sift",
+    classifiers=[
+        "Development Status :: 5 - Production/Stable",
+        "Intended Audience :: Science/Research",
+        "License :: OSI Approved :: GNU General Public License v3 " + "or later (GPLv3+)",
+        "Operating System :: OS Independent",
+        "Programming Language :: Python",
+        "Programming Language :: Python :: 3",
+        "Topic :: Scientific/Engineering",
+    ],
     zip_safe=False,
     include_package_data=True,
-    install_requires=['numpy', 'pillow', 'numba', 'vispy>=0.7.1',
-                      'netCDF4', 'h5py', 'pyproj',
-                      'pyshp', 'shapely', 'rasterio', 'sqlalchemy',
-                      'appdirs', 'pyyaml', 'pyqtgraph', 'satpy', 'matplotlib',
-                      'scikit-image', 'donfig',
-                      'pygrib;sys_platform=="linux" or sys_platform=="darwin"', 'imageio', 'pyqt5>=5.9'
-                      ],
-    tests_requires=['pytest', 'pytest-qt', 'pytest-mock'],
-    python_requires='>=3.7',
+    install_requires=[
+        "appdirs",
+        "donfig",
+        "h5py",
+        "imageio",
+        "matplotlib",
+        "netCDF4",
+        "numba",
+        "numpy",
+        "pillow",
+        "pyproj",
+        "pyqt5>=5.15",
+        "pyqtgraph",
+        "pyqtwebengine",
+        "pyshp",
+        "pyyaml",
+        "rasterio",
+        "satpy",
+        "scikit-image",
+        "shapely",
+        "sqlalchemy",
+        "trollsift",
+        "vispy>=0.10.0",
+        'pygrib;sys_platform=="linux" or sys_platform=="darwin"',
+        "ecmwflibs",
+        "eccodes",
+        "cfgrib",
+    ],
+    tests_requires=["pytest", "pytest-qt", "pytest-mock"],
+    python_requires=">=3.8",
     extras_require=extras_require,
     packages=find_packages(),
     entry_points={
         "console_scripts": [
             "SIFT = uwsift.__main__:main",
         ],
     },
     cmdclass={
-        'bump': BumpCommand,
-    }
+        "bump": BumpCommand,
+    },
 )
```

### Comparing `uwsift-1.2.3/uwsift/__main__.py` & `uwsift-2.0.0b0/uwsift/__main__.py`

 * *Files 24% similar despite different names*

```diff
@@ -14,68 +14,88 @@
 
 
 :author: R.K.Garcia <rayg@ssec.wisc.edu>
 :copyright: 2014 by University of Wisconsin Regents, see AUTHORS for more details
 :license: GPLv3, see LICENSE for more details
 """
 
-__author__ = 'rayg'
-
+import gc
 import logging
 import os
+import signal
 import sys
+import typing as typ
 from collections import OrderedDict
+from datetime import datetime, timedelta, timezone
 from functools import partial
 from glob import glob
-import typing as typ
+from types import FrameType
 from uuid import UUID
 
 from PyQt5 import QtCore, QtGui, QtWidgets
 from vispy import app
 
 import uwsift.ui.open_cache_dialog_ui as open_cache_dialog_ui
-from uwsift import __version__
-from uwsift.common import Info, Tool, CompositeType
-from uwsift.control.doc_ws_as_timeline_scene import SiftDocumentAsFramesInTracks
-from uwsift.control.layer_tree import LayerStackTreeViewModel
-from uwsift.control.rgb_behaviors import UserModifiesRGBLayers
+from uwsift import (
+    AUTO_UPDATE_MODE__ACTIVE,
+    IMAGE_DISPLAY_MODE,
+    USE_INVENTORY_DB,
+    __version__,
+    config,
+)
+from uwsift.common import ImageDisplayMode, Info, Tool
+from uwsift.model.area_definitions_manager import AreaDefinitionsManager
+
+# To have consistent logging for all modules (also for their static
+# initialization) it must be set up before importing them.
+from uwsift.model.composite_recipes import RecipeManager
 from uwsift.model.document import Document
-from uwsift.model.layer import DocRGBLayer
-from uwsift.queue import TaskQueue, TASK_PROGRESS, TASK_DOING
+from uwsift.model.layer_model import LayerModel
+from uwsift.model.product_dataset import ProductDataset
+from uwsift.queue import TASK_DOING, TASK_PROGRESS, TaskQueue
+
 # this is generated with pyuic4 pov_main.ui >pov_main_ui.py
 from uwsift.ui.pov_main_ui import Ui_MainWindow
-from uwsift.util import (WORKSPACE_DB_DIR, DOCUMENT_SETTINGS_DIR,
-                         get_package_data_dir, check_grib_definition_dir, check_imageio_deps)
+from uwsift.util import (
+    DOCUMENT_SETTINGS_DIR,
+    USER_CACHE_DIR,
+    WORKSPACE_DB_DIR,
+    HeapProfiler,
+    check_grib_definition_dir,
+    check_imageio_deps,
+    get_package_data_dir,
+)
+from uwsift.util.common import normalize_longitude
+from uwsift.util.logger import configure_loggers
+from uwsift.view.algebraic_config import AlgebraicLayerConfigPane
 from uwsift.view.colormap_editor import ColormapEditor
-from uwsift.view.create_algebraic import CreateAlgebraicDialog
 from uwsift.view.export_image import ExportImageHelper
-from uwsift.view.layer_details import SingleLayerInfoPane
-from uwsift.view.probes import ProbeGraphManager, DEFAULT_POINT_PROBE
+from uwsift.view.open_file_wizard import OpenFileWizard
+from uwsift.view.probes import DEFAULT_POINT_PROBE, ProbeGraphManager
 from uwsift.view.rgb_config import RGBLayerConfigPane
 from uwsift.view.scene_graph import SceneGraphManager
-from uwsift.workspace import Workspace
+from uwsift.workspace import CachingWorkspace, SimpleWorkspace
 from uwsift.workspace.collector import ResourceSearchPathCollector
 
 LOG = logging.getLogger(__name__)
+configure_loggers()
+
 PROGRESS_BAR_MAX = 1000
 STATUS_BAR_DURATION = 2000  # ms
 
+WATCHDOG_DATETIME_FORMAT_DISPLAY = "%Y-%m-%d %H:%M:%S %Z"
+WATCHDOG_DATETIME_FORMAT_STORE = "%Y-%m-%d %H:%M:%S %z"
 
-def test_layers_from_directory(doc, layer_tiff_glob):
-    return doc.open_file(glob(layer_tiff_glob))
-
-
-def test_layers(doc, glob_pattern=None):
-    if glob_pattern:
-        return test_layers_from_directory(doc, glob_pattern)
-    return []
+EXIT_FORCED_SHUTDOWN = 101
+EXIT_CONFIRMED_SHUTDOWN = 102
 
 
 async def do_test_cycle(txt: QtWidgets.QWidget):
     from asyncio import sleep
+
     n = 0
     while True:
         txt.setText(str(n))
         await sleep(1)
         n += 1
 
 
@@ -92,16 +112,14 @@
         self._opener = opener
         self._remover = remover
 
     def activate(self, uuid_to_name):
         self.ui.cacheListWidget.clear()
         # assume uuid_to_name is already an OrderedDict:
         sorted_items = uuid_to_name.items()
-        # sorted_items = sorted(uuid_to_name.items(),
-        #                       key=lambda x: x[1])
         for uuid, name in sorted_items:
             li = QtWidgets.QListWidgetItem(name)
             li.setData(QtCore.Qt.UserRole, uuid)
             self.ui.cacheListWidget.addItem(li)
         self.show()
 
     def _do_remove(self, *args, **kwargs):
@@ -110,39 +128,36 @@
             to_remove.append(item.data(QtCore.Qt.UserRole))
             self.ui.cacheListWidget.removeItemWidget(item)
         self._remover(to_remove)
         self.hide()
 
     def accept(self, *args, **kwargs):
         self.hide()
-        to_open = [item.data(QtCore.Qt.UserRole)
-                   for item in self.ui.cacheListWidget.selectedItems()]
+        to_open = [item.data(QtCore.Qt.UserRole) for item in self.ui.cacheListWidget.selectedItems()]
         LOG.info("opening from cache: " + repr(to_open))
         self._opener(to_open)
 
     def reject(self, *args, **kwargs):
         self.hide()
 
 
 class AnimationSpeedPopupWindow(QtWidgets.QWidget):
     _slider = None
     _active = False
 
     def __init__(self, slot, *args, **kwargs):
         super(AnimationSpeedPopupWindow, self).__init__(*args, **kwargs)
         from PyQt5.QtCore import Qt
+
         self.setWindowFlags(Qt.FramelessWindowHint | Qt.Popup)
         self.setFocusPolicy(Qt.ClickFocus)
-        self.setToolTip('Set animation speed')
+        self.setToolTip("Set animation speed")
         self._slider = QtWidgets.QSlider(parent=self)
-        # n, x = self._convert(10, reverse=True), self._convert(5000, reverse=True)
         n, x = 2, 150  # frames per 10 seconds
         self._slider.setRange(n, x)  #
-        # self._slider.setSingleStep(1)
-        # self._slider.setInvertedAppearance(True)
         self._slot = slot
         self._slider.valueChanged.connect(self._changed)
         self._layout = QtWidgets.QHBoxLayout()
         self._layout.addWidget(self._slider)
         self.setLayout(self._layout)
 
     def _convert(self, val: int, reverse: bool = False) -> float:
@@ -161,20 +176,21 @@
             ms = 10000.0 / float(val)
             return ms
 
     def _changed(self, value):
         if not self._active:
             return
         fps = float(value) / 10.0
-        self.setToolTip('{0:.1f} fps'.format(fps))
+        self.setToolTip("{0:.1f} fps".format(fps))
         val = self._convert(value)
         self._slot(val)
 
     def show_at(self, pos, val):
-        from PyQt5.QtCore import QRect, QPoint, QSize
+        from PyQt5.QtCore import QPoint, QRect, QSize
+
         sz = QSize(40, 180)
         pt = QPoint(pos.x() - 20, pos.y() - 160)
         rect = QRect(pt, sz)
         self.setGeometry(rect)
         self.show()
         self._slider.setValue(int(self._convert(val, reverse=True)))
         self._active = True
@@ -203,1010 +219,1282 @@
         else:
             break
 
 
 def _common_path_prefix(paths):
     "find the most common directory shared by a list of paths"
     paths = list(paths)
-    LOG.debug('looking for common path prefix for {}'.format(repr(paths)))
+    LOG.debug("looking for common path prefix for {}".format(repr(paths)))
     if len(paths) == 1:
         return os.path.split(paths[0])[0]
     parts = list(_common_path_prefix_seq(paths))
     if parts:
         return os.path.join(*parts)
     else:
         return None
 
 
 class UserControlsAnimation(QtCore.QObject):
     """Controller behavior focused around animation bar and next/last key bindings.
 
-    Connects between document, scene graph manager, layer list view of document; uses UI elements.
+    Connects between scene graph managers animation controller and the UI elements.
     Is composed into MainWindow.
-    - scrub left and right on slider
-    - next and last timestep (if time animation)
-    - next and last bandstep (if band animation)
-    - update time display above time slider
+    - next and last frame
     - update animation rate using popup widget
+    - start/stop animation
     """
-    ui = None
-    document: Document = None
-    scene_manager: SceneGraphManager = None
-    layer_list_model: LayerStackTreeViewModel = None
-    _animation_speed_popup = None  # window we'll show temporarily with animation speed popup
-
-    def __init__(self, ui,
-                 scene_manager: SceneGraphManager,
-                 document: Document,
-                 layer_list_model: LayerStackTreeViewModel
-                 ):
+
+    def __init__(self, ui, scene_manager: SceneGraphManager):
         """
         Args:
             ui: QtDesigner UI element tree for application
             scene_manager: Map display manager, needed for controller screen animation
-            document: document object, needed for sibling lookups
-            layer_list_model: model used to display current layer list, needed for selection
         """
         super(UserControlsAnimation, self).__init__()
         self.ui = ui
         self.scene_manager = scene_manager
-        self.document = document
-        self.layer_list_model = layer_list_model
+        # window we'll show temporarily with animation speed popup
+        self._animation_speed_popup: typ.Optional[AnimationSpeedPopupWindow] = None
 
-        self.scene_manager.didChangeFrame.connect(self.update_frame_slider)
         self.ui.animPlayPause.clicked.connect(self.toggle_animation)
         self.ui.animPlayPause.setContextMenuPolicy(QtCore.Qt.CustomContextMenu)
         self.ui.animPlayPause.customContextMenuRequested.connect(self.show_animation_speed_slider)
 
         self.ui.animForward.clicked.connect(self.next_frame)
         self.ui.animBack.clicked.connect(self.prev_frame)
 
-        # allow animation slider to set animation frame being displayed:
-        self.ui.animationSlider.valueChanged.connect(self.animation_slider_jump_frame)
-
-        # allow animation, once stopped, to propagate visibility to the document and layerlist:
-        self.scene_manager.didChangeLayerVisibility.connect(self.document.animation_changed_visibility)
-
-        self.document.didSwitchLayerSet.connect(self.animation_reset_by_layer_set_switch)
-        self.document.didChangeLayerVisibility.connect(self.update_frame_time_to_top_visible)
-        self.document.didReorderLayers.connect(self.update_frame_time_to_top_visible)
-        self.document.didRemoveLayers.connect(self.update_frame_time_to_top_visible)
-        self.document.didAddBasicLayer.connect(self.update_frame_time_to_top_visible)
-        self.document.didAddCompositeLayer.connect(self.update_frame_time_to_top_visible)
-
     def next_frame(self, *args, **kwargs):
         """Advance a frame along the animation order."""
-        self.scene_manager.layer_set.animating = False
-        self.scene_manager.layer_set.next_frame()
+        self.scene_manager.animation_controller.animating = False
+        self.scene_manager.animation_controller.time_manager.step()
 
     def prev_frame(self, *args, **kwargs):
         """Retreat a frame along the animation list."""
-        self.scene_manager.layer_set.animating = False
-        self.scene_manager.layer_set.next_frame(frame_number=-1)
-
-    def reset_frame_slider(self, *args, **kwargs):
-        """Reset frame slider to show current animation state in document when aniamtion list changes."""
-        frame_count = len(self.document.current_animation_order)
-        frame_index = None  # self.scene_manager.layer_set._frame_number  # FIXME BAAD
-        self.ui.animationSlider.setRange(0, frame_count - 1)
-        self.ui.animationSlider.setValue(frame_index or 0)
-        self.ui.animPlayPause.setDown(False)
-        self.ui.animationSlider.repaint()
-        self.update_frame_time_to_top_visible()
-
-    def update_frame_slider(self, frame_info):
-        """
-        Update animation frame slider and time display to reflect current document animation state.
-        Args:
-            frame_info: tuple, ultimately from SceneGraphManager.layer_set callback
-        """
-        frame_index, frame_count, animating, uuid = frame_info[:4]
-        self.ui.animationSlider.setRange(0, frame_count - 1)
-        self.ui.animationSlider.setValue(frame_index or 0)
-        # LOG.debug('did update animation slider {} {}'.format(frame_index, frame_count))
-        self.ui.animPlayPause.setDown(animating)
-        self.ui.animationSlider.repaint()
-        if animating:
-            self.ui.animationLabel.setText(self.document.time_label_for_uuid(uuid))
-        else:
-            self.update_frame_time_to_top_visible()
-
-    def update_frame_time_to_top_visible(self, *args):
-        """Update frame slider's time display to show the current top layer's time."""
-        self.ui.animationLabel.setText(self.document.time_label_for_uuid(self.document.current_visible_layer_uuid))
-
-    def animation_slider_jump_frame(self, event, *args, **kwargs):
-        """Update display to match frame slider change."""
-        frame = self.ui.animationSlider.value()
-        self.scene_manager.set_frame_number(frame)
-        # TODO: update layer list to reflect what layers are visible/hidden?
-
-    def _next_last_time_visibility(self, direction=0, *args, **kwargs):
-        LOG.info('time incr {}'.format(direction))
-        # TODO: if this frame is part of the animation sequence, update the slider as well!
-        uuids = self.layer_list_model.current_selected_uuids()
-        if not uuids:
-            self.ui.statusbar.showMessage('ERROR: No layer selected', STATUS_BAR_DURATION)
-        new_focus = None
-        for uuid in uuids:
-            new_focus = self.document.next_last_step(uuid, direction, bandwise=False)
-        return new_focus
-
-    def update_slider_if_frame_is_in_animation(self, uuid, **kwargs):
-        """Update frame slider to the specified frame UUID, but only if it's part of animation order."""
-        # FUTURE: this could be a cheaper operation but it's probably fine since it's input-driven
-        cao = self.document.current_animation_order
-        try:
-            dex = cao.index(uuid)
-        except ValueError:
-            return
-        frame_change_tuple = (dex, len(cao), False, uuid)
-        self.update_frame_slider(frame_change_tuple)
-
-    def next_last_time(self, direction=0, *args, **kwargs):
-        """Move forward (direction=+1) or backward (-1) a time step in animation order."""
-        self.scene_manager.layer_set.animating = False
-        new_focus = self._next_last_time_visibility(direction=direction)
-        self.layer_list_model.select([new_focus])
-        # if this part of the animation cycle, update the animation slider and displayed time as well
-        self.update_slider_if_frame_is_in_animation(new_focus)
-        return new_focus
-
-    def next_last_band(self, direction=0, *args, **kwargs):
-        """Move forward (direction=+1) or backward (-1) a band step in animation order."""
-        LOG.info('band incr {}'.format(direction))
-        uuids = self.layer_list_model.current_selected_uuids()
-        new_focus = None
-        if not uuids:
-            pass  # FIXME: notify user
-        for uuid in uuids:
-            new_focus = self.document.next_last_step(uuid, direction, bandwise=True)
-        if new_focus is not None:
-            self.layer_list_model.select([new_focus])
-            self.update_frame_time_to_top_visible()
-            self.update_slider_if_frame_is_in_animation(new_focus)
+        self.scene_manager.animation_controller.animating = False
+        self.scene_manager.animation_controller.time_manager.step(backwards=True)
 
     def set_animation_speed(self, milliseconds):
         """Change frame rate as measured in milliseconds."""
-        LOG.info('animation speed set to {}ms'.format(milliseconds))
-        self.scene_manager.layer_set.animation_speed = milliseconds
+        LOG.info("animation speed set to {}ms".format(milliseconds))
+        self.scene_manager.animation_controller.animation_speed = milliseconds
 
     def show_animation_speed_slider(self, pos: QtCore.QPoint, *args):
         """Show frame-rate slider as a pop-up control, at current mouse position."""
-        LOG.info('menu requested for animation control')
+        LOG.info("menu requested for animation control")
         gpos = self.ui.animPlayPause.mapToGlobal(pos)
 
         if self._animation_speed_popup is None:
             self._animation_speed_popup = popup = AnimationSpeedPopupWindow(slot=self.set_animation_speed, parent=None)
         else:
             popup = self._animation_speed_popup
         if not popup.isVisible():
-            popup.show_at(gpos, self.scene_manager.layer_set.animation_speed)
-
-    def animation_reset_by_layer_set_switch(self, *args, **kwargs):
-        """Perform necessary control resets when document layer set is swapped."""
-        self.reset_frame_slider()
-        self.update_frame_time_to_top_visible()
-
-    def change_animation_to_current_selection_siblings(self, *args, **kwargs):
-        """Assign new animation order based on selection."""
-        uuid = self._next_last_time_visibility(direction=0)
-        if uuid is None:
-            self.ui.statusbar.showMessage("ERROR: No layer selected", STATUS_BAR_DURATION)
-            return
-        # calculate the new animation sequence by consulting the guidebook
-        uuids = self.document.animate_siblings_of_layer(uuid)
-        if uuids:
-            self.ui.statusbar.showMessage("Info: Frame order updated", STATUS_BAR_DURATION)
-            self.layer_list_model.select(uuids)
-        else:
-            self.ui.statusbar.showMessage("ERROR: Layer with time steps or band siblings needed", STATUS_BAR_DURATION)
-        LOG.info('using siblings of {} for animation loop'.format(uuids[0] if uuids else '-unknown-'))
+            popup.show_at(gpos, self.scene_manager.animation_controller.animation_speed)
 
     def toggle_animation(self, action: QtWidgets.QAction = None, *args):
         """Toggle animation on/off."""
-        new_state = self.scene_manager.layer_set.toggle_animation()
+        new_state = self.scene_manager.animation_controller.toggle_animation()
         self.ui.animPlayPause.setChecked(new_state)
 
 
 class Main(QtWidgets.QMainWindow):
-    _last_open_dir: str = None  # directory to open files in
+    # TODO: the following settings, is this actually the best place to store
+    #  them here?
+    _last_open_dir: str = ""  # directory to open files in (preselection)
+    _last_reader: str = ""  # reader to open files with (preselection)
+
     _recent_files_menu: QtWidgets.QMenu = None  # QMenu
     _open_cache_dialog: QtWidgets.QDialog = None
     _screenshot_dialog: QtWidgets.QDialog = None
     _cmap_editor = None  # Gradient editor widget
-    _resource_collector: ResourceSearchPathCollector = None
     _resource_collector_timer: QtCore.QTimer = None
-    _timeline_scene: SiftDocumentAsFramesInTracks = None
+    _last_imported_dataset_uuid: typ.Optional[UUID] = None
+    _palette_text_green: QtGui.QPalette = None
+    _palette_text_red: QtGui.QPalette = None
+    _max_tolerable_idle_time: float = -1
+    _max_tolerable_dataset_age: float = -1
+
+    didFinishLoading = QtCore.pyqtSignal(list)
+    didSelectRegionProbeTool = QtCore.pyqtSignal()
 
-    def interactive_open_files(self, *args, files=None, **kwargs):
-        self.scene_manager.layer_set.animating = False
+    def _interactive_open_files(self, *args, files=None, **kwargs):
+        self.scene_manager.animation_controller.animating = False
         # http://pyqt.sourceforge.net/Docs/PyQt4/qfiledialog.html#getOpenFileNames
         filename_filters = [
             # 'All files (*.*)',
-            'All supported files (*.nc *.nc4)',
-            'GOES-16 NetCDF (*.nc *.nc4)',
+            "All supported files (*.nc *.nc4)",
+            "GOES-16 NetCDF (*.nc *.nc4)",
         ]
-        filter_str = ';;'.join(filename_filters)
+        filter_str = ";;".join(filename_filters)
         files = QtWidgets.QFileDialog.getOpenFileNames(
-            self, "Select one or more files to open", self._last_open_dir or os.getenv("HOME"), filter_str)[0]
+            self, "Select one or more files to open", self._last_open_dir or os.getenv("HOME"), filter_str
+        )[0]
         self.open_paths(files)
 
     def _bgnd_open_paths(self, paths, uuid_list, **importer_kwargs):
-        """Background task runs on a secondary thread
-        """
-        LOG.info("opening products from {} paths in background".format(
-            len(paths)))
+        """Background task runs on a secondary thread"""
+        LOG.info("opening products from {} paths in background".format(len(paths)))
         for progress in self.document.import_files(paths, **importer_kwargs):
             yield progress
-            uuid_list.append(progress['uuid'])
-        yield {TASK_DOING: 'products loaded from paths', TASK_PROGRESS: 1.0}
+            uuid_list.append(progress["uuid"])
+        yield {TASK_DOING: "products loaded from paths", TASK_PROGRESS: 1.0}
 
     def _bgnd_open_paths_finish(self, isok: bool, uuid_list: typ.List[UUID]):
         """Main thread finalization after background imports are done.
 
         Args:
             isok: whether _bgnd_open_paths ran without exception
             uuid_list: list of UUIDs it generated
         """
+        self.didFinishLoading.emit(uuid_list)
+
         if not uuid_list:
-            raise ValueError("no UUIDs provided by background open in _bgnd_open_paths_when_done")
+            raise ValueError("no UUIDs provided by background open" " in _bgnd_open_paths_finish")
         if not isok:
             raise ValueError("background open did not succeed")
-        uuid = uuid_list[-1]
-        self.layer_list_model.select([uuid])
-        # set the animation based on the last added (topmost) layer
-        self.document.animate_siblings_of_layer(uuid)
-        # force the newest layer to be visible
-        self.document.next_last_step(uuid)
+
+        if AUTO_UPDATE_MODE__ACTIVE:
+            # Choose one of the recently loaded datasets for reporting
+            # "vital signs"
+            dataset = self.layer_model.get_dataset_by_uuid(uuid_list[0])
+
+            self._update_dataset_timestamps(dataset)
+            self._update_heartbeat_file(dataset)
 
     def open_paths(self, paths, **importer_kwargs):
         paths = list(paths)
         if not paths:
             return
         uli = []
         bop = partial(self._bgnd_open_paths, uuid_list=uli, **importer_kwargs)
         bopf = partial(self._bgnd_open_paths_finish, uuid_list=uli)
         self.queue.add("load_files", bop(paths), "Open {} files".format(len(paths)), and_then=bopf, interactive=False)
         # don't use <algebraic layer ...> type paths
-        self._last_open_dir = _common_path_prefix([x for x in paths if x[0] != '<']) or self._last_open_dir
-        self.update_recent_file_menu()
+        self._last_open_dir = _common_path_prefix([x for x in paths if x[0] != "<"]) or self._last_open_dir
+        if USE_INVENTORY_DB:
+            self._update_recent_file_menu()
 
-    def activate_products_by_uuid(self, uuids):
+    def _activate_products_by_uuid(self, uuids):
         uuids = list(uuids)
         if not uuids:
             return
         for uuid in uuids:
-            self.document.activate_product_uuid_as_new_layer(uuid)
-        uuid = uuids[-1]
-        self.layer_list_model.select([uuid])
-        # set the animation based on the last added (topmost) layer
-        self.document.animate_siblings_of_layer(uuid)
-        # force the newest layer to be visible
-        self.document.next_last_step(uuid)
-        # don't use <algebraic layer ...> type paths
+            self.document.activate_product_uuid_as_new_dataset(uuid)
 
     def dropEvent(self, event):
-        LOG.debug('drop event on mainwindow')
+        LOG.debug("drop event on mainwindow")
         mime = event.mimeData()
         if mime.hasUrls:
             event.setDropAction(QtCore.Qt.CopyAction)
             event.accept()
             paths = [str(url.toLocalFile()) for url in mime.urls()]
             self.document.import_files(paths)
         else:
             event.ignore()
 
-    def change_tool(self, checked, name=Tool.PAN_ZOOM):
+    def _change_tool(self, checked, name=Tool.PAN_ZOOM):
         if checked is not True:
             return
+        if name == Tool.REGION_PROBE:
+            self.didSelectRegionProbeTool.emit()
         self.scene_manager.change_tool(name)
 
-    def update_recent_file_menu(self, *args, **kwargs):
+    def _update_recent_file_menu(self, *args, **kwargs):
+        assert isinstance(self.workspace, CachingWorkspace)  # nosec B101
         uuid_to_name = self.workspace.recently_used_products()
-        LOG.debug('recent uuids: {}'.format(repr(uuid_to_name.keys())))
+        LOG.debug("recent uuids: {}".format(repr(uuid_to_name.keys())))
         self._recent_files_menu.clear()
         for uuid, p_name in uuid_to_name.items():
+
             def openit(checked=False, uuid=uuid):
-                LOG.debug('open recent product {}'.format(uuid))
-                self.scene_manager.layer_set.animating = False
-                self.activate_products_by_uuid([uuid])
+                LOG.debug("open recent product {}".format(uuid))
+                self.scene_manager.animation_controller.animating = False
+                self._activate_products_by_uuid([uuid])
 
             open_action = QtWidgets.QAction(p_name, self)
             open_action.triggered.connect(openit)
             self._recent_files_menu.addAction(open_action)
 
-    def update_progress_bar(self, status_info, *args, **kwargs):
+    def _update_progress_bar(self, status_info, *args, **kwargs):
         active = status_info[0] if len(status_info) > 0 else None
-        # LOG.debug('{0!r:s}'.format(status_info))
         val = active[TASK_PROGRESS] if active else 0.0
-        txt = active[TASK_DOING] if active else ''
+        txt = active[TASK_DOING] if active else ""
         val = self.queue.progress_ratio(val)
         self.ui.progressBar.setValue(int(val * PROGRESS_BAR_MAX))
         self.ui.progressText.setText(txt)
-        # LOG.warning('progress bar updated to {}'.format(val))
-
-    def toggle_visibility_on_selected_layers(self, *args, **kwargs):
-        uuids = self.layer_list_model.current_selected_uuids()
-        self.document.toggle_layer_visibility(uuids)
-        self.animation.update_frame_time_to_top_visible()
-
-    def remove_layer(self, *args, **kwargs):
-        uuids = self.layer_list_model.current_selected_uuids()
-        rgb_uuids_handled = set()
-        uuids_to_remove = set()
-        # if we are deleting an RGB layer then we have to remove all of them
-        uuids = list(uuids)
-        for uuid in uuids:
-            layer = self.document[uuid]
-            if not isinstance(layer, DocRGBLayer):
-                uuids_to_remove.add(uuid)
-                continue
-            elif uuid in rgb_uuids_handled:
-                continue
-
-            rgbs_uuids = self.document.family_uuids_for_uuid(uuid, active_only=True)
-            all_rgbs_uuids = self.document.family_uuids_for_uuid(uuid)
-            if all(l_uuid in uuids for l_uuid in rgbs_uuids):
-                # there is only one of these RGBs so just remove it
-                # or they have selected all of the layers in this family
-                rgb_uuids_handled.update(all_rgbs_uuids)
-                uuids_to_remove.update(all_rgbs_uuids)
-                continue
 
-            # Ask the user if this is what they want
-            msg_box = QtWidgets.QMessageBox()
-            msg_box.setText("Deleting RGB layer, delete all times for this RGB?")
-            msg_box.setInformativeText("All related RGBs must also be deleted.")
-            msg_box.setStandardButtons(msg_box.Yes | msg_box.No)
-            msg_box.setDefaultButton(msg_box.No)
-            response = msg_box.exec_()
-            if response == msg_box.Yes:
-                LOG.debug("Setting all RGB family UUIDs to be removed: %s", uuid)
-                rgb_uuids_handled.update(all_rgbs_uuids)
-                uuids_to_remove.update(all_rgbs_uuids)
-            else:
-                LOG.debug("Will not delete RGB or its family: %s", uuid)
-                rgb_uuids_handled.update(all_rgbs_uuids)
-                continue
-
-        if uuids_to_remove:
-            self.document.remove_layers_from_all_sets(uuids_to_remove)
-
-    def _refresh_probe_results(self, *args):
-        arg1 = args[0]
-        if isinstance(arg1, dict):
-            # Given a dictionary of changes
-            uuids = arg1.keys()
+    def _toggle_visibility_on_selected_layers(self, *args, **kwargs):
+        model_indexes = self.ui.treeView.selectedIndexes()
+        self.layer_model.toggle_layers_visibility(model_indexes)
+
+    def _update_point_probe_text(self, probe_name):
+        current_row = self.ui.treeView.currentIndex().row()
+        product_dataset = None
+        if current_row >= 0:
+            selected_layer = self.layer_model.layers[current_row]
+            product_dataset = selected_layer.get_first_active_product_dataset()
         else:
-            uuids = [arg1]
-        _state, _xy_pos = self.graphManager.current_point_probe_status(DEFAULT_POINT_PROBE)
-        self.document.update_equalizer_values(DEFAULT_POINT_PROBE, _state, _xy_pos, uuids=uuids)
-
-    def update_point_probe_text(self, probe_name, state=None, xy_pos=None, uuid=None, animating=None):
-        if uuid is None:
-            uuid = self.document.current_visible_layer_uuid
-        if state is None or xy_pos is None:
-            _state, _xy_pos = self.graphManager.current_point_probe_status(probe_name)
-            if state is None:
-                state = _state
-            if xy_pos is None:
-                xy_pos = _xy_pos
+            selected_layer = None
+        uuid = None if product_dataset is None else product_dataset.uuid
+
+        state, xy_pos = self.graphManager.current_point_probe_status(probe_name)
 
-        if xy_pos is not None and state:
+        if state and xy_pos is not None:
             lon, lat = xy_pos
-            lon = lon % 360 if lon > 0 else lon % -360 + 360
-            lon = lon - 360 if lon > 180 else lon
+            lon = normalize_longitude(lon)
             lon_str = "{:>6.02f} {}".format(abs(lon), "W" if lon < 0 else "E")
             lat_str = "{:>6.02f} {}".format(abs(lat), "S" if lat < 0 else "N")
             probe_loc = "{}, {}".format(lon_str, lat_str)
         else:
             probe_loc = "{:>6s}  , {:>6s}  ".format("N/A", "N/A")
 
-        if animating:
-            data_str = "<animating>"
-        elif state and uuid is not None:
-            try:
-                data_point = self.workspace.get_content_point(uuid, xy_pos)
-            except ValueError:
-                LOG.debug("Could not get data value", exc_info=True)
-                data_point = None
-
-            if data_point is None:
-                data_str = "N/A"
-                layer_str = "N/A"
-            else:
-                info = self.document[uuid]
-                unit_info = info[Info.UNIT_CONVERSION]
-                data_point = unit_info[1](data_point)
-                data_str = unit_info[2](data_point, numeric=False)
-                if info.get(Info.CENTRAL_WAVELENGTH):
-                    wl = info[Info.CENTRAL_WAVELENGTH]
-                    if wl < 4.1:
-                        wl_str = "{:0.02f} m".format(wl)
-                    else:
-                        wl_str = "{:0.01f} m".format(wl)
-                    layer_str = "{}, {}".format(info[Info.SHORT_NAME],
-                                                wl_str)
-                else:
-                    layer_str = info[Info.SHORT_NAME]
-        else:
-            data_str = "N/A"
-            layer_str = "N/A"
+        col, row = "N/A", "N/A"
+        data_str = "N/A"
+        layer_str = "N/A"
+
+        if state:
+            if selected_layer:
+                layer_str = selected_layer.short_descriptor
+
+            if uuid is not None:
+                data_point = selected_layer.probe_value
+
+                if data_point is not None:
+                    unit_info = selected_layer.info[Info.UNIT_CONVERSION]
+                    convert_unit_func = unit_info[1]
+                    format_quantity_func = unit_info[2]
+                    data_str = format_quantity_func(convert_unit_func(data_point), numeric=False)
+                    col, row = self.workspace.position_to_grid_index(uuid, xy_pos)
+
         self.ui.cursorProbeLayer.setText(layer_str)
-        self.ui.cursorProbeText.setText("{} ({})".format(data_str, probe_loc))
+        self.ui.cursorProbeText.setText(f"{data_str} ({probe_loc}) [{col}, {row}]")
+
+    def run_gc_after_layer_deletion(self):
+        """
+        Trigger a full garbage collection run after the deletion of a layer and its datasets from the scene graph.
+        The code uses cyclic and weak references, which can only be freed by the GC.
+        """
+        unreachable_object_count = gc.collect()
+        LOG.debug(f"GC found {unreachable_object_count} unreachable objects")
+        if hasattr(self.workspace, "remove_content_data_from_cache_dir_checked"):
+            self.workspace.remove_content_data_from_cache_dir_checked()
 
-    def _init_timeline(self, doc: Document, ws: Workspace):
-        gv = self.ui.timelineView
+    def _update_heartbeat_file(self, dataset: ProductDataset):
+        """
+        Write the dataset creation time into the heartbeat file. The time of
+        last dataset update can be retrieved as the file modification time.
 
-        # set up the widget itself
-        gv.setViewportUpdateMode(QtWidgets.QGraphicsView.FullViewportUpdate)
-        gv.setHorizontalScrollBarPolicy(QtCore.Qt.ScrollBarAlwaysOn)
-        gv.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)
-        # gv.setRenderHints(QtGui.QPainter.Antialiasing)
-
-        # connect up the scene
-        doc.sync_potential_tracks_from_metadata()
-        LOG.debug("Potential tracks: {}".format(repr(doc.track_order)))
-        self._timeline_scene = SiftDocumentAsFramesInTracks(doc, self.workspace)
-        gv.setScene(self._timeline_scene)
-        QtWidgets.QApplication.instance().aboutToQuit.connect(self._timeline_scene.clear)
-
-        self._timeline_scene.sync_items()
-
-        def center_timeline_view_on_single_frame(frame_uuids, gv=gv, timeline_scene=self._timeline_scene):
-            frame_uuids = list(frame_uuids) if not isinstance(frame_uuids, list) else frame_uuids
-            # FIXME: again, we're assuming product id = frame id = layer id
-            if len(frame_uuids) == 1 and gv.isVisible():
-                LOG.debug('centering timeline view on single selected frame')
+        :param dataset: recently loaded dataset
+        """
+        dataset_sched_time_utc = dataset.info[Info.SCHED_TIME].replace(tzinfo=timezone.utc)
+        fmt_time = dataset_sched_time_utc.strftime(WATCHDOG_DATETIME_FORMAT_STORE).rstrip()
 
-                timeline_scene.center_view_on_frame(gv, frame_uuids[0])
+        journal_path = self._heartbeat_file + "-journal"
+        with open(journal_path, "w") as file:
+            file.write(f"{self.pid}\n")
+            file.write(fmt_time + "\n")
+            file.flush()
 
-        self.layer_list_model.uuidSelectionChanged.connect(center_timeline_view_on_single_frame)
+        os.rename(journal_path, self._heartbeat_file)
+
+    def _update_dataset_timestamps(self, dataset: ProductDataset):
+        """
+        Update the timestamp displayed in timeLastDatasetCreationLineEdit and
+        timeLastDatasetImportLineEdit. The import time is the current local
+        time.
+
+        If max_update_interval isn't None, then the dataset import time will be
+        colored green if the time between now and last import time is smaller
+        than max_update_interval. Otherwise it will be colored red.
 
-    def __init__(self, config_dir=None, workspace_dir=None, cache_size=None, glob_pattern=None, search_paths=None,
-                 border_shapefile=None, center=None, clear_workspace=False):
+        :param dataset: recently loaded dataset
+        """
+
+        self._last_imported_dataset_uuid = dataset.uuid
+
+        dataset_sched_time_utc = dataset.info[Info.SCHED_TIME].replace(tzinfo=timezone.utc)
+        self.ui.timeLastDatasetCreationLineEdit.setText(
+            dataset_sched_time_utc.strftime(WATCHDOG_DATETIME_FORMAT_DISPLAY)
+        )
+
+        self._last_imported_dataset_import_time = datetime.now(tz=timezone.utc)
+        self.ui.timeLastDatasetImportLineEdit.setText(
+            self._last_imported_dataset_import_time.strftime(WATCHDOG_DATETIME_FORMAT_DISPLAY)
+        )
+
+    def _clear_last_dataset_creation_time(self, removed_uuids: typ.List[UUID]):
+        """
+        Clear the LineEdit if the dataset, from which the creation time was
+        extracted, is deleted from the scene graph.
+        """
+        if self._last_imported_dataset_uuid in removed_uuids:
+            self.ui.timeLastDatasetCreationLineEdit.clear()
+            self._last_imported_dataset_uuid = None
+
+    def _update_current_time(self):
+        """
+        Update currentTimeLineEdit with the current local time.
+        """
+        now_utc = datetime.now(tz=timezone.utc)
+        self.ui.currentTimeLineEdit.setText(now_utc.strftime(WATCHDOG_DATETIME_FORMAT_DISPLAY))
+
+        if not self._last_imported_dataset_uuid:
+            return
+
+        if self._max_tolerable_dataset_age > 0:
+            dataset = self.document[self._last_imported_dataset_uuid]
+
+            dataset_sched_time_utc = dataset.get(Info.SCHED_TIME).replace(tzinfo=timezone.utc)
+            dataset_age = now_utc - dataset_sched_time_utc
+            if dataset_age.total_seconds() > self._max_tolerable_dataset_age:
+                palette = self._palette_text_red
+            else:
+                palette = self._palette_text_green
+            self.ui.timeLastDatasetCreationLineEdit.setPalette(palette)
+
+        if self._max_tolerable_idle_time > 0:
+            idle_time = now_utc - self._last_imported_dataset_import_time
+            if idle_time.total_seconds() > self._max_tolerable_idle_time:
+                palette = self._palette_text_red
+            else:
+                palette = self._palette_text_green
+            self.ui.timeLastDatasetImportLineEdit.setPalette(palette)
+
+    def __init__(
+        self,
+        config_dir=None,
+        workspace_dir=None,
+        cache_size=None,
+        search_paths=None,
+        border_shapefile=None,
+        center=None,
+        clear_workspace=False,
+    ):
         super(Main, self).__init__()
+
         self.ui = Ui_MainWindow()
         self.ui.setupUi(self)
-        # FIXME: Slider does not currently work as intended. Re-enable later
-        self.ui.timelineScaleSlider.setDisabled(True)
+        if AUTO_UPDATE_MODE__ACTIVE:
+            self.ui.animFrame.hide()
+            self.ui.timelineFrame.hide()
+            self.ui.progressBar.hide()
+            self.ui.progressText.hide()
 
+            self._init_auto_restart()
+        else:
+            self.ui.watchdogFrame.hide()
         self._init_font_sizes()
 
         self.setWindowTitle(self.windowTitle().replace("|X.X.X|", __version__))
         self._init_arrange_panes()
 
         self.queue = TaskQueue()
         self.ui.progressBar.setRange(0, PROGRESS_BAR_MAX)
-        self.queue.didMakeProgress.connect(self.update_progress_bar)
+        self.queue.didMakeProgress.connect(self._update_progress_bar)
 
         # create manager and helper classes
-        self.workspace = Workspace(workspace_dir, max_size_gb=cache_size, queue=self.queue,
-                                   initial_clear=clear_workspace)
+        if USE_INVENTORY_DB:
+            self.workspace = CachingWorkspace(
+                workspace_dir, max_size_gb=cache_size, queue=self.queue, initial_clear=clear_workspace
+            )
+        else:
+            self.workspace = SimpleWorkspace(workspace_dir)
         self.document = doc = Document(self.workspace, config_dir=config_dir, queue=self.queue)
-        self.scene_manager = SceneGraphManager(doc, self.workspace, self.queue,
-                                               border_shapefile=border_shapefile,
-                                               center=center,
-                                               parent=self)
-        self.export_image = ExportImageHelper(self, self.document, self.scene_manager)
-        self._wizard_dialog = None
+        self.scene_manager = SceneGraphManager(
+            doc, self.workspace, self.queue, borders_shapefiles=border_shapefile, center=center, parent=self
+        )
 
+        self._init_layer_model()
         self._init_layer_panes()
+        self._init_algebraic_pane()
         self._init_rgb_pane()
+        self._init_statistics_pane()
+        self._init_recipe_manager()
         self._init_map_widget()
+        self._init_qml_timeline()
+
+        self.export_image = ExportImageHelper(self, self.scene_manager, self.layer_model)
+        self._wizard_dialog = None
 
-        self.animation = UserControlsAnimation(self.ui,
-                                               self.scene_manager,
-                                               self.document,
-                                               self.layer_list_model
-                                               )
+        if AUTO_UPDATE_MODE__ACTIVE:
+            self._init_update_times_display()
+
+        self.animation = UserControlsAnimation(self.ui, self.scene_manager)
 
         # disable close button on panes
-        panes = [self.ui.areaProbePane, self.ui.layersPane, self.ui.layerDetailsPane, self.ui.rgbConfigPane]
+        panes = [
+            self.ui.algebraicConfigPaneDockWidget,
+            self.ui.areaProbePaneDockWidget,
+            self.ui.datasetStatisticsPaneDockWidget,
+            self.ui.layerDetailsPaneDockWidget,
+            self.ui.layerManagerPaneDockWidget,
+            self.ui.rgbConfigPaneDockWidget,
+        ]
         for pane in panes:
-            pane.setFeatures(QtWidgets.QDockWidget.DockWidgetFloatable |
-                             QtWidgets.QDockWidget.DockWidgetMovable)
+            pane.setFeatures(QtWidgets.QDockWidget.AllDockWidgetFeatures)
         # Make the panes on the right side 375px wide
         self.resizeDocks(panes, [375] * len(panes), QtCore.Qt.Horizontal)
 
-        test_layers(self.document, glob_pattern=glob_pattern)
-
-        # quamash async test pattern updates a control once a second
-        # loop.create_task(do_test_cycle(self.ui.cursorProbeText))
-
         # Interaction Setup
         self._init_key_releases()
 
         self.scheduler = QtCore.QTimer(parent=self)
         self.scheduler.setInterval(200)
         self.scheduler.timeout.connect(partial(self.scene_manager.on_view_change, self.scheduler))
 
         def start_wrapper(timer, event):
-            """Simple wrapper around a timers start method so we can accept but ignore the event provided
-            """
+            """Simple wrapper around a timers start method so we can accept but ignore the event provided"""
             timer.start()
 
         self.scene_manager.main_view.scene.transform.changed.connect(partial(start_wrapper, self.scheduler))
 
         print(self.scene_manager.main_view.describe_tree(with_transform=True))
 
+        self._init_point_polygon_probes()
         self._init_tool_controls()
         self._init_menu()
-        self._init_point_polygon_probes()
 
         # Set the projection based on the document's default
         self.document.change_projection()
         self.ui.projectionComboBox.setCurrentIndex(self.document.current_projection_index())
+        if USE_INVENTORY_DB:
+            self._init_metadata_background_collection(search_paths)
 
-        self._init_metadata_background_collection(search_paths)
+        # FIXME: make sure sync of metadata signals sync of document potentials and track display
 
-        # set up timeline
-        # LOG.info("potential tracks already in database: {}".format(repr(doc.potential_tracks())))
-        # self._init_timeline(doc, self.workspace)
+    def _init_auto_restart(self):
+        restart_popup_deadline = config.get("watchdog.auto_restart_popup_deadline", 0)
+        if restart_popup_deadline == 0:
+            LOG.warning("deadline for the auto restart is disabled")
+            self._restart_popup_deadline = None
+        else:
+            self._restart_popup_deadline = int(restart_popup_deadline)
 
-        # FIXME: make sure sync of metadata signals sync of document potentials and track display
+        restart_ask_again_interval = config.get("watchdog.auto_restart_ask_again_interval", 0)
+        if restart_ask_again_interval == 0:
+            LOG.warning("User won't be asked again to restart")
+            self._restart_ask_again_interval = None
+        else:
+            self._restart_ask_again_interval = timedelta(seconds=int(restart_ask_again_interval))
+
+        self._restart_handler_active = False
+        self._last_restart_request = None
+        signal.signal(signal.SIGUSR1, self._restart_handler)
 
     def _init_metadata_background_collection(self, search_paths):
         # if search paths are provided on the command line,
         self._resource_collector = collector = ResourceSearchPathCollector(self.workspace)
         collector.paths = search_paths or []
         # periodically launch a background scan
         self._resource_collector_timer = timer = QtCore.QTimer()
         self._timer_collect_resources()
         timer.timeout.connect(self._timer_collect_resources)
         timer.start(60000)
 
     def _init_point_polygon_probes(self):
-        self.graphManager = ProbeGraphManager(self.ui.probeTabWidget, self.workspace, self.document, self.queue)
+        self.graphManager = ProbeGraphManager(
+            self.ui.probeTabWidget,
+            self.ui.autoUpdateCheckbox,
+            self.ui.updateButton,
+            self.workspace,
+            self.layer_model,
+            self.queue,
+        )
         self.graphManager.didChangeTab.connect(self.scene_manager.show_only_polygons)
         self.graphManager.didClonePolygon.connect(self.scene_manager.copy_polygon)
         self.graphManager.pointProbeChanged.connect(self.scene_manager.on_point_probe_set)
-        self.graphManager.pointProbeChanged.connect(self.document.update_equalizer_values)
-        self.graphManager.pointProbeChanged.connect(self.update_point_probe_text)
-        self.graphManager.pointProbeChanged.connect(self.graphManager.update_point_probe_graph)
+        self.graphManager.pointProbeChanged.connect(self.layer_model.on_point_probe_set)
+        self.graphManager.pointProbeChanged.connect(self._update_point_probe_text)
+
+        self.didSelectRegionProbeTool.connect(self.graphManager.on_region_probe_tool_selected)
 
         self.scene_manager.newPointProbe.connect(self.graphManager.update_point_probe)
 
-        def _update_point_probe_slot(*args):
-            return self.graphManager.update_point_probe(DEFAULT_POINT_PROBE)
-        self.document.didAddBasicLayer.connect(_update_point_probe_slot)
-        self.document.didAddCompositeLayer.connect(_update_point_probe_slot)
-
-        # FIXME: These were added as a simple fix to update the probe value on layer changes, but this should really
-        #        have its own manager-like object
-        def _blackhole(*args, **kwargs):
-            return self.update_point_probe_text(DEFAULT_POINT_PROBE)
-
-        self.document.didChangeLayerVisibility.connect(_blackhole)
-        self.document.didAddBasicLayer.connect(_blackhole)
-        self.document.didAddCompositeLayer.connect(_blackhole)
-        self.document.didRemoveLayers.connect(_blackhole)
-        self.document.didReorderLayers.connect(_blackhole)
-        if False:
-            # XXX: Disable the below line if updating during animation is too much work
-            # self.scene_manager.didChangeFrame.connect(lambda frame_info: update_probe_point(uuid=frame_info[-1]))
-            pass
-        else:
-            # XXX: Disable the below line if updating the probe value during animation isn't a performance problem
-            self.scene_manager.didChangeFrame.connect(
-                lambda frame_info: self.ui.cursorProbeText.setText("Probe Value: <animating>"))
-
-        def update_probe_polygon(uuid, points, layerlist=self.layer_list_model):
-            top_uuids = list(self.document.current_visible_layer_uuids)
-            LOG.debug("top visible UUID is {0!r:s}".format(top_uuids))
+        self.layer_model.didUpdateLayers.connect(self.graphManager.update_point_probe)
+        self.layer_model.didUpdateLayers.connect(self.graphManager.handleActiveProductDatasetsChanged)
+        self.layer_model.didChangeRecipeLayerNames.connect(self.graphManager.handleActiveProductDatasetsChanged)
+
+        self.ui.treeView.selectedLayerForProbeChanged.connect(self._update_point_probe_text)
+        self.layer_model.didChangeRecipeLayerNames.connect(self._update_point_probe_text)
+
+        # Connect to an unnamed slot (lambda: ...) to strip off the argument
+        # (of type dict) from the signal 'didMatchTimes'
+        self.scene_manager.animation_controller.time_manager.didMatchTimes.connect(
+            lambda *args: self.graphManager.update_point_probe()
+        )
+
+        def update_probe_polygon(points: list):
+            probeable_layers = self.layer_model.get_probeable_layers()
+            probeable_layers_uuids = [layer.uuid for layer in probeable_layers]
 
             # TODO, when the plots manage their own layer selection, change this call
             # FUTURE, once the polygon is a layer, this will need to change
             # set the selection for the probe plot to the top visible layer(s)
             # new tabs should clone the information from the currently selected tab
             # the call below will check if this is a new polygon
-            self.graphManager.set_default_layer_selections(*top_uuids)
+            self.graphManager.set_default_layer_selections(probeable_layers_uuids)
+
             # update our current plot with the new polygon
-            polygon_name = self.graphManager.currentPolygonChanged(polygonPoints=points)
+            polygon_name = self.graphManager.current_graph_set_region(polygon_points=points)
 
             # do whatever other updates the scene manager needs
             self.scene_manager.on_new_polygon(polygon_name, points)
 
             if self.scene_manager._current_tool == Tool.REGION_PROBE:
                 self.ui.panZoomToolButton.click()
 
         self.scene_manager.newProbePolygon.connect(update_probe_polygon)
-        # setup RGB configuration
-        self.document.didChangeComposition.connect(lambda *args: self._refresh_probe_results(*args[1:]))
-        self.document.didChangeColorLimits.connect(self._refresh_probe_results)
 
     def _init_tool_controls(self):
-        self.ui.panZoomToolButton.toggled.connect(partial(self.change_tool, name=Tool.PAN_ZOOM))
-        self.ui.pointSelectButton.toggled.connect(partial(self.change_tool, name=Tool.POINT_PROBE))
-        self.ui.regionSelectButton.toggled.connect(partial(self.change_tool, name=Tool.REGION_PROBE))
-        self.change_tool(True)
+        self.ui.panZoomToolButton.toggled.connect(partial(self._change_tool, name=Tool.PAN_ZOOM))
+        self.ui.pointSelectButton.toggled.connect(partial(self._change_tool, name=Tool.POINT_PROBE))
+        self.ui.regionSelectButton.toggled.connect(partial(self._change_tool, name=Tool.REGION_PROBE))
+        self.ui.regionSelectButton.toggled.connect(self.ui.areaProbePaneDockWidget.show)
+        self.ui.regionSelectButton.toggled.connect(self.ui.areaProbePaneDockWidget.raise_)
+        self._change_tool(True)
+
+        def update_full_data_selection():
+            # TODO: this slot implementation should be revised, parts of it
+            #  should be merged into method provided by SceneManager, then
+            #  called from here and in remove_region_polygon() and
+            #  update_probe_polygon()
+
+            # Reset graph X layer and Y layer to the two top visible layers,
+            # see update_probe_polygon(), copied from there
+            probeable_layers = self.layer_model.get_probeable_layers()
+            probeable_layers_uuids = [layer.uuid for layer in probeable_layers]
+            LOG.debug(f"Probeable layer UUIDs are {probeable_layers_uuids!r:s}")
+            # TODO, when the plots manage their own layer selection, change this
+            #  call (see update_probe_polygon())
+            self.graphManager.set_default_layer_selections(probeable_layers_uuids)
+
+            must_remove_polygon = self.graphManager.current_graph_has_polygon()
+            current_graph_name = self.graphManager.current_graph_set_region(select_full_data=True)
+            if must_remove_polygon:
+                self.scene_manager.remove_polygon(current_graph_name)
+
+            if self.scene_manager._current_tool == Tool.REGION_PROBE:
+                self.ui.panZoomToolButton.click()
+
+            if self.scene_manager.has_pending_polygon():
+                self.scene_manager.clear_pending_polygon()
+
+        menu = QtWidgets.QMenu(parent=self)
+        select_full_data_action = QtWidgets.QAction("Select Full Data", parent=menu)
+        select_full_data_action.triggered.connect(update_full_data_selection)
+        select_full_data_action.triggered.connect(self.ui.areaProbePaneDockWidget.show)
+        select_full_data_action.triggered.connect(self.ui.areaProbePaneDockWidget.raise_)
+        select_full_data_action.triggered.connect(self.graphManager.on_region_probe_tool_selected)
+        menu.addAction(select_full_data_action)
+        self.ui.regionSelectButton.setMenu(menu)
+
+    def _init_layer_model(self):
+        self.layer_model = LayerModel(self.document)
+
+        self.document.didAddDataset.connect(self.layer_model.add_dataset)
+
+        self.layer_model.didCreateLayer.connect(self.scene_manager.add_node_for_layer)
+        self.layer_model.didAddImageDataset.connect(self.scene_manager.add_node_for_image_dataset)
+        self.layer_model.didAddLinesDataset.connect(self.scene_manager.add_node_for_lines_dataset)
+        self.layer_model.didAddMCImageDataset.connect(self.scene_manager.add_node_for_mc_image_dataset)
+        self.layer_model.didAddPointsDataset.connect(self.scene_manager.add_node_for_points_dataset)
+
+        self.layer_model.didAddSystemLayer.connect(self.scene_manager.add_node_for_system_generated_data)
+
+        self.layer_model.didReorderLayers.connect(self.scene_manager.update_layers_z)
+
+        self.layer_model.didChangeLayerVisible.connect(self.scene_manager.change_layer_visible)
+        self.layer_model.didChangeLayerOpacity.connect(self.scene_manager.change_layer_opacity)
+
+        self.layer_model.didChangeColormap.connect(self.scene_manager.change_dataset_nodes_colormap)
+        self.document.didUpdateUserColormap.connect(self.layer_model.update_user_colormap_for_layers)
+        self.layer_model.didChangeGamma.connect(self.scene_manager.change_dataset_nodes_gamma)
+        self.layer_model.didChangeColorLimits.connect(self.scene_manager.change_dataset_nodes_color_limits)
+
+        self.scene_manager.animation_controller.connect_to_model(self.layer_model)
+        self.layer_model.didActivateProductDataset.connect(self.scene_manager.change_dataset_visible)
+        self.layer_model.didAddCompositeDataset.connect(self.scene_manager.add_node_for_composite_dataset)
+        self.layer_model.didChangeCompositeProductDataset.connect(self.scene_manager.change_node_for_composite_dataset)
+        self.layer_model.willDeleteProductDataset.connect(self.scene_manager.purge_dataset)
+        self.layer_model.didRequestSelectionOfLayer.connect(self.ui.treeView.setCurrentIndex)
+
+        self.layer_model.willRemoveLayer.connect(self.scene_manager.remove_layer_node)
+
+        # Connect to an unnamed slot (lambda: ...) to strip off the argument
+        # (of type list) from the signal 'didDeleteProductDataset'
+        self.layer_model.didDeleteProductDataset.connect(lambda *args: self.run_gc_after_layer_deletion())
+
+        self.ui.treeView.setModel(self.layer_model)
+
+        self.layer_model.init_system_layers()
+
+    def _init_algebraic_pane(self):
+        self.algebraic_config_pane = AlgebraicLayerConfigPane(
+            self.ui, self.ui.algebraicScrollAreaWidget, self.layer_model
+        )
+
+        self.ui.treeView.layerSelectionChanged.connect(self.algebraic_config_pane.selection_did_change)
+        self.layer_model.didAddImageLayer.connect(self.algebraic_config_pane.layer_added)
+        self.algebraic_config_pane.didTriggeredUpdate.connect(self.layer_model.update_recipe_layer_timeline)
+        self.layer_model.didRemoveLayer.connect(self.algebraic_config_pane.layer_removed)
 
     def _init_rgb_pane(self):
-        self.rgb_config_pane = RGBLayerConfigPane(self.ui, self.ui.layersPaneWidget)
-        self.user_rgb_behavior = UserModifiesRGBLayers(self.document,
-                                                       self.rgb_config_pane,
-                                                       self.layer_list_model,
-                                                       parent=self)
+        self.rgb_config_pane = RGBLayerConfigPane(self.ui, self.ui.rgbScrollAreaWidget, self.layer_model)
+        self.ui.treeView.layerSelectionChanged.connect(self.rgb_config_pane.selection_did_change)
+        self.layer_model.didAddImageLayer.connect(self.rgb_config_pane.layer_added)
+        self.layer_model.didChangeRecipeLayerNames.connect(self.rgb_config_pane.set_combos_to_layer_names)
+        self.layer_model.didRemoveLayer.connect(self.rgb_config_pane.layer_removed)
+
+    def _init_statistics_pane(self):
+        self.ui.treeView.layerSelectionChanged.connect(self.ui.datasetStatisticsPane.selection_did_change)
+        self.layer_model.didFinishActivateProductDatasets.connect(self.ui.datasetStatisticsPane.initiate_update)
+
+    def _init_recipe_manager(self):
+        self.recipe_manager = RecipeManager()
+
+        # RGB Composites ------------------------------------------------------
+        self.layer_model.didRequestRGBCompositeRecipeCreation.connect(self.recipe_manager.create_rgb_recipe)
+        self.recipe_manager.didCreateRGBCompositeRecipe.connect(self.layer_model.create_rgb_composite_layer)
+        self.rgb_config_pane.didChangeRecipeName.connect(self.recipe_manager.update_recipe_name)
+        #   regarding name change notification recipe_manager -> layer_model: see 'Common' below
+
+        self.rgb_config_pane.didChangeRGBInputLayers.connect(self.recipe_manager.update_rgb_recipe_input_layers)
+        self.recipe_manager.didUpdateRGBInputLayers.connect(self.layer_model.update_recipe_layer_timeline)
+
+        self.rgb_config_pane.didChangeRGBColorLimits.connect(self.recipe_manager.update_rgb_recipe_color_limits)
+        self.recipe_manager.didUpdateRGBColorLimits.connect(self.layer_model.update_rgb_layer_color_limits)
+
+        self.rgb_config_pane.didChangeRGBGamma.connect(self.recipe_manager.update_rgb_recipe_gammas)
+        self.recipe_manager.didUpdateRGBGamma.connect(self.layer_model.update_rgb_layer_gamma)
+
+        # Algebraics ----------------------------------------------------------
+        self.layer_model.didRequestAlgebraicRecipeCreation.connect(self.recipe_manager.create_algebraic_recipe)
+        self.recipe_manager.didCreateAlgebraicRecipe.connect(self.layer_model.create_algebraic_composite_layer)
+        self.algebraic_config_pane.didChangeRecipeName.connect(self.recipe_manager.update_recipe_name)
+        #   regarding name change notification recipe_manager -> layer_model: see 'Common' below
+
+        self.algebraic_config_pane.didChangeAlgebraicInputLayers.connect(
+            self.recipe_manager.update_algebraic_recipe_input_layers
+        )
+        self.recipe_manager.didUpdateAlgebraicInputLayers.connect(self.layer_model.update_recipe_layer_timeline)
+
+        self.algebraic_config_pane.didChangeAlgebraicOperationKind.connect(
+            self.recipe_manager.update_algebraic_recipe_operation_kind
+        )
+        self.algebraic_config_pane.didChangeAlgebraicOperationFormula.connect(
+            self.recipe_manager.update_algebraic_recipe_operation_formula
+        )
+
+        # Common --------------------------------------------------------------
+        self.recipe_manager.didUpdateRecipeName.connect(self.layer_model.update_recipe_layer_name)
+
+        self.layer_model.willRemoveLayer.connect(self.recipe_manager.remove_layer_as_recipe_input)
 
     def _init_layer_panes(self):
-        # convey action between document and layer list view
-        self.layer_info_pane = SingleLayerInfoPane(self.document, parent=self.ui.layerDetailsContents)
-        self.layer_list_model = LayerStackTreeViewModel([self.ui.layerListView], self.document,
-                                                        parent=self.ui.layersPaneWidget)
-        self.layer_list_model.uuidSelectionChanged.connect(self.layer_info_pane.update_display)
+        self.ui.treeView.layerSelectionChanged.connect(self.ui.layerDetailsPane.selection_did_change)
+        self.layer_model.didFinishActivateProductDatasets.connect(self.ui.layerDetailsPane.initiate_update)
+
+        self.layer_model.didChangeColorLimits.connect(self.ui.layerDetailsPane.update_displayed_clims)
+        self.layer_model.didChangeColormap.connect(self.ui.layerDetailsPane.update_displayed_colormap)
 
     def _init_map_widget(self):
         # connect canvas and projection pieces
         self.ui.mainMapWidget.layout().addWidget(self.scene_manager.main_canvas.native)
-        self.ui.projectionComboBox.addItems(tuple(self.document.available_projections.keys()))
+        self.ui.projectionComboBox.addItems(tuple(AreaDefinitionsManager.available_area_def_names()))
         self.ui.projectionComboBox.currentIndexChanged.connect(self.document.change_projection_index)
         self.document.didChangeProjection.connect(self.scene_manager.set_projection)
 
+    def _init_qml_timeline(self):
+        from uwsift.control.qml_utils import QmlBackend
+        from uwsift.ui import QML_PATH
+
+        root_context = self.ui.timelineQuickWidget.engine().rootContext()
+
+        time_manager = self.scene_manager.animation_controller.time_manager
+        time_manager.qml_engine = self.ui.timelineQuickWidget.engine()
+        time_manager.qml_root_object = self.ui.timelineQuickWidget.rootObject()
+        time_manager.qml_backend = QmlBackend()
+        time_manager.qml_backend.didJumpInTimeline.connect(self.scene_manager.animation_controller.jump)
+        time_manager.qml_backend.didChangeTimebase.connect(time_manager.on_timebase_change)
+        # TODO(mk): refactor all QML related objects as belonging to TimeManager's QMLBackend
+        #           instance -> communication between TimeManager and QMLBackend via Signal/Slot?
+        time_manager.qml_backend.qml_layer_manager = time_manager.qml_layer_manager
+
+        root_context.setContextProperty("LayerManager", time_manager.qml_layer_manager)
+        root_context.setContextProperty("timebaseModel", time_manager.qml_timestamps_model)
+        root_context.setContextProperty("backend", time_manager.qml_backend)
+
+        self.ui.timelineQuickWidget.setSource(QtCore.QUrl.fromLocalFile(str(QML_PATH / "timeline.qml")))
+
+    # TODO(mk): replace with method to set all relevant ContextProperties?
+    def _get_qml_context(self):
+        engine = self.ui.timelineQuickWidget.engine()
+        return engine.rootContext()
+
     def _init_arrange_panes(self):
-        self.tabifyDockWidget(self.ui.layersPane, self.ui.areaProbePane)
-        self.tabifyDockWidget(self.ui.layerDetailsPane, self.ui.rgbConfigPane)
-        # self.tabifyDockWidget(self.ui.layerDetailsPane, self.ui.timelinePane)
-        self.layout().removeWidget(self.ui.timelinePane)
-        self.ui.timelinePane.deleteLater()
-        self.ui.timelinePane = None
-        # self.tabifyDockWidget(self.ui.rgbConfigPane, self.ui.layerDetailsPane)
-        # Make the layer list and layer details shown
-        self.ui.layersPane.raise_()
-        self.ui.layerDetailsPane.raise_()
+        self.tabifyDockWidget(self.ui.layerDetailsPaneDockWidget, self.ui.rgbConfigPaneDockWidget)
+        self.tabifyDockWidget(self.ui.layerDetailsPaneDockWidget, self.ui.algebraicConfigPaneDockWidget)
+
+        self.ui.layerDetailsPaneDockWidget.show()
+        self.ui.layerDetailsPaneDockWidget.raise_()
+
+        self.ui.datasetStatisticsPaneDockWidget.show()
+        self.ui.datasetStatisticsPaneDockWidget.raise_()
+
+        self.ui.areaProbePaneDockWidget.hide()
+        self.ui.algebraicConfigPaneDockWidget.hide()
+        self.ui.rgbConfigPaneDockWidget.hide()
+
         # refer to objectName'd entities as self.ui.objectName
         self.setAcceptDrops(True)
 
     def _init_font_sizes(self):
         # hack some font sizes until we migrate to PyQt5 and handle it better
         # was 14 on osx
-        font = QtGui.QFont('Andale Mono')
+        font = QtGui.QFont("Andale Mono")
         font.setPointSizeF(14)
         self.ui.cursorProbeLayer.setFont(font)
         self.ui.cursorProbeText.setFont(font)
 
+    def _init_update_times_display(self):
+        self._palette_text_green = QtGui.QPalette()
+        self._palette_text_green.setColor(QtGui.QPalette.Text, QtGui.QColor(23, 193, 23))
+        self._palette_text_red = QtGui.QPalette()
+        self._palette_text_red.setColor(QtGui.QPalette.Text, QtGui.QColor(220, 0, 0))
+
+        self._max_tolerable_idle_time = config.get("watchdog.max_tolerable_idle_time", -1)
+        if self._max_tolerable_idle_time <= 0:
+            LOG.warning(
+                "No valid configuration for"
+                " 'watchdog.max_tolerable_idle_time'. Can't highlight"
+                " last import time display when delayed."
+            )
+        else:
+            LOG.info(
+                f"Highlighting last import time display when delayed for"
+                f" more than {self._max_tolerable_idle_time} seconds."
+            )
+
+        self._max_tolerable_dataset_age = config.get("watchdog.max_tolerable_dataset_age", -1)
+        if self._max_tolerable_dataset_age <= 0:
+            LOG.warning(
+                "No valid configuration for"
+                " 'watchdog.max_tolerable_dataset_age'. Can't highlight"
+                " last data time display when delayed."
+            )
+        else:
+            LOG.info(
+                f"Highlighting last data time display when delayed for"
+                f" more than {self._max_tolerable_dataset_age} seconds."
+            )
+
+        # don't clear the time of last import when the layers are removed
+        self.layer_model.didDeleteProductDataset.connect(self._clear_last_dataset_creation_time)
+
+        self.currentTimeTimer = QtCore.QTimer(parent=self)
+        self.currentTimeTimer.timeout.connect(self._update_current_time)
+        self.currentTimeTimer.start(250)
+
+        heartbeat_file = config.get("watchdog.heartbeat_file", None)
+        if heartbeat_file is None:
+            LOG.warning("No configuration for 'watchdog.heartbeat_file'." " Can't send heartbeats to the watchdog.")
+        else:
+            self.pid = os.getpid()
+            self._heartbeat_file = heartbeat_file.replace("$$CACHE_DIR$$", USER_CACHE_DIR)
+            LOG.info(f"Communication with watchdog via heartbeat file " f" '{self._heartbeat_file}' configured.")
+
     def _timer_collect_resources(self):
         if self._resource_collector:
             LOG.debug("launching background resource search")
-            self.queue.add('resource_find', self._resource_collector.bgnd_look_for_new_files(),
-                           "look for new or modified files",
-                           and_then=self._finish_collecting_resources, interactive=False)
+            self.queue.add(
+                "resource_find",
+                self._resource_collector.bgnd_look_for_new_files(),
+                "look for new or modified files",
+                and_then=self._finish_collecting_resources,
+                interactive=False,
+            )
 
     def _finish_collecting_resources(self, previous_stage_ok: bool = True):
         ntodo = self._resource_collector.has_pending_files
         if ntodo:
             LOG.debug("{} new resources to collect metadata from".format(ntodo))
-            self.queue.add("resource_collect", self._resource_collector.bgnd_merge_new_file_metadata_into_mdb(),
-                           "add metadata for newly found files", interactive=False)
+            self.queue.add(
+                "resource_collect",
+                self._resource_collector.bgnd_merge_new_file_metadata_into_mdb(),
+                "add metadata for newly found files",
+                interactive=False,
+            )
         else:
             LOG.debug("no resources to collect, skipping followup task")
 
+    def _restart_handler(self, signal: int, frame: FrameType):
+        if self._restart_handler_active:
+            return
+        self._restart_handler_active = True
+
+        if self._restart_ask_again_interval is not None:
+            if self._last_restart_request is None:
+                self._last_restart_request = datetime.now()
+            else:
+                since_last_restart = datetime.now() - self._last_restart_request
+                if since_last_restart < self._restart_ask_again_interval:
+                    LOG.debug("Ignoring restart request because last restart " "request was denied recently")
+                    return
+
+        msg_box = QtWidgets.QMessageBox()
+        msg_box.setIcon(QtWidgets.QMessageBox.Information)
+        msg_box.setText("Do you want to perform the requested restart?")
+        msg_box.setWindowTitle("Restart Request")
+        msg_box.setStandardButtons(QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No)
+
+        def force_restart():
+            msg_box.close()
+            LOG.info("forced shutdown after restart request")
+            sys.exit(EXIT_FORCED_SHUTDOWN)
+
+        if self._restart_popup_deadline is not None:
+            timer = QtCore.QTimer()
+            timer.setSingleShot(True)
+            timer.timeout.connect(force_restart)
+            timer.start(self._restart_popup_deadline * 1000)
+
+        if msg_box.exec() == QtWidgets.QMessageBox.Yes:
+            LOG.info("shutdown in order to comply with restart request")
+            sys.exit(EXIT_CONFIRMED_SHUTDOWN)
+        else:
+            LOG.info("ignored restart request")
+            self._last_restart_request = datetime.now()
+            self._restart_handler_active = False
+
     def closeEvent(self, event, *args, **kwargs):
-        LOG.debug('main window closing')
+        LOG.debug("main window closing")
         self.workspace.close()
 
-    def _remove_paths_from_cache(self, paths):
-        self.workspace.remove_all_workspace_content_for_resource_paths(paths)
-        self.update_recent_file_menu()
+    def _open_from_cache(self, *args, **kwargs):
+        assert isinstance(self.workspace, CachingWorkspace)  # nosec B101
 
-    def open_from_cache(self, *args, **kwargs):
         def _activate_products_for_names(uuids):
-            LOG.info('activating cached products with uuids: {}'.format(repr(uuids)))
-            self.activate_products_by_uuid(uuids)
+            LOG.info("activating cached products with uuids: {}".format(repr(uuids)))
+            self._activate_products_by_uuid(uuids)
 
         def _purge_content_for_names(uuids):
-            LOG.info('removing cached products with uuids: {}'.format(repr(uuids)))
+            LOG.info("removing cached products with uuids: {}".format(repr(uuids)))
             self.workspace.purge_content_for_product_uuids(uuids, also_products=False)
-            self.update_recent_file_menu()
+            if USE_INVENTORY_DB:
+                self._update_recent_file_menu()
 
         if not self._open_cache_dialog:
-            self._open_cache_dialog = OpenCacheDialog(self,
-                                                      _activate_products_for_names,
-                                                      _purge_content_for_names)
+            self._open_cache_dialog = OpenCacheDialog(self, _activate_products_for_names, _purge_content_for_names)
 
         uuid_to_name = self.workspace.product_names_available_in_cache
         ordered_uuids = self.document.sort_product_uuids(uuid_to_name.keys())
         ordered_uuid_to_name = OrderedDict([(u, uuid_to_name[u]) for u in ordered_uuids])
         self._open_cache_dialog.activate(ordered_uuid_to_name)
 
-    # def open_glob(self, *args, **kwargs):
-    #     text, ok = QtWidgets.QInputDialog.getText(self, 'Open Glob Pattern', 'Open files matching pattern:')
-    #     from glob import glob
-    #     if ok:
-    #         paths = list(glob(text))
-    #         self.open_paths(paths)
-
-    def open_wizard(self, *args, **kwargs):
-        from uwsift.view.open_file_wizard import OpenFileWizard
-        wizard_dialog = OpenFileWizard(base_dir=self._last_open_dir, parent=self)
-        self._wizard_dialog = wizard_dialog
-        if wizard_dialog.exec_():
+    def _open_wizard(self, *args, **kwargs):
+        if not self._wizard_dialog:
+            self._wizard_dialog = OpenFileWizard(
+                base_dir=self._last_open_dir, base_reader=self._last_reader, parent=self
+            )
+        else:
+            self._wizard_dialog.restart()
+
+        if self._wizard_dialog.exec_():
             LOG.info("Loading products from open wizard...")
-            scenes = wizard_dialog.scenes
-            reader = wizard_dialog.previous_reader
+            scenes = self._wizard_dialog.scenes
+            reader = self._wizard_dialog.get_reader()
+
+            merge_with_existing = config.get("data_reading.merge_with_existing", True)
+            if USE_INVENTORY_DB and merge_with_existing:
+                # TODO(AR): provide a choice in the wizard for
+                #  'merge_with_existing' but only, if caching is off. The latter
+                #  condition becomes obsolete, when the CachingWorkspace becomes
+                #  able to merge too.
+                LOG.error(
+                    "Merging new data granules into existing data does not work"
+                    " when the caching database is active, i.e. not both"
+                    " 'storage.use_inventory_db' and"
+                    " 'data_reading.merge_with_existing' can be True."
+                    "  Deactivating merging, the caching database wins."
+                )
+                merge_with_existing = False
+
+            if IMAGE_DISPLAY_MODE == ImageDisplayMode.TILED_GEOLOCATED and merge_with_existing:
+                LOG.warning(
+                    "Merging of new data segments into existing data"
+                    " does not work well with adaptive tiled image"
+                    " rendering. Consider switching it of by configuring"
+                    " 'display.use_tiled_geolocated_images: False'"
+                )
+
             importer_kwargs = {
-                'reader': reader,
-                'scenes': scenes,
-                'dataset_ids': wizard_dialog.collect_selected_ids(),
+                "reader": reader,
+                "scenes": scenes,
+                "dataset_ids": self._wizard_dialog.collect_selected_ids(),
+                "resampling_info": self._wizard_dialog.resampling_info,
+                "merge_with_existing": merge_with_existing,
             }
-            self._last_open_dir = wizard_dialog.last_open_dir
-            self.open_paths(wizard_dialog.files_to_load,
-                            **importer_kwargs)
+            self._last_reader = reader
+            self._last_open_dir = self._wizard_dialog.get_directory()
+
+            self.open_paths(self._wizard_dialog.files_to_load, **importer_kwargs)
         else:
             LOG.debug("Wizard closed, nothing to load")
-        self._wizard_dialog = None
 
-    def remove_region_polygon(self, action: QtWidgets.QAction = None, *args):
+    def _reload_config(self):
+        config.refresh()
+
+    def _remove_region_polygon(self, action: QtWidgets.QAction = None, *args):
+        if self.scene_manager._current_tool == Tool.REGION_PROBE:
+            self.ui.panZoomToolButton.click()
+
         if self.scene_manager.has_pending_polygon():
             self.scene_manager.clear_pending_polygon()
             return
 
-        # Remove the polygon from other locations
-        removed_name = self.graphManager.currentPolygonChanged(None)
-        LOG.info("Clearing polygon with name '%s'", removed_name)
-        self.scene_manager.remove_polygon(removed_name)
-
-    def create_algebraic(self, action: QtWidgets.QAction = None, uuids=None, composite_type=CompositeType.ARITHMETIC):
-        if uuids is None:
-            uuids = list(self.layer_list_model.current_selected_uuids())
-        dialog = CreateAlgebraicDialog(self.document, uuids, parent=self)
-        dialog.show()
-        dialog.raise_()
-        dialog.activateWindow()
+        must_remove_polygon = self.graphManager.current_graph_has_polygon()
+        removed_name = self.graphManager.current_graph_set_region(None)
+
+        if must_remove_polygon:
+            # Remove the polygon from other locations
+            LOG.info("Clearing polygon with name '%s'", removed_name)
+            self.scene_manager.remove_polygon(removed_name)
 
     def _init_menu(self):
         open_action = QtWidgets.QAction("&Open...", self)
         open_action.setShortcut("Ctrl+Shift+O")
-        open_action.triggered.connect(self.interactive_open_files)
+        open_action.triggered.connect(self._interactive_open_files)
 
         exit_action = QtWidgets.QAction("&Exit", self)
         exit_action.setShortcut("Ctrl+Q")
         exit_action.triggered.connect(QtWidgets.QApplication.quit)
 
-        open_cache_action = QtWidgets.QAction("Open from Cache...", self)
-        open_cache_action.setShortcut("Ctrl+A")
-        open_cache_action.triggered.connect(self.open_from_cache)
-
-        # open_glob_action = QtWidgets.QAction("Open Filename Pattern...", self)
-        # open_glob_action.setShortcut("Ctrl+Shift+O")
-        # open_glob_action.triggered.connect(self.open_glob)
+        if USE_INVENTORY_DB:
+            open_cache_action = QtWidgets.QAction("Open from Cache...", self)
+            open_cache_action.setShortcut("Ctrl+A")
+            open_cache_action.triggered.connect(self._open_from_cache)
 
         open_wizard_action = QtWidgets.QAction("Open File Wizard...", self)
         open_wizard_action.setShortcuts(["Ctrl+O", "Ctrl+Alt+O"])
-        open_wizard_action.triggered.connect(self.open_wizard)
+        open_wizard_action.triggered.connect(self._open_wizard)
+
+        reload_config_action = QtWidgets.QAction("Reload Configuration", self)
+        reload_config_action.setShortcuts(["Ctrl+K", "Ctrl+Alt+K"])
+        reload_config_action.triggered.connect(self._reload_config)
 
         menubar = self.ui.menubar
-        file_menu = menubar.addMenu('&File')
+        file_menu = menubar.addMenu("&File")
         self.addAction(open_action)  # add it to the main window, not the menu (hide it)
-        file_menu.addAction(open_cache_action)
-        # file_menu.addAction(open_glob_action)
+        if USE_INVENTORY_DB:
+            file_menu.addAction(open_cache_action)
         file_menu.addAction(open_wizard_action)
-        self._recent_files_menu = file_menu.addMenu('Open Recent')
+        if USE_INVENTORY_DB:
+            self._recent_files_menu = file_menu.addMenu("Open Recent")
 
         screenshot_action = QtWidgets.QAction("Export Image", self)
         screenshot_action.setShortcut("Ctrl+I")
         screenshot_action.triggered.connect(self.export_image.take_screenshot)
         file_menu.addAction(screenshot_action)
 
+        file_menu.addSeparator()
+        file_menu.addAction(reload_config_action)
+        file_menu.addSeparator()
+
         file_menu.addAction(exit_action)
 
         next_time = QtWidgets.QAction("Next Time", self)
         next_time.setShortcut(QtCore.Qt.Key_Right)
-        next_slot = partial(self.animation.next_last_time, direction=1)
-        next_time.triggered.connect(next_slot)
-        # self.ui.animForward.clicked.connect(next_slot)
-
-        focus_current = QtWidgets.QAction("Focus Current Timestep", self)
-        focus_current.setShortcut('.')
-        focus_current.triggered.connect(partial(self.animation.next_last_band, direction=0))
+        next_time.triggered.connect(self.animation.next_frame)
 
         prev_time = QtWidgets.QAction("Previous Time", self)
         prev_time.setShortcut(QtCore.Qt.Key_Left)
-        prev_slot = partial(self.animation.next_last_time, direction=-1)
-        prev_time.triggered.connect(prev_slot)
-        # self.ui.animBack.clicked.connect(prev_slot)
-
-        focus_prev_band = QtWidgets.QAction("Next Band", self)
-        focus_prev_band.setShortcut(QtCore.Qt.Key_Up)
-        focus_prev_band.triggered.connect(partial(self.animation.next_last_band, direction=-1))
-
-        focus_next_band = QtWidgets.QAction("Previous Band", self)
-        focus_next_band.setShortcut(QtCore.Qt.Key_Down)
-        focus_next_band.triggered.connect(partial(self.animation.next_last_band, direction=1))
+        prev_time.triggered.connect(self.animation.prev_frame)
 
         toggle_vis = QtWidgets.QAction("Toggle &Visibility", self)
-        toggle_vis.setShortcut('V')
-        toggle_vis.triggered.connect(self.toggle_visibility_on_selected_layers)
+        toggle_vis.setShortcut("V")
+        toggle_vis.triggered.connect(self._toggle_visibility_on_selected_layers)
 
         animate = QtWidgets.QAction("Animate", self)
-        animate.setShortcut('A')
+        animate.setShortcut("A")
         animate.triggered.connect(partial(self.animation.toggle_animation, action=animate))
 
-        change_order = QtWidgets.QAction("Set Animation &Order", self)
-        change_order.setShortcut('O')
-        change_order.triggered.connect(self.animation.change_animation_to_current_selection_siblings)
-
-        flip_colormap = QtWidgets.QAction("Flip Color Limits (Top Layer)", self)
-        flip_colormap.setShortcut("/")
-        flip_colormap.triggered.connect(
-            lambda: self.document.flip_climits_for_layers([self.document.current_visible_layer_uuid]))
-
         cycle_borders = QtWidgets.QAction("Cycle &Borders", self)
-        cycle_borders.setShortcut('B')
+        cycle_borders.setShortcut("B")
         cycle_borders.triggered.connect(self.scene_manager.cycle_borders_color)
 
         cycle_grid = QtWidgets.QAction("Cycle &Lat/Lon Grid", self)
-        cycle_grid.setShortcut('L')
-        cycle_grid.triggered.connect(self.scene_manager.cycle_grid_color)
+        cycle_grid.setShortcut("L")
+        cycle_grid.triggered.connect(self.scene_manager.cycle_latlon_grid_color)
 
-        remove = QtWidgets.QAction("Remove Layer", self)
+        remove = QtWidgets.QAction("Remove Selected Layer(s)", self)
         remove.setShortcut(QtCore.Qt.Key_Delete)
-        remove.triggered.connect(self.remove_layer)
+        remove.triggered.connect(self.ui.treeView.begin_layers_removal)
 
         clear = QtWidgets.QAction("Clear Region Selection", self)
         clear.setShortcut(QtCore.Qt.Key_Escape)
-        clear.triggered.connect(self.remove_region_polygon)
+        clear.triggered.connect(self._remove_region_polygon)
 
         composite = QtWidgets.QAction("Create Composite", self)
-        composite.setShortcut('C')
-        composite.triggered.connect(self.user_rgb_behavior.create_rgb)
+        composite.setShortcut("C")
+        composite.triggered.connect(self.layer_model.start_rgb_composite_creation)
+        composite.triggered.connect(self.ui.rgbConfigPaneDockWidget.show)
+        composite.triggered.connect(self.ui.rgbConfigPaneDockWidget.raise_)
 
         algebraic = QtWidgets.QAction("Create Algebraic", self)
-        algebraic.triggered.connect(self.create_algebraic)
+        algebraic.triggered.connect(self.layer_model.start_algebraic_composite_creation)
+        algebraic.triggered.connect(self.ui.algebraicConfigPaneDockWidget.show)
+        algebraic.triggered.connect(self.ui.algebraicConfigPaneDockWidget.raise_)
 
         toggle_point = QtWidgets.QAction("Toggle Point Probe", self)
-        toggle_point.setShortcut('X')
+        toggle_point.setShortcut("X")
         toggle_point.triggered.connect(lambda: self.graphManager.toggle_point_probe(DEFAULT_POINT_PROBE))
 
         open_gradient = QtWidgets.QAction("Toggle Colormap Editor", self)
         open_gradient.setShortcut("Ctrl+E")
-        open_gradient.triggered.connect(self.open_colormap_editor)
+        open_gradient.triggered.connect(self._open_colormap_editor)
+
+        focus_layer_manager = QtWidgets.QAction("Set Focus to LayerManager", self)
+        focus_layer_manager.setShortcut("Shift+L")
+        focus_layer_manager.triggered.connect(self.ui.treeView.setFocus)
 
-        edit_menu = menubar.addMenu('&Edit')
+        edit_menu = menubar.addMenu("&Edit")
         edit_menu.addAction(remove)
         edit_menu.addAction(clear)
         edit_menu.addAction(toggle_point)
         edit_menu.addAction(open_gradient)
 
-        layer_menu = menubar.addMenu('&Layer')
+        layer_menu = menubar.addMenu("&Layer")
         layer_menu.addAction(composite)
         layer_menu.addAction(algebraic)
 
-        view_menu = menubar.addMenu('&View')
+        view_menu = menubar.addMenu("&View")
         view_menu.addAction(animate)
         view_menu.addAction(prev_time)
-        view_menu.addAction(focus_current)
         view_menu.addAction(next_time)
-        view_menu.addAction(focus_next_band)
-        view_menu.addAction(focus_prev_band)
-        view_menu.addAction(change_order)
         view_menu.addAction(toggle_vis)
-        view_menu.addAction(flip_colormap)
         view_menu.addAction(cycle_borders)
         view_menu.addAction(cycle_grid)
+        view_menu.addAction(focus_layer_manager)
 
-        self.update_recent_file_menu()
+        if USE_INVENTORY_DB:
+            self._update_recent_file_menu()
         menubar.setEnabled(True)
 
     def _init_key_releases(self):
         def cb_factory(required_key, cb):
             def tmp_cb(key, cb=cb):
                 if key.text == required_key:
                     return cb()
 
             return tmp_cb
 
         self.scene_manager.main_canvas.events.key_release.connect(cb_factory("t", self.scene_manager.next_tool))
 
-    def updateLayerList(self):
-        # self.ui.layers.add
-        pass
-
-    def open_colormap_editor(self):
+    def _open_colormap_editor(self):
         if self._cmap_editor is None:
             self._cmap_editor = ColormapEditor(doc=self.document)
         self._cmap_editor.show()
 
 
-def set_default_geometry(window, desktop=0):
-    screen = QtWidgets.QApplication.desktop()
-    screen_geometry = screen.screenGeometry(desktop)
-    # TODO: Remove platform specific code
-    if 'darwin' not in sys.platform:
-        w, h = screen_geometry.width() - 400, screen_geometry.height() - 300
-        window.setGeometry(200, 150, w, h)
-    else:
-        size = window.size()
-        w, h = size.width(), size.height()
-        center = screen_geometry.center()
-        screen_x, screen_y = center.x(), center.y()
-        window.move(int(screen_x - w / 2.), int(screen_y - h / 2.))
+def set_default_geometry(window, desktop=-1):
+    """
+    Try to fit the window centered on the screen given by its number
+    (`desktop`) or on its current screen.
+
+    If the screen resolution is too small, it may not be possible to shrink the
+    window to fit entirely on the screen. Also centering may not be perfect
+    since it is difficult to correctly take the window decorations and
+    restricted areas of the desktop like the task bar into account.
+    """
+    window.show()  # assures that the window has a windowHandle
+
+    desktop_window: QtGui.QWindow = window.windowHandle()
+    assert desktop_window  # Only call this for windows, i.e. toplevel widgets # nosec B101
+    screen: QtGui.QScreen = desktop_window.screen()
+    if desktop >= 0:
+        screens = QtWidgets.QApplication.screens()
+        if desktop < len(screens):
+            screen = screens[desktop]
+            desktop_window.setScreen(screen)
+
+    screen_available_geometry = screen.availableGeometry()
+    screen_margins = QtCore.QMargins(200, 150, 200, 150)  # left, top, right, bottom
+
+    # To cope for the space used by window decorations (the "frame"):
+    # TODO: without waiting for the window to be drawn at least the first time,
+    #  the following code does only results in frame margins (0, 0, 0, 0):
+    #  frame_margins = desktop_window.frameMargins()
+    #  To solve this, the call to this function must be deferred, e.g.
+    #  triggered by a timer event or a QShowEvent of the respective window.
+    #  So for now we set a sensible default:
+    frame_margins = QtCore.QMargins(0, 30, 0, 0)  # left, top, right, bottom
+
+    target_size = screen_available_geometry.size().shrunkBy(screen_margins).shrunkBy(frame_margins)
+    # If the screen is small, the target_size may be too small for the window:
+    possible_target_size = QtCore.QSize(
+        max(target_size.width(), desktop_window.minimumWidth()),
+        max(target_size.height(), desktop_window.minimumHeight()),
+    )
+
+    target_geometry = QtWidgets.QStyle.alignedRect(
+        QtCore.Qt.LeftToRight, QtCore.Qt.AlignCenter, possible_target_size, screen_available_geometry
+    )
+    desktop_window.setGeometry(target_geometry)
 
 
 def _search_paths(arglist):
     for arg in arglist:
-        for subpath in arg.split(':'):
+        for subpath in arg.split(":"):
             yield subpath
 
 
-def create_app() -> (app.Application, QtWidgets.QApplication):
+def autoconfigure_xritdecompress():
+    xrit_env_var = "XRIT_DECOMPRESS_PATH"
+    xrit_binary_names = ("xRITDecompress", "xRITDecompress.exe")
+    if os.environ.get(xrit_env_var, None):
+        LOG.debug("XRIT_DECOMPRESS_PATH already set to %s", os.environ[xrit_env_var])
+        return
+    if "PATH" in os.environ:
+        for p in os.environ["PATH"].split(os.pathsep):
+            if not p:
+                continue
+            for xrit_binary_name in xrit_binary_names:
+                xrit_cmd = os.path.join(p, xrit_binary_name)
+                if os.path.exists(xrit_cmd) and not os.path.isdir(xrit_cmd):
+                    os.environ[xrit_env_var] = xrit_cmd
+                    LOG.info("Auto configured xRITDecompress to %s", xrit_cmd)
+                    return
+
+
+def create_app() -> typ.Tuple[app.Application, QtWidgets.QApplication]:
     QtWidgets.QApplication.setAttribute(QtCore.Qt.AA_EnableHighDpiScaling)
-    vispy_app = app.use_app('pyqt5')
+    vispy_app = app.use_app("pyqt5")
     qt_app = vispy_app.create()
-    if hasattr(QtWidgets.QStyleFactory, 'AA_UseHighDpiPixmaps'):
+    if hasattr(QtWidgets.QStyleFactory, "AA_UseHighDpiPixmaps"):
         qt_app.setAttribute(QtCore.Qt.AA_UseHighDpiPixmaps)
     return vispy_app, qt_app
 
 
-def main():
+def main() -> int:
     import argparse
+
     parser = argparse.ArgumentParser(description="Run SIFT")
-    parser.add_argument("-w", "--workspace-dir", default=WORKSPACE_DB_DIR,
-                        help="Specify workspace base directory")
-    parser.add_argument("--cache-dir",
-                        help="(DEPRECATED: use --workspace-dir) Specify workspace directory")
-    parser.add_argument('--clear-workspace', action='store_true',
-                        help="Remove workspace contents during start up")
-    parser.add_argument("--config-dir", default=DOCUMENT_SETTINGS_DIR,
-                        help="Specify config directory")
-    parser.add_argument("-s", "--space", default=256, type=int,
-                        help="Specify max amount of data to hold in workspace cache in Gigabytes")
-    parser.add_argument("--border-shapefile", default=None,
-                        help="Specify alternative coastline/border shapefile")
-    parser.add_argument("--glob-pattern", default=os.environ.get("TIFF_GLOB", None),
-                        help="Specify glob pattern for input images")
-    parser.add_argument('-p', '--path', dest='paths', action="append",
-                        help='directory to search for data [MULTIPLE ALLOWED]')
-    parser.add_argument("-c", "--center", nargs=2, type=float,
-                        help="Specify center longitude and latitude for camera")
-    parser.add_argument("--desktop", type=int, default=0,
-                        help="Number of monitor/display to show the main window on (0 for main, 1 for secondary, etc.)")
-    parser.add_argument('-v', '--verbose', dest='verbosity', action="count",
-                        default=int(os.environ.get("VERBOSITY", 2)),
-                        help='each occurrence increases verbosity 1 level through '
-                             'ERROR-WARNING-Info-DEBUG (default Info)')
+    parser.add_argument("-w", "--workspace-dir", default=WORKSPACE_DB_DIR, help="Specify workspace base directory")
+    parser.add_argument("--cache-dir", help="(DEPRECATED: use --workspace-dir) Specify workspace directory")
+    parser.add_argument("--clear-workspace", action="store_true", help="Remove workspace contents during start up")
+    # FIXME Config dir from command line does not work currently because config is already loaded and used
+    # in various places before main is executed.
+    #    parser.add_argument("--config-dir", default=DOCUMENT_SETTINGS_DIR, help="Specify config directory")
+    parser.add_argument(
+        "-s",
+        "--space",
+        default=256,
+        type=int,
+        help="Specify max amount of data to hold in workspace cache in Gigabytes",
+    )
+    parser.add_argument("--border-shapefile", default=None, help="Specify alternative coastline/border shapefile")
+    parser.add_argument(
+        "-p", "--path", dest="paths", action="append", help="directory to search for data [MULTIPLE ALLOWED]"
+    )
+    parser.add_argument("-c", "--center", nargs=2, type=float, help="Specify center longitude and latitude for camera")
+    parser.add_argument(
+        "--desktop",
+        type=int,
+        default=0,
+        help="Number of monitor/display to show the main window on (0 for main, 1 for secondary, etc.)",
+    )
+    parser.add_argument(
+        "--profile-heap", type=float, help="take a snapshot of the heap in the given interval (in seconds)"
+    )
+    parser.add_argument(
+        "-v",
+        "--verbose",
+        dest="verbosity",
+        action="count",
+        default=int(os.environ.get("VERBOSITY", 2)),
+        help="each occurrence increases verbosity 1 level through " "ERROR-WARNING-Info-DEBUG (default Info)",
+    )
     args = parser.parse_args()
 
-    levels = [logging.ERROR, logging.WARN, logging.INFO, logging.DEBUG]
-    level = levels[min(3, args.verbosity)]
-    logging.basicConfig(level=level, datefmt='%H:%M:%S',
-                        format='%(levelname)s %(asctime)s %(module)s:%(funcName)s:L%(lineno)d %(message)s')
+    if args.profile_heap:
+        heap_profiler = HeapProfiler(args.profile_heap)
+        heap_profiler.start()
+
     check_grib_definition_dir()
     check_imageio_deps()
-    # logging.getLogger('vispy').setLevel(level)
 
     if args.cache_dir:
         LOG.warning("'--cache-dir' is deprecated, use '--workspace-dir'")
         args.workspace_dir = args.cache_dir
 
-    LOG.info("Using configuration directory: %s", args.config_dir)
+    #    LOG.info("Using configuration directory: %s", args.config_dir)
+    LOG.info("Using configuration directory: %s", DOCUMENT_SETTINGS_DIR)
     LOG.info("Using cache directory: %s", args.cache_dir)
     vispy_app, qt_app = create_app()
 
     # Add our own fonts to Qt windowing system
-    font_pattern = os.path.join(get_package_data_dir(), 'fonts', '*')
+    font_pattern = os.path.join(get_package_data_dir(), "fonts", "*")
     for fn in glob(font_pattern):
         QtGui.QFontDatabase.addApplicationFont(fn)
 
     data_search_paths = [] if not args.paths else list(_search_paths(args.paths))
     LOG.info("will search {} for new data periodically".format(repr(data_search_paths)))
 
+    autoconfigure_xritdecompress()
+
     window = Main(
         workspace_dir=args.workspace_dir,
-        config_dir=args.config_dir,
+        config_dir=DOCUMENT_SETTINGS_DIR,
         cache_size=args.space,
-        glob_pattern=args.glob_pattern,
         search_paths=data_search_paths,
         border_shapefile=args.border_shapefile,
         center=args.center,
         clear_workspace=args.clear_workspace,
     )
 
     set_default_geometry(window, desktop=args.desktop)
     window.show()
     # bring window to front
     window.raise_()
-    vispy_app.run()
+
+    if AUTO_UPDATE_MODE__ACTIVE:
+        # FIXME: let the AutoUpdateManager be in control...
+        from uwsift import config
+        from uwsift.control.auto_update import AutoUpdateManager
+
+        minimum_interval = config.get("auto_update.interval", None)
+        if minimum_interval is None:
+            raise ValueError("Auto update interval needs to be set!")
+        auto_update_manager = AutoUpdateManager(window, minimum_interval)
+        # connect signal to start timer anew when loading is done
+        window.didFinishLoading.connect(auto_update_manager.on_loading_done)
+
+    # run the event loop until the user closes the application
+    exit_code = vispy_app.run()
+    # Workaround PyCharm issue: The PyCharm dev console raises a TypeError if
+    # None is passed to 'sys.exit()'. Thus replace None by 0, both represent
+    # success for 'sys.exit()'.
+    if exit_code is not None:
+        return exit_code
+    return 0
+
+
+def close_splash_screen():
+    """
+    Helper function to close startup splash screen provided by pyinstaller.
+    If pyinstaller package has no splash screen or this is not started as
+    pyinstaller package at all the function does nothing.
+    """
+    if "_PYIBoot_SPLASH" in os.environ:
+        try:
+            import pyi_splash
+
+            pyi_splash.close()
+        except ModuleNotFoundError:
+            # not good not terrible. Splash screen might be still open but
+            # more likely there was no splash screen to begin with and the
+            # environment variable set defined by something else.
+            pass
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
+    close_splash_screen()
     sys.exit(main())
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `uwsift-1.2.3/uwsift/control/layer_tree.py` & `uwsift-2.0.0b0/uwsift/view/open_file_wizard.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,768 +1,780 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
-"""
+# This file is part of SIFT.
+#
+# SIFT is free software: you can redistribute it and/or modify
+# it under the terms of the GNU General Public License as published by
+# the Free Software Foundation, either version 3 of the License, or
+# (at your option) any later version.
+#
+# SIFT is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+# GNU General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with SIFT.  If not, see <http://www.gnu.org/licenses/>.
+import logging
+import os
+from collections import OrderedDict
+from enum import Enum
+from typing import Generator, Tuple, Union
+
+import satpy.resample
+import trollsift.parser as fnparser
+from PyQt5 import QtCore, QtGui, QtWidgets
+from PyQt5.QtCore import QPoint
+from PyQt5.QtWidgets import QMenu
+from satpy.readers import find_files_and_readers, group_files
+
+from uwsift import config
+from uwsift.model.area_definitions_manager import AreaDefinitionsManager
+from uwsift.satpy_compat import DataID, get_id_value
+from uwsift.ui.open_file_wizard_ui import Ui_openFileWizard
+from uwsift.util.common import create_scenes
+from uwsift.workspace.importer import available_satpy_readers, filter_dataset_ids
 
-PURPOSE
-Behaviors involving layer list controls
-Layer list shows:
-- visibility status
-- animation order
-- active color bar
-- active enhancements or formulas, if any
-- indicator colors
-
-Layer stack actions:
-- rearrange layers
-- de/select one or more layers
-- rename layer
-- bring up context menu for layer
-
-REFERENCES
-http://pyqt.sourceforge.net/Docs/PyQt4/qabstractlistmodel.html
-https://github.com/Werkov/PyQt4/blob/8cc9d541119226703668d34e06b430370fbfb482/examples/itemviews/simpledommodel.py
-
-REQUIRES
-
-
-:author: R.K.Garcia <rayg@ssec.wisc.edu>
-:copyright: 2014 by University of Wisconsin Regents, see AUTHORS for more details
-:license: GPLv3, see LICENSE for more details
-"""
+LOG = logging.getLogger(__name__)
 
-__author__ = 'rayg'
-__docformat__ = 'reStructuredText'
+# Key in .yaml files where data reading information is stored for each reader
+DATA_READING_CONFIG_KEY = "data_reading"
 
-import logging
-import pickle as pkl
-import sys
+CHECKMARK = ""
 
-from PyQt5.QtCore import (QAbstractItemModel, Qt, QSize, QModelIndex, QPoint, QMimeData,
-                          pyqtSignal, QRect, QItemSelection, QItemSelectionModel)
-from PyQt5.QtGui import QColor, QFont, QPen
-from PyQt5.QtWidgets import (QTreeView, QStyledItemDelegate, QAbstractItemView,
-                             QMenu, QStyle, QStyleOptionViewItem, QActionGroup, QAction)
-
-from uwsift.common import Info, Kind
-from uwsift.model.document import Document
-from uwsift.view.colormap_dialogs import ChangeColormapDialog
+PAGE_ID_FILE_SELECTION = 0
+PAGE_ID_PRODUCT_SELECTION = 1
 
-LOG = logging.getLogger(__name__)
 
-COLUMNS = ('Visibility', 'Name', 'Enhancement')
+class Conf(Enum):
+    # Just to have a well-defined constant to express "skip this resampler"
+    SKIP = 1
+
+
+RESAMPLING_METHODS = {
+    # Configure a display name for each resampling method ID as well as for
+    # which geometry definition (AreaDefinition or SwathDefinition for now) it
+    # works by associating an according tuple of strings where the first item is
+    # the display name followed by any suitable geometry definition.
+    #
+    # To hide a resampling method just associate Conf.SKIP with its ID.
+    #
+    # This configuration is evaluated in
+    # OpenFileWizard.update_resampling_method_combobox().
+    "none": ("None", "AreaDefinition"),
+    "kd_tree": Conf.SKIP,  # synonym for `nearest`, don't show both
+    "nearest": ("Nearest Neighbor", "AreaDefinition", "SwathDefinition"),
+    "ewa": ("Elliptical Weighted Averaging", "AreaDefinition", "SwathDefinition"),
+    "bilinear": ("Bilinear", "AreaDefinition", "SwathDefinition"),
+    "native": ("Native", "AreaDefinition"),
+    "gradient_search": ("Gradient Search", "AreaDefinition"),
+    "bucket_avg": ("Bucket Average", "AreaDefinition", "SwathDefinition"),
+    "bucket_sum": ("Bucket Sum", "AreaDefinition"),
+    "bucket_count": ("Bucket Count", "AreaDefinition"),
+    "bucket_fraction": ("Bucket Fraction", "AreaDefinition"),
+}
+
+
+class GroupingMode(Enum):
+    # Keep in sync with uwsift/ui/open_file_wizard.ui
+    # TODO initialize groupingModeComboBox programmatically
+    BY_GROUP_KEYS = 0
+    KEEP_SEPARATE = 1
+    MERGE_ALL = 2
+
+
+class OpenFileWizard(QtWidgets.QWizard):
+    configured_readers = None
+    inputParametersChanged = QtCore.pyqtSignal()
+    directoryChanged = QtCore.pyqtSignal(str)
+
+    def __init__(self, base_dir=None, base_reader=None, parent=None):
+        super(OpenFileWizard, self).__init__(parent)
+        super(OpenFileWizard, self).__init__(parent)
+
+        self._initial_directory = base_dir
+        self._initial_reader = base_reader
+
+        # tuple(filenames) -> scene object
+        self.scenes = {}
+        self.all_available_products = None
+        self.file_groups = {}
+        self.unknown_files = set()
+
+        # ------------------------------------------------------------------------------------------
+        # GENERAL GUI SETTINGS
+        # ------------------------------------------------------------------------------------------
+
+        # allow maximizing the wizard and minimizing the whole application from wizard window
+        self.setWindowFlags(
+            self.windowFlags()
+            | QtCore.Qt.CustomizeWindowHint
+            | QtCore.Qt.WindowMinimizeButtonHint
+            | QtCore.Qt.WindowMaximizeButtonHint
+            | QtCore.Qt.WindowCloseButtonHint
+        )
+        # enable context menus
+        self.setContextMenuPolicy(QtCore.Qt.ActionsContextMenu)
+        # assume the config doesn't change through the lifetime of the wizard
+        self.config = config.get("open_file_wizard")
+
+        self.ui = Ui_openFileWizard()
+        self.ui.setupUi(self)
+
+        # ------------------------------------------------------------------------------------------
+        # SIGNAL & SLOT CONNECTIONS
+        # ------------------------------------------------------------------------------------------
+
+        # Page 1 - File selection
+
+        # open folder dialog on '...' button click
+        self.ui.selectFolderButton.released.connect(self._open_select_folder_dialog)
+
+        # On reader index change: update filter patterns. Also triggers
+        # input_parameters_changed in the end so that file table is updated with the new pattern.
+        self.ui.readerComboBox.currentIndexChanged.connect(self._update_filter_patterns)
+        self.ui.readerComboBox.currentIndexChanged.connect(self._update_grouping_mode_combobox)
+
+        # on filter pattern (displayed value, as this is editable) change: update file table
+        self.ui.filterPatternComboBox.currentTextChanged.connect(self.inputParametersChanged.emit)
+        # on folder change: update file table
+        self.ui.folderTextBox.textChanged.connect(self.inputParametersChanged.emit)
+        # on input parameter change (e.g.: filter pattern, folder): update file table
+        self.inputParametersChanged.connect(self._update_file_table)
+        # directory was entered or selected with the QFileDialog
+        self.directoryChanged.connect(self._update_input_directory)
+        # on change of selection: group files and check if selection is valid
+        self.ui.fileTable.itemSelectionChanged.connect(self._synchronize_checkmarks_and_check_file_page_completeness)
+        # on change of sorting: temporarily pause sorting while sorted by checked state
+        self.ui.fileTable.horizontalHeader().sortIndicatorChanged.connect(self._file_sorting_changed)
+        # the dialog for selecting a data directory, lazily initialized but to be reused
+        self.file_dialog = None
+
+        # Page 2 - Product selection
+
+        self._all_selected = False
+        self.ui.selectAllButton.clicked.connect(self._select_all_products_state)
+        self.ui.selectIDTable.setContextMenuPolicy(QtCore.Qt.CustomContextMenu)
+        self.ui.selectIDTable.customContextMenuRequested.connect(self._product_context_menu)
+
+        self.ui.resamplingMethodComboBox.currentIndexChanged.connect(self._update_resampling_info)
+        self.ui.resamplingMethodComboBox.currentIndexChanged.connect(self._update_activation_of_projection_combobox)
+
+        self.ui.radiusOfInfluenceSpinBox.valueChanged.connect(self._update_resampling_info)
+
+        self.ui.projectionComboBox.setModel(parent.ui.projectionComboBox.model())
+        self.ui.projectionComboBox.currentIndexChanged.connect(self._update_resampling_info)
+
+        self._update_resampling_shape_spin_boxes()
+        self.ui.projectionComboBox.currentIndexChanged.connect(self._update_resampling_shape_spin_boxes)
+        self.ui.resamplingShapeRowSpinBox.valueChanged.connect(self._update_resampling_info)
+        self.ui.resamplingShapeColumnSpinBox.valueChanged.connect(self._update_resampling_info)
+
+        # GUI has been initialized, make sure we have a consistent
+        # resampling_info
+        self.resampling_info = None
+        self._update_resampling_info()
+
+        # on cell change: check if page is complete
+        self.ui.selectIDTable.cellChanged.connect(self._check_product_page_completeness)
+
+        # wizard state
+        self.file_page_initialized = False
+
+    # ==============================================================================================
+    # PUBLIC GENERAL WIZARD INTERFACE
+    # ==============================================================================================
+
+    def initializePage(self, page_id: int):
+        if page_id == PAGE_ID_FILE_SELECTION:
+            self._initialize_page_file_selection()
+        elif page_id == PAGE_ID_PRODUCT_SELECTION:
+            self._initialize_page_product_selection()
+
+    def validateCurrentPage(self) -> bool:
+        """Check that the current page will generate the necessary data."""
+
+        # start of general validation logic
+        valid = super(OpenFileWizard, self).validateCurrentPage()
+        if not valid:
+            self.ui.statusMessage.setText("")
+            return valid
+
+        page_id: int = self.currentId()
+        if page_id == PAGE_ID_FILE_SELECTION:
+            # Check for completeness when pressing NEXT, and not a lot of times when selection
+            # changes. In case the check fails 'page_complete' is set to False.
+            self._check_selected_files_for_compatibility_with_reader()
+            if not self.ui.fileSelectionPage.page_complete:
+                return False
+
+            # try to create scenes
+            try:
+                self.all_available_products = create_scenes(self.scenes, self.file_groups)
+            except IOError as e:
+                self.ui.statusMessage.setText(f"ERROR: {e}")
+                self.ui.statusMessage.setStyleSheet("color: red")
+                return False
+            except (RuntimeError, ValueError):
+                LOG.error("Could not load files with Satpy reader.")
+                LOG.debug("Could not load files with Satpy reader.", exc_info=True)
+                self.ui.statusMessage.setText("ERROR: Could not load files with specified reader")
+                self.ui.statusMessage.setStyleSheet("color: red")
+                return False
+
+            # ensure at least 1 product could be created
+            if not self.all_available_products:
+                LOG.error("No known products can be loaded from the selected files.")
+                self.ui.statusMessage.setText("ERROR: No known products can be loaded from the selected files.")
+                self.ui.statusMessage.setStyleSheet("color: red")
+                return False
 
-CELL_HEIGHT = 36 if 'darwin' in sys.platform else 48
-CELL_WIDTH = 128 if 'darwin' in sys.platform else 160
-LEFT_OFFSET = 28 if 'darwin' in sys.platform else 32
-TOP_OFFSET = 3
-
-
-class LayerWidgetDelegate(QStyledItemDelegate):
-    """
-    set for a specific column, controls the rendering and editing of items in that column or row of a list or table
-    see QAbstractItemView.setItemDelegateForRow/Column
-    """
-    _doc: Document = None  # document we're representing
-
-    # _doc: DocumentAsLayerStack = None  # document we're representing
-
-    def layer_prez(self, index: int):
-        cll = self._doc.current_layer_set
-        return cll[index] if index < len(cll) and index >= 0 else None
-
-    def __init__(self, doc: Document, *args, **kwargs):
-        super(LayerWidgetDelegate, self).__init__(*args, **kwargs)
-        self._doc = doc
-        # self._doc = doc.as_layer_stack
-        self.font = QFont('Andale Mono')
-        self.font.setPointSizeF(12)
-
-    def sizeHint(self, option: QStyleOptionViewItem, index: QModelIndex):
-        # pz = self.layer_prez(index.row())
-        # if pz.kind == Kind.RGB:
-        #     LOG.debug('triple-sizing composite layer')
-        #     return QSize(CELL_WIDTH, CELL_HEIGHT*3)
-        # else:
-        return QSize(CELL_WIDTH, CELL_HEIGHT)
+        self.ui.statusMessage.setText("")
+        return True
 
-    def displayText(self, *args, **kwargs):
-        return None
+    # ----------------------------------------------------------------------------------------------
+    # PRIVATE GENERAL WIZARD INTERFACE
+    # ----------------------------------------------------------------------------------------------
 
-    def paint(self, painter, option, index):
-        """draw the individual lines in the layers control
-        """
-        painter.save()
+    def _initialize_page_file_selection(self):
+        if self.file_page_initialized:
+            return
 
-        color = QColor(187, 213, 255, 255) if index.row() % 2 == 0 else QColor(177, 223, 255, 255)
-        # color = QColor(187, 213, 255, 255)
-        painter.setPen(QPen(color))
-        painter.setFont(self.font)
-        value = index.data(Qt.UserRole)
-        text = index.data(Qt.DisplayRole)
-        rect = option.rect
-
-        # if we have a value, break out the animation order and other info
-        animation_order = None
-        if value:
-            value, animation_order = value
-
-        # if we have a point probe value, draw the filled bar to represent where it is in that layer's data range
-        if value:
-            value, bar, fmtd_str = value
-            width = bar * float(rect.width())
-            right = QRect(rect.left(), rect.top(), int(width), rect.height())
-            painter.fillRect(right, color)
-
-        super(LayerWidgetDelegate, self).paint(painter, option, index)
-
-        # if this layer is selected, draw a colored rectangle to highlight it
-        if option.state & QStyle.State_Selected and value:
-            painter.fillRect(right, QColor(213, 187, 255, 96))
-
-        # draw the name of the layer
-        painter.setPen(QPen(Qt.black))
-        painter.setFont(self.font)
-        bounds = painter.drawText(rect.left() + LEFT_OFFSET,
-                                  rect.top() + TOP_OFFSET,
-                                  rect.width() - LEFT_OFFSET,
-                                  int(CELL_HEIGHT / 2 - TOP_OFFSET),
-                                  Qt.AlignLeft,
-                                  text,
-                                  )
-
-        # also draw the animation order
-        if animation_order is not None:
-            painter.setPen(QPen(Qt.white))
-            ao_rect = QRect(bounds.right(),
-                            rect.top() + TOP_OFFSET,
-                            rect.width() - bounds.right(),
-                            int(CELL_HEIGHT / 2 - TOP_OFFSET),
-                            )
-            # draw the text once to get the bounding rectangle
-            bounds = painter.drawText(ao_rect,
-                                      Qt.AlignRight,
-                                      str(animation_order + 1),
-                                      )
-            painter.fillRect(bounds, Qt.black)
-            # draw the text a second time to make sure it appears in the rectangle
-            painter.drawText(ao_rect,
-                             Qt.AlignRight,
-                             str(animation_order + 1),
-                             )
-
-        # if we have a point probe value, draw text with it's value
-        if value:
-            painter.setPen(Qt.darkBlue)
-            theight = CELL_HEIGHT / 2
-            top = rect.top() + rect.height() - theight
-            if width < rect.width() / 3:  # place the text to the right of the bar instead of inside
-                left = max(int(width), LEFT_OFFSET)
-                right = rect.width()
-                align = Qt.AlignLeft
+        if not OpenFileWizard.configured_readers:
+            self._update_configured_readers()
+        readers = OpenFileWizard.configured_readers
+
+        reader_to_preselect = self._initial_reader or self.config["default_reader"]
+        for idx, (reader_short_name, reader_name) in enumerate(readers.items()):
+            self.ui.readerComboBox.addItem(reader_short_name, reader_name)
+            if reader_name == reader_to_preselect:
+                self.ui.readerComboBox.setCurrentIndex(idx)
+
+        self.ui.folderTextBox.setText(self._initial_directory)
+        self._update_grouping_mode_combobox()
+        self.file_page_initialized = True
+
+    @classmethod
+    def _update_configured_readers(cls):
+        """Update the list of readers that are both configured and a reader is available from Satpy."""
+        configured_readers = config.get("data_reading.readers", None)
+        readers = available_satpy_readers(as_dict=True)
+        readers = (r for r in readers if not configured_readers or r["name"] in configured_readers)
+        readers = sorted(readers, key=lambda x: x.get("short_name", x["name"]))
+        OpenFileWizard.configured_readers = OrderedDict(
+            (ri.get("short_name", ri["name"]), ri["name"]) for ri in readers
+        )
+
+    def _initialize_page_product_selection(self):
+        # name and level
+        id_components = self.config["id_components"]
+        self.ui.selectIDTable.setColumnCount(len(id_components))
+        self.ui.selectIDTable.setHorizontalHeaderLabels([x.title() for x in id_components])
+        self.ui.selectIDTable.cellChanged.disconnect(self._check_product_page_completeness)
+        for idx, ds_id in enumerate(filter_dataset_ids(self.all_available_products)):
+            col_idx = 0
+            for id_key, id_val, pretty_val in self._pretty_identifiers(ds_id):
+                if id_key not in id_components:
+                    continue
+
+                self.ui.selectIDTable.setRowCount(idx + 1)
+                item = QtWidgets.QTableWidgetItem(pretty_val)
+                item.setData(QtCore.Qt.UserRole, ds_id if col_idx == 0 else id_val)
+                item.setFlags((item.flags() ^ QtCore.Qt.ItemIsEditable) | QtCore.Qt.ItemIsUserCheckable)
+                if id_key == "name":
+                    item.setCheckState(_to_Qt_CheckState(self._all_selected))
+                self.ui.selectIDTable.setItem(idx, col_idx, item)
+                col_idx += 1
+        self.ui.selectIDTable.cellChanged.connect(self._check_product_page_completeness)
+        # resize columns to fit to content (table's sizeAdjustPolicy is set to AdjustToContents)
+        self.ui.selectIDTable.resizeColumnsToContents()
+
+        self.ui.projectionComboBox.setCurrentIndex(self.parent().document.current_projection_index())
+        self._update_resampling_method_combobox()
+        self._update_resampling_info()
+
+    def _pretty_identifiers(self, data_id: DataID) -> Generator[Tuple[str, object, str], None, None]:
+        """Determine pretty version of each identifier."""
+        for key in self.config["id_components"]:
+            value = get_id_value(data_id, key)
+            if value is None:
+                pretty_val = "N/A"
+            elif key == "wavelength":
+                pretty_val = "{:0.02f} m".format(value[1])
+            elif key == "level":
+                pretty_val = "{:d} hPa".format(int(value))
+            elif key == "resolution":
+                pretty_val = "{:d}m".format(int(value))
+            elif key == "calibration" and isinstance(value, Enum):
+                # calibration is an enum in newer Satpy version
+                pretty_val = value.name
+                value = value.name
             else:
-                left = 0
-                right = width
-                align = Qt.AlignRight
-            painter.drawText(left, top, right - left, theight, align, fmtd_str)
-
-        painter.restore()
-
-    # def paint(self, painter, option, index):
-    #     '''
-    #     Paint a checkbox without the label.
-    #     from http://stackoverflow.com/questions/17748546/pyqt-column-of-checkboxes-in-a-qtableview
-    #     '''
-    #     checked = index.model().data(index, QtCore.Qt.DisplayRole) == 'True'
-    #     check_box_style_option = QtGui.QStyleOptionButton()
-    #
-    #     if (index.Flags() & QtCore.Qt.ItemIsEditable) > 0:
-    #         check_box_style_option.state |= QtGui.QStyle.State_Enabled
-    #     else:
-    #         check_box_style_option.state |= QtGui.QStyle.State_ReadOnly
-    #
-    #     if checked:
-    #         check_box_style_option.state |= QtGui.QStyle.State_On
-    #     else:
-    #         check_box_style_option.state |= QtGui.QStyle.State_Off
-    #
-    #     check_box_style_option.rect = self.getCheckBoxRect(option)
-    #
-    #     # this will not run - hasFlag does not exist
-    #     #if not index.model().hasFlag(index, QtCore.Qt.ItemIsEditable):
-    #         #check_box_style_option.state |= QtGui.QStyle.State_ReadOnly
-    #
-    #     check_box_style_option.state |= QtGui.QStyle.State_Enabled
-    #
-    #     QtGui.QApplication.style().drawControl(QtGui.QStyle.CE_CheckBox, check_box_style_option, painter)
-    #
-    # def editorEvent(self, event, model, option, index):
-    #     '''
-    #     Change the data in the model and the state of the checkbox
-    #     if the user presses the left mousebutton or presses
-    #     Key_Space or Key_Select and this cell is editable. Otherwise do nothing.
-    #     '''
-    #     print 'Check Box editor Event detected : '
-    #     if not (index.Flags() & QtCore.Qt.ItemIsEditable) > 0:
-    #         return False
-    #
-    #     print 'Check Box edior Event detected : passed first check'
-    #     # Do not change the checkbox-state
-    #     if event.type() == QtCore.QEvent.MouseButtonRelease or event.type() == QtCore.QEvent.MouseButtonDblClick:
-    #         if event.button() != QtCore.Qt.LeftButton or not self.getCheckBoxRect(option).contains(event.pos()):
-    #             return False
-    #         if event.type() == QtCore.QEvent.MouseButtonDblClick:
-    #             return True
-    #     elif event.type() == QtCore.QEvent.KeyPress:
-    #         if event.key() != QtCore.Qt.Key_Space and event.key() != QtCore.Qt.Key_Select:
-    #             return False
-    #         else:
-    #             return False
-    #
-    #     # Change the checkbox-state
-    #     self.setModelData(None, model, index)
-    #     return True
-
-
-class LayerStackTreeViewModel(QAbstractItemModel):
-    """Behavior connecting list widget to layer stack (both ways)
+                pretty_val = value
 
-    Each table view represents a different configured document layer stack "set" - user can select from at least four.
-    Convey layer set information to/from the document to the respective table, including selection.
+            yield key, value, pretty_val
 
-    References:
+    # ==============================================================================================
+    # PUBLIC CUSTOM INTERFACE
+    # ==============================================================================================
+
+    def collect_selected_ids(self):
+        selected_ids = []
+        prime_key = self.config["id_components"][0]
+        for item_idx in range(self.ui.selectIDTable.rowCount()):
+            id_items = OrderedDict(
+                (key, self.ui.selectIDTable.item(item_idx, id_idx))
+                for id_idx, key in enumerate(self.config["id_components"])
+            )
+            if id_items[prime_key].checkState():
+                data_id = id_items[prime_key]
+                selected_ids.append(data_id.data(QtCore.Qt.UserRole))
+        return selected_ids
 
-        - http://duganchen.ca/a-pythonic-qt-list-model-implementation/
-        - http://doc.qt.io/qt-5/qabstractitemmodel.html#beginMoveRows
-        - http://pyqt.sourceforge.net/Docs/PyQt4/qabstractitemmodel.html
-        - http://doc.qt.io/qt-5/qtwidgets-itemviews-simpletreemodel-example.html
+    @property
+    def files_to_load(self):
+        """Return files that should be used by the Document/Workspace."""
+        return [fn for fgroup in self.file_groups.values() for fn in fgroup]
+
+    def get_reader(self) -> str:
+        return self.ui.readerComboBox.currentData()
+
+    def get_directory(self) -> str:
+        return self.ui.folderTextBox.text()
+
+    # ==============================================================================================
+    # PAGE 1 RELATED FUNCTIONALITY
+    # ==============================================================================================
+
+    def _open_select_folder_dialog(self):
+        """Show folder chooser and update table with files matching the filter pattern."""
+        if not self.file_dialog:
+            self.file_dialog = QtWidgets.QFileDialog(self, "Select Data Directory")
+            self.file_dialog.setFileMode(QtWidgets.QFileDialog.Directory)
+            self.file_dialog.setOption(QtWidgets.QFileDialog.ShowDirsOnly, True)  # Must come after setFileMode()
+
+            # Try enabling directory tree navigation in the file dialog.
+            # Qt prefers to use the platform native file dialog. If this dialog
+            # is not based on Qt (e.g. on Windows), the next call returns None,
+            # then we have to live with what the file dialog of the platform
+            # provides.
+            tree = self.file_dialog.findChild(QtWidgets.QTreeView)
+            if tree:
+                tree.setRootIsDecorated(True)
+                tree.setItemsExpandable(True)
 
-    """
-    widgets = None
-    doc = None
-    item_delegate = None
-    _last_equalizer_values = {}
-    _mimetype = 'application/vnd.row.list'
+            if self._initial_directory:
+                self.file_dialog.setDirectory(self._initial_directory)
+            else:
+                home_dir = os.getenv("HOME")
+                if home_dir:
+                    self.file_dialog.setDirectory(home_dir)
+
+            self.file_dialog.currentChanged.connect(self.directoryChanged)
+            self.file_dialog.directoryEntered.connect(self.directoryChanged)
+            self.file_dialog.fileSelected.connect(self.directoryChanged)
+            self.file_dialog.fileSelected.connect(self.file_dialog.setDirectory)
+
+        self.file_dialog.open()
+
+    def _update_input_directory(self, path: str):
+        # The DirectoryOnly FileMode is obsolete and the Directory FileMode
+        # doesn't work with the ShowDirsOnly option. Thus, the user is able to
+        # select regular files. Filter these paths from the currentChanged
+        # event.
+        if not os.path.isdir(path):
+            return
 
-    # signals
-    uuidSelectionChanged = pyqtSignal(tuple)  # the list is a list of the currently selected UUIDs
-    didRequestRGBCreation = pyqtSignal(dict)
+        # Don't update the table twice in the following case: The user may
+        # select the directory with a single click (currentChanged) and then
+        # enter the directory with a double click (directoryEntered).
+        if path != self._initial_directory:
+            self._initial_directory = path
+            self.ui.folderTextBox.setText(path)
+            self.inputParametersChanged.emit()
+
+    def _update_filter_patterns(self):
+        """Updates available file filter patterns by reading the config. Selects first entry."""
+        reader = self.ui.readerComboBox.currentData()
 
-    def __init__(self, widgets: list, doc: Document, parent=None):
-        """
-        Connect one or more table views to the document via this model.
-        :param widgets: list of TableViews to wire up
-        :param doc: document to communicate with
-        :return:
-        """
-        super(LayerStackTreeViewModel, self).__init__(parent)
+        if reader is None:
+            return
 
-        self.widgets = []
-        self.doc = doc
-        # self._column = [self._visibilityData, self._nameData]
-        self.item_delegate = LayerWidgetDelegate(doc)
-        # FIXME: Reset colormap change dialog when layer set changes
-
-        # for now, a copout by just having a refresh to the content when document changes
-        doc.didReorderLayers.connect(self.refresh)
-        doc.didRemoveLayers.connect(self.drop_layers_just_removed)
-        doc.didChangeColormap.connect(self.refresh)
-        # doc.didChangeColorLimits.connect(self.refresh)
-        doc.didChangeLayerVisibility.connect(self.refresh)
-        doc.didChangeLayerName.connect(self.refresh)
-        doc.didAddBasicLayer.connect(self.doc_added_basic_layer)
-        doc.didAddCompositeLayer.connect(self.refresh)
-        doc.willPurgeLayer.connect(self.refresh)
-        doc.didSwitchLayerSet.connect(self.refresh)
-        doc.didReorderAnimation.connect(self.refresh)
-        doc.didCalculateLayerEqualizerValues.connect(self.update_equalizer)
-
-        # self.setSupportedDragActions(Qt.MoveAction)
-
-        # set up each of the widgets
-        for widget in widgets:
-            self._init_widget(widget)
-
-    # TODO, this wrapper is probably not needed, possibly remove later
-    def add_widget(self, listbox: QTreeView):
-        self._init_widget(listbox)
-
-    def _init_widget(self, listbox: QTreeView):
-        listbox.setModel(self)
-        listbox.setItemDelegate(self.item_delegate)
-        listbox.setContextMenuPolicy(Qt.CustomContextMenu)
-        # listbox.customContextMenuRequested.connect(self.context_menu)
-        listbox.customContextMenuRequested.connect(self.menu)
-        listbox.setDragEnabled(True)
-        listbox.setAcceptDrops(True)
-        listbox.setDropIndicatorShown(True)
-        listbox.setSelectionMode(listbox.ExtendedSelection)
-        # listbox.setMovement(QTreeView.Snap)
-        # listbox.setDragDropMode(QTreeView.InternalMove)
-        listbox.setDragDropMode(QAbstractItemView.DragDrop)
-        # listbox.setAlternatingRowColors(True)
-        # listbox.setDefaultDropAction(Qt.MoveAction)
-        # listbox.setDragDropOverwriteMode(True)
-        # listbox.entered.connect(self.layer_entered)
-        # listbox.setFont(QFont('Andale Mono', 13))
-
-        # the various signals that may result from the user changing the selections
-        # listbox.activated.connect(self.changedSelection)
-        # listbox.clicked.connect(self.changedSelection)
-        # listbox.doubleClicked.connect(self.changedSelection)
-        # listbox.pressed.connect(self.changedSelection)
-        listbox.selectionModel().selectionChanged.connect(self.changedSelection)
+        filter_patterns = config.get(DATA_READING_CONFIG_KEY + "." + reader + "." + "filter_patterns", None)
+        # Example value:
+        # filter_patterns = ['{rate:1s}-000-{hrit_format:_<6s}-{platform_shortname:4s}_{service:_<7s}'
+        #                  '-{channel:_<6s}___-{segment:_<6s}___-{start_time:%Y%m%d%H%M}-{:1s}_']
+        if filter_patterns is None:
+            filter_patterns = []
 
-        self.widgets.append(listbox)
+        # always append wildcard to show all files, i.e. not filter at all
+        filter_patterns = filter_patterns.copy()
+        filter_patterns.append("")
 
-    # def supportedDragActions(self):
-    #     return Qt.MoveAction
-    #
-    def supportedDropActions(self):
-        return Qt.MoveAction | Qt.LinkAction
+        self.ui.filterPatternComboBox.clear()
+        self.ui.filterPatternComboBox.addItems(filter_patterns)
 
-    def changedSelection(self, *args):
-        """connected to the various listbox signals that represent the user changing selections
-        """
-        selected_uuids = tuple(self.current_selected_uuids(self.current_set_listbox))
-        # FUTURE: this is needed in order to prevent selection display artifacts. why?
-        self.current_set_listbox.update()
-        self.uuidSelectionChanged.emit(selected_uuids)
+        self.inputParametersChanged.emit()
 
-    @property
-    def current_set_listbox(self):
+    def _update_file_table(self):
         """
-        We can have several list boxes, one for each layer_set in the document.
-        Return whichever one is currently active.
-        :return:
+        Clears and re-populates the file table. Columns may be removed/added.
+        This method considers the selected reader, filter pattern and folder.
         """
-        # FUTURE this is brute force and could be tracked
-        for widget in self.widgets:
-            if widget.isVisible():
-                return widget
-
-    def doc_added_basic_layer(self, new_order, layer, presentation):
-        # dexes = [i for i,q in enumerate(new_order) if q==None]
-        # for dex in dexes:
-        #     self.beginInsertRows(QModelIndex(), dex, dex)
-        #     self.endInsertRows()
-        self.refresh()
-
-    def refresh(self):
-        # self.beginResetModel()
-        # self.endResetModel()
-        self.layoutAboutToBeChanged.emit()
-        self.revert()
-        self.layoutChanged.emit()
 
-        # this is an ugly way to make sure the selection stays current
-        try:
-            self.changedSelection(None)
-            self.current_set_listbox.update()
-        except IndexError:
-            pass
-
-    def current_selected_uuids(self, lbox: QTreeView = None):
-        lbox = self.current_set_listbox if lbox is None else lbox
-        if lbox is None:
-            LOG.error('not sure which list box is active! oh pooh.')
-            return
-        for q in lbox.selectedIndexes():
-            yield self.doc.uuid_for_current_layer(q.row())
+        self.ui.statusMessage.setText("")
 
-    def select(self, uuids, lbox: QTreeView = None, scroll_to_show_single=True):
-        lbox = self.current_set_listbox if lbox is None else lbox
-        lbox.clearSelection()
-        if not uuids:
-            return
-        # FUTURE: this is quick and dirty
-        rowdict = dict((u, i) for i, u in enumerate(self.doc.current_layer_uuid_order))
-        items = QItemSelection()
-        q = None
-        for uuid in uuids:
-            row = rowdict.get(uuid, None)
-            if row is None:
-                LOG.error('UUID {} cannot be selected in list view'.format(uuid))
-                continue
-            q = self.createIndex(row, 0)
-            items.select(q, q)
-            lbox.selectionModel().select(items, QItemSelectionModel.Select)
-            # lbox.setCurrentIndex(q)
-        if scroll_to_show_single and len(uuids) == 1 and q is not None:
-            lbox.scrollTo(q)
+        table = self.ui.fileTable
 
-    def drop_layers_just_removed(self, layer_indices, uuid, row, count):
-        """
-        a layer was removed in the document, update the listview
-        :param layer_indices: list of new layer indices
-        :param uuid:
-        :return:
-        """
-        self.refresh()
-        # self.removeRows(row, count)
+        # clear table: remove all rows
+        table.setRowCount(0)
 
-    def update_equalizer(self, doc_values):
-        """
-        User has clicked on a point probe
-        Document is conveniently providing us values for all the image layers
-        Let's update the display to show them like a left-to-right equalizer
-        :param doc_values: {uuid: value, value-relative-to-base, is-base:bool}, values can be NaN
-        :return:
-        """
-        if not doc_values:
-            # turn off all equalizer values
-            self._last_equalizer_values = {}
+        # retrieve column names from pattern, prepend 2 columns for selection state and filename
+        filter_pattern = self.ui.filterPatternComboBox.currentText()
+        pattern_convert_dict = None
+        try:
+            pattern_convert_dict = fnparser.get_convert_dict(filter_pattern)
+        except ValueError:
+            LOG.error(f"Invalid filter pattern: {filter_pattern}")
+            self.ui.statusMessage.setText("Invalid filter pattern")
+            self.ui.statusMessage.setStyleSheet("color: red")
+
+        column_names = list((CHECKMARK, "Filename"))
+        if pattern_convert_dict is not None:
+            column_names.extend([key for key in pattern_convert_dict if len(key) > 0])
+
+        # update columns
+        table.setColumnCount(len(column_names))
+        table.setHorizontalHeaderLabels([c for c in column_names])
+
+        # find files in selected folder
+        folder = self.ui.folderTextBox.text()
+        if os.path.exists(folder):
+            # Inserting items while sorting is active may lead to inconsistent table data.
+            # It is suggested to disable sorting while inserting items when using a table widget.
+            table_sorting_enabled = table.isSortingEnabled()
+            table.setSortingEnabled(False)
+            for file in os.listdir(folder):
+                self._add_row_to_file_table(column_names, file, filter_pattern, pattern_convert_dict, table)
+            table.setSortingEnabled(table_sorting_enabled)
+
+        # Initially (and if no sorting is applied), sort by filename (column: 1)
+        if table.horizontalHeader().sortIndicatorSection() >= table.columnCount():
+            self.ui.fileTable.sortByColumn(1, QtCore.Qt.AscendingOrder)
+
+        # resize columns to fit content (table's sizeAdjustPolicy is set to AdjustToContents)
+        table.resizeColumnsToContents()
+
+    def _add_row_to_file_table(self, column_names, file, filter_pattern, pattern_convert_dict, table):
+        try:
+            if len(pattern_convert_dict) == 0:
+                # if pattern is empty, show all files
+                table.insertRow(table.rowCount())
+                table.setItem(table.rowCount() - 1, 1, QtWidgets.QTableWidgetItem(file))
+            elif pattern_convert_dict is None or fnparser.validate(filter_pattern, file):
+                # if pattern matches, add more columns, and show error when pattern is invalid
+                table.insertRow(table.rowCount())
+                table.setItem(table.rowCount() - 1, 1, QtWidgets.QTableWidgetItem(file))
+                p = fnparser.parse(filter_pattern, file)
+                for col in range(2, len(column_names)):
+                    table.setItem(
+                        table.rowCount() - 1, col, QtWidgets.QTableWidgetItem(str(p.get(column_names[col], "")))
+                    )
+        except Exception:  # FIXME: Don't catch generic Exception
+            # As the error thrown by trollsift's validate function in case of an
+            # unparsable pattern has no class, a general 'Exception' is caught although
+            # this is not PEP8-compliant.
+            LOG.error(f"Invalid filter pattern: {filter_pattern}")
+            self.ui.statusMessage.setText("Invalid filter pattern")
+            self.ui.statusMessage.setStyleSheet("color: red")
+
+    def _file_sorting_changed(self, logical_index, order):
+        """
+        Pause sorting after sorting by checked state until choosing another column.
+
+        When sorting by checked state is active, clicking a row will lead to reordering of the
+        table as the checked state of that row changes. To prevent confusion sorting needs to be
+        paused until the user clicks another column to sort by. Clicking the checked state column
+        itself will still work as usual and allow to sort asc/desc.
+        """
+
+        if logical_index == 0:
+            # actually sort once, then disable sorting temporarily
+            self.ui.fileTable.sortByColumn(logical_index, order)
+            self.ui.fileTable.setSortingEnabled(False)
         else:
-            self._last_equalizer_values.update(doc_values)
-        self.refresh()
+            # activate sorting when choosing another column
+            self.ui.fileTable.setSortingEnabled(True)
 
-    def change_layer_colormap_menu(self, menu, lbox: QTreeView, selected_uuids: list, *args):
-        def _show_change_colormap_dialog(action):
-            d = ChangeColormapDialog(self.doc, selected_uuids[0], parent=lbox)
-            d.show()
-            d.raise_()
-            d.activateWindow()
+    def _synchronize_checkmarks_and_check_file_page_completeness(self, *args, **kwargs):
+        """update status message, check if this page is complete."""
 
-        action = menu.addAction('Change Colormap...')
-        return {action: _show_change_colormap_dialog}
+        self.ui.fileSelectionPage.page_complete = False
 
-    def composite_layer_menu(self, menu, lbox: QTreeView, selected_uuids: list, *args):
-        """
-        provide common options for RGB or other composite layers, eventually with option to go to a compositing dialog
+        # synchronize selection with checkmarks to enable sorting by marked files
+        indices = self.ui.fileTable.selectionModel().selectedRows()
+        for r in range(self.ui.fileTable.rowCount()):
+            if r in [index.row() for index in indices]:
+                self.ui.fileTable.setItem(r, 0, QtWidgets.QTableWidgetItem(CHECKMARK))
+                self.ui.fileSelectionPage.page_complete = True
+            else:
+                self.ui.fileTable.setItem(r, 0, QtWidgets.QTableWidgetItem(""))
 
-        """
-        actions = {}
-        requests = {}
-        if len(selected_uuids) > 3 or \
-                any(self.doc[u][Info.KIND] not in [Kind.IMAGE, Kind.COMPOSITE] for u in selected_uuids):
-            LOG.warning('Need 3 image layers to create a composite')
-            return {}
-
-        def _make_rgb_composite(action, requests=requests):
-            request = requests.get(action, None)
-            if request is not None:
-                LOG.debug('RGB creation using {0!r:s}'.format(request))
-                self.didRequestRGBCreation.emit(request)
-
-        rgb_menu = QMenu("Create RGB From Selections...", menu)
-        for rgb in sorted(set(x[:len(selected_uuids)] for x in ['RGB', 'RBG', 'GRB', 'GBR', 'BRG', 'BGR']),
-                          reverse=True):
-            # Only include the number of channels selected
-            rgb = rgb[:len(selected_uuids)]
-            action = rgb_menu.addAction(rgb)
-            request = dict((channel.lower(), uuid) for (channel, uuid) in zip(rgb, selected_uuids))
-            actions[action] = _make_rgb_composite
-            requests[action] = request
-        menu.addMenu(rgb_menu)
-        return actions
-
-    def change_layer_image_kind_menu(self, menu, lbox, selected_uuids, *args):
-        current_kind = self.doc.prez_for_uuid(selected_uuids[0]).kind
-        kind_menu = QMenu("Change Image Kind", menu)
-        action_group = QActionGroup(menu, exclusive=True)
-        actions = {}
-        action_kinds = {}
-
-        def _change_layers_image_kind(action, action_kinds=action_kinds):
-            if not action.isChecked():
-                # can't uncheck an image kind
-                LOG.debug("Selected Kind action is not checked")
-                return
-            kind = action_kinds[action]
-            return self.doc.change_layers_image_kind(selected_uuids, kind)
-
-        for kind in [Kind.IMAGE, Kind.CONTOUR]:
-            action = action_group.addAction(QAction(kind.name, menu, checkable=True))
-            action_kinds[action] = kind
-            action.setChecked(kind == current_kind)
-            actions[action] = _change_layers_image_kind
-            kind_menu.addAction(action)
-
-        menu.addMenu(kind_menu)
-        return actions
-
-    def menu(self, pos: QPoint, *args):
-        lbox = self.current_set_listbox
-        selected_uuids = list(self.current_selected_uuids(lbox))
-        LOG.debug("selected UUID set is {0!r:s}".format(selected_uuids))
-        menu = QMenu()
-        actions = {}
-        if len(selected_uuids) == 1:
-            if self.doc[selected_uuids[0]][Info.KIND] in [Kind.IMAGE, Kind.COMPOSITE, Kind.CONTOUR]:
-                actions.update(self.change_layer_colormap_menu(menu, lbox, selected_uuids, *args))
-            if self.doc[selected_uuids[0]][Info.KIND] in [Kind.CONTOUR]:
-                actions.update(self.change_layer_image_kind_menu(menu, lbox, selected_uuids, *args))
-        if 0 < len(selected_uuids) <= 3:
-            if all(self.doc[u][Info.KIND] in [Kind.IMAGE, Kind.COMPOSITE]
-                   for u in selected_uuids):
-                actions.update(self.composite_layer_menu(
-                    menu, lbox, selected_uuids, *args))
-
-        if not actions:
-            action = menu.addAction("No actions available for this layer")
-            action.setEnabled(False)
+        self.ui.fileSelectionPage.completeChanged.emit()
 
-        sel = menu.exec_(lbox.mapToGlobal(pos))
-        if sel is None:
-            return
-        elif sel in actions:
-            return actions[sel](sel)
+    def _check_selected_files_for_compatibility_with_reader(self):
+        """TODO: description"""
+        self.ui.statusMessage.setText("Checking file/reader compatibility...")
+        self.ui.statusMessage.setStyleSheet("color: black")
+        reader = self.ui.readerComboBox.currentData()
+        groups_updated = self._group_files(reader)
+        if groups_updated:
+            self._mark_unknown_files()
+        if not self.file_groups:
+            # if none of the files were usable then the user can't click Next
+            self.ui.fileSelectionPage.page_complete = False
+            LOG.error("Could not load any file with specified reader.")
+            self.ui.statusMessage.setText("ERROR: Could not load any file with specified reader")
+            self.ui.statusMessage.setStyleSheet("color: red")
         else:
-            LOG.debug("Unimplemented menu option '{}'".format(sel.text()))
+            self.ui.statusMessage.setText("")
 
-    def columnCount(self, QModelIndex_parent=None, *args, **kwargs):
-        return 1
+    def _group_files(self, reader) -> bool:
+        """Group provided files by some keys, especially time step."""
 
-    #
-    # def hasChildren(self, QModelIndex_parent=None, *args, **kwargs):
-    #     return False  # FIXME
+        # reset state
+        self.scenes = {}
+        self.file_groups = {}
+        self.unknown_files = set()
+
+        # get filenames from table's 'Filename' column
+        # TODO: in future, use a data model for the table and get filenames from there
+        folder = self.ui.folderTextBox.text()
+        selected_items = set(
+            [
+                os.path.join(folder, self.ui.fileTable.item(r.row(), 1).text())
+                for r in self.ui.fileTable.selectionModel().selectedRows()
+            ]
+        )
 
-    def headerData(self, section: int, Qt_Orientation, role=None):
-        pass
+        # if there's nothing to group, return
+        if len(selected_items) == 0:
+            return True
 
-    @property
-    def listing(self):
-        return [self.doc.get_info(dex) for dex in range(len(self.doc))]
+        selected_files_from_items = []
+        for selected_item in selected_items:
+            if os.path.isdir(selected_item):
+                files_in_dir = find_files_and_readers(base_dir=selected_item, reader=reader)
+                selected_files_from_items.extend(files_in_dir[reader])
+            else:
+                selected_files_from_items.extend([selected_item])
+        selected_files = set(selected_files_from_items)
 
-    def dropMimeData(self, mime: QMimeData, action, row: int, column: int, parent: QModelIndex):
-        LOG.debug('dropMimeData at row {}'.format(row))
-        if action == Qt.IgnoreAction:
-            return True
+        # Read group_keys from SIFT reader-specific config. If not present, Satpy's config is used.
+        group_keys = config.get(DATA_READING_CONFIG_KEY + "." + reader + ".group_keys", None)
 
-        if mime.hasFormat('text/uri-list'):
-            if mime.hasUrls():
-                LOG.debug('found urls in drop!')
-                paths = [qurl.path() for qurl in mime.urls() if qurl.isLocalFile()]
-                self.doc.import_files(paths)  # FIXME: replace with a signal
-                return True
-        elif mime.hasFormat(self._mimetype):
-            # unpickle the presentation information and re-insert it
-            # b = base64.decodebytes(mime.text())
-            b = mime.data(self._mimetype)
-            layer_set_len, insertion_info = pkl.loads(b)
-            LOG.debug('dropped: {0!r:s}'.format(insertion_info))
-            count = len(insertion_info)
-            if row == -1:
-                row = len(self.doc)  # append
-                # FIXME: row=col=-1 implies drop-on-parent
-                #  which may mean replace or may mean append for composite layers
-            # self.insertRows(row, count)
-            # for i, presentation in enumerate(l):
-            #     self.setData(self.index(row+i, 0), presentation)
-            order = list(range(layer_set_len))
-            inserted_row_numbers = []
-            # inserted_presentations = []
-            # delete_these_rows = []
-            insertion_point = row
-            uuids = []
-            for old_row, presentation in reversed(sorted(insertion_info)):
-                del order[old_row]
-                if old_row < insertion_point:
-                    insertion_point -= 1
-                inserted_row_numbers.insert(0, old_row)
-                uuids.append(presentation.uuid)
-                # delete_these_rows.append(old_row if old_row<row else old_row+count)
-                # inserted_presentations.append(presentation)
-            order = order[:insertion_point] + inserted_row_numbers + order[insertion_point:]
-            LOG.debug('new order after drop {0!r:s}'.format(order))
-            self.select([])
-            self.doc.reorder_by_indices(order)
-            # self.doc.insert_layer_prez(row, inserted_presentations)
-            # LOG.debug('after insertion removing rows {0!r:s}'.format(delete_these_rows))
-            # for exrow in delete_these_rows:
-            #     self.doc.remove_layer_prez(exrow)
-            # self.doc.didReorderLayers.emit(order)  # FUTURE: not our business to be emitting on behalf of the document
-            assert (count == len(insertion_info))
-            return True
-        return False
-        # return super(LayerStackListViewModel, self).dropMimeData(mime, action, row, column, parent)
+        grouping_mode = GroupingMode(self.ui.groupingModeComboBox.currentIndex())
 
-    def mimeData(self, list_of_QModelIndex):
-        valid_rows = []
-        for index in list_of_QModelIndex:
-            # create a list of Presentation tuples showing how layers are presented
-            if index.isValid():
-                valid_rows.append((index.row(), self.doc.current_layer_set[index.row()]))
-        p = pkl.dumps((len(self.doc.current_layer_set), valid_rows), pkl.HIGHEST_PROTOCOL)
-        mime = QMimeData()
-        # t = base64.encodebytes(p).decode('ascii')
-        # LOG.debug('mimetext for drag is "{}"'.format(t))
-        mime.setData(self._mimetype, p)
-        LOG.debug('presenting mime data for {0!r:s}'.format(valid_rows))
-        return mime
-
-    def mimeTypes(self):
-        return ['text/uri-list',
-                self._mimetype]  # ref https://github.com/shotgunsoftware/pyqt-uploader/blob/master/uploader.py
-
-    # http://stackoverflow.com/questions/6942098/qt-qtreeview-only-allow-to-drop-on-an-existing-item
-    # Reimplement the Flags method of the underlying model to return
-    #     Qt::ItemIsDropEnabled only if passed index is valid.
-    # When in between items, Flags() is called with an invalid index so I can decide not to accept the drop
-    # For the inverse you can do this:
-    #     if ( index.isValid() ) { return Qt::ItemIsSelectable | Qt::ItemIsEnabled | Qt::ItemIsDragEnabled; }
-    # else { return Qt::ItemIsSelectable | Qt::ItemIsDragEnabled | Qt::ItemIsDropEnabled | Qt::ItemIsEnabled; }
-    def flags(self, index):
-        # Flags = super(LayerStackListViewModel, self).Flags(index)
-        if index.isValid():
-            flags = (Qt.ItemIsEnabled |
-                     Qt.ItemIsSelectable |
-                     Qt.ItemIsDragEnabled |
-                     Qt.ItemIsUserCheckable |
-                     Qt.ItemIsEditable)
+        file_groups = None
+        if grouping_mode == GroupingMode.BY_GROUP_KEYS:
+            file_groups = group_files(selected_files, reader=reader, group_keys=group_keys)
+        elif grouping_mode == GroupingMode.MERGE_ALL:
+            file_groups = [{reader: list(selected_files)}]
         else:
-            flags = Qt.ItemIsDropEnabled
-        return flags
+            file_groups = [{reader: [file]} for file in selected_files]
 
-    def hasIndex(self, row, col, QModelIndex_parent=None, *args, **kwargs):
-        if QModelIndex_parent.isValid() or col != 0:
-            # then look up whether this layer has child layers, e.g. for RGB or algebraic
-            # actually: current layer list ignores RGB/algebraic child layers
-            return False
-        return (row >= 0 and row < len(self.doc))
-
-    def rowCount(self, QModelIndex_parent=None, *args, **kwargs):
-        # LOG.debug('{} layers'.format(len(self.doc)))
-        if QModelIndex_parent is None or QModelIndex_parent == QModelIndex():
-            return len(self.doc)
-        return 0
-
-    def index(self, row, column, parent):
-        if self.hasIndex(row, column, parent):
-            return self.createIndex(row, column, parent)
-        else:
-            return QModelIndex()
+        if not file_groups:
+            self.unknown_files = selected_files
+            self.file_groups = {}
+            return True
 
-    def parent(self, index=None):
-        return QModelIndex()
-        # FIXME
-        # if not index.isValid():
-        #     return QModelIndex()
-        #
-        # childItem = index.internalPointer()
-        # if not childItem:
-        #     return QModelIndex()
-        #
-        # parentItem = childItem.parent()
-        #
-        # if parentItem == self.rootItem:
-        #     return QModelIndex()
-        #
-        # return self.createIndex(parentItem.row(), 0, parentItem)
-
-    def data(self, index: QModelIndex, role: int = None):
-        if not index.isValid():
-            return None
-        row = index.row()
-        # LOG.debug("getting data for row %d" % row)
-        # col = index.column()
-        el = self.listing
-        info = el[row] if row < len(self.doc) else None
-        if not info:
-            return None
-
-        # pass auxiliary info about the layer through the Qt.UserRole for use when displaying
-        eq_content = self._last_equalizer_values.get(info[Info.UUID], None)
-        if role == Qt.UserRole:
-            # get the animation order also
-            animation_order = self.doc.layer_animation_order(row)
-            return (eq_content, animation_order)
-
-        elif role == Qt.EditRole:
-            return self.doc.current_layer_set[index.row()] if index.row() < len(self.doc) else None
-        elif role == Qt.CheckStateRole:
-            check = Qt.Checked if self.doc.is_layer_visible(row) else Qt.Unchecked
-            return check
-        elif role == Qt.ToolTipRole:
-            if not eq_content:
-                return None
-            value, normalized = eq_content[:2]
-            return str(value)
-        elif role == Qt.DisplayRole:
-            # lao = self.doc.layer_animation_order(row)
-            name = info[Info.DISPLAY_NAME]
-            # return  ('[-]  ' if lao is None else '[{}]'.format(lao+1)) + el[row]['name']
-            # if leroy:
-            #     data = '[%.2f] ' % leroy[0]
-            #     return data + name
-            return name
-        return None
-
-    def setData(self, index: QModelIndex, data, role: int = None):
-        LOG.debug('setData {0!r:s}'.format(data))
-        if not index.isValid():
-            return False
-        if role == Qt.EditRole:
-            if isinstance(data, str):
-                LOG.debug("changing row {0:d} name to {1!r:s}".format(index.row(), data))
-                self.doc.change_layer_name(index.row(), data)
-                self.dataChanged.emit(index, index)
-                return True
+        scenes: dict = {}  # recreate Scene dictionary
+        file_group_map = {}
+        known_files: set = set()
+        for file_group in file_groups:
+            # file_group includes what reader to use
+            # NOTE: We only allow a single reader at a time
+            group_id = tuple(sorted(fn for group_list in file_group.values() for fn in group_list))
+            known_files.update(group_id)
+            if group_id not in self.scenes:
+                # never seen this exact group of files before
+                scenes[group_id] = None  # filled in later
             else:
-                LOG.debug("data type is {0!r:s}".format(type(data)))
-        elif role == Qt.CheckStateRole:
-            newvalue = True if data == Qt.Checked else False
-            LOG.debug('toggle layer visibility for row {} to {}'.format(index.row(), newvalue))
-            self.doc.toggle_layer_visibility(index.row(), newvalue)
-            self.dataChanged.emit(index, index)
-            return True
-        elif role == Qt.ItemDataRole:
-            LOG.warning('attempting to change layer')
-            # self.doc.replace_layer()
-            # FIXME implement this
-            self.dataChanged.emit(index, index)
-            return True
-        elif role == Qt.DisplayRole:
-            if index.isValid():
-                LOG.debug("changing row {0} name to {1!r:s}".format(index.row(), data))
-                self.doc.change_layer_name(index.row(), data)
-                return True
-        return False
-
-    def insertRows(self, row, count, parent=None):
-        if parent is None:
-            parent = QModelIndex()
-        self.beginInsertRows(QModelIndex(), row, row + count - 1)
-        LOG.debug(">>>> INSERT {} rows".format(count))
-        # TODO: insert 'count' empty rows into document
-        self.endInsertRows()
+                scenes[group_id] = self.scenes[group_id]
+            file_group_map[group_id] = file_group
+        self.scenes = scenes
+        self.file_groups = file_group_map
+        self.unknown_files = selected_files - known_files
         return True
 
-    def removeRows(self, row, count, QModelIndex_parent=None, *args, **kwargs):
-        self.beginRemoveRows(QModelIndex(), row, row + count - 1)
-        LOG.debug(">>>> REMOVE {} rows at {}".format(count, row))
-        # self.doc.remove_layer_prez(row, count)
-        self.endRemoveRows()
-        return True
+    def _mark_unknown_files(self):
+        """Mark rows unknown to the reader in red color and remove selection"""
+        unknown_filenames = [os.path.basename(f) for f in self.unknown_files]
+        for r in range(self.ui.fileTable.rowCount()):
+            filename = self.ui.fileTable.item(r, 1).text()
+            if filename in unknown_filenames:
+                # change currently selected row visually (allows change of background/foreground)
+                self.ui.fileTable.setCurrentCell(-1, -1)
+                # remove selection from model
+                self.ui.fileTable.item(r, 0).setSelected(False)
+                # change visual representation of all cells in this row
+                for c in range(self.ui.fileTable.columnCount()):
+                    self.ui.fileTable.item(r, c).setForeground(QtGui.QColor(255, 0, 0))
+
+    # ==============================================================================================
+    # PAGE 2 RELATED FUNCTIONALITY
+    # ==============================================================================================
+
+    def _select_all_products_state(self, checked: bool):
+        """Select all or deselect all products listed on the product table."""
+        # the new state (all selected or all unselected)
+        self._all_selected = not self._all_selected
+        self._select_all_products(select=self._all_selected)
+
+    def _select_all_products(self, select=True, prop_key: Union[str, None] = None, prop_val: Union[str, None] = None):
+        """Select products based on a specific property."""
+        for row_idx in range(self.ui.selectIDTable.rowCount()):
+            # our check state goes on the name item (always)
+            name_item = self.ui.selectIDTable.item(row_idx, 0)
+            if prop_key is not None:
+                item_id = name_item.data(QtCore.Qt.UserRole)
+                if get_id_value(item_id, prop_key) != get_id_value(prop_val, prop_key):
+                    continue
+            name_item.setCheckState(_to_Qt_CheckState(select))
+
+    def _product_context_menu(self, position: QPoint):
+        item = self.ui.selectIDTable.itemAt(position)
+        col = item.column()
+        id_comp = self.config["id_components"][col]
+        # first column always has DataID
+        id_data = self.ui.selectIDTable.item(item.row(), 0).data(QtCore.Qt.UserRole)
+        menu = QMenu()
+        select_action = menu.addAction("Select all by '{}'".format(id_comp))
+        deselect_action = menu.addAction("Deselect all by '{}'".format(id_comp))
+        action = menu.exec_(self.ui.selectIDTable.mapToGlobal(position))
+        if action == select_action or action == deselect_action:
+            select = action == select_action
+            self._select_all_products(select=select, prop_key=id_comp, prop_val=id_data)
+
+    def _check_product_page_completeness(self):
+        """update status message, check if this page is complete."""
+
+        self.ui.productSelectionPage.page_complete = False
+
+        for row_idx in range(self.ui.selectIDTable.rowCount()):
+            item = self.ui.selectIDTable.item(row_idx, 0)
+            # if at least 1 item is checked
+            if item is not None and item.checkState():
+                self.ui.productSelectionPage.page_complete = True
+                break
+
+        self.ui.productSelectionPage.completeChanged.emit()
+
+    def _update_resampling_method_combobox(self):
+        reader = self.get_reader()
+        geometry_definition: str = config.get(f"data_reading.{reader}.geometry_definition", "AreaDefinition")
+
+        previous_resampling_method = self.ui.resamplingMethodComboBox.currentData()
+
+        self.ui.resamplingMethodComboBox.blockSignals(True)
+        self.ui.resamplingMethodComboBox.clear()
+
+        cb_model = self.ui.resamplingMethodComboBox.model()
+
+        known_resampling_methods = ["none"]
+        known_resampling_methods.extend(satpy.resample.RESAMPLERS)
+        first_enabled_item_index = -1
+        for resampling_method in known_resampling_methods:
+            configuration = RESAMPLING_METHODS.get(resampling_method, None)
+            if configuration == Conf.SKIP:
+                continue
+            # If not explicitly skipped but also not "configured" in
+            # RESAMPLING_METHODS add the resampler's ID, disabled (see below).
+            # This makes "unknown" (newly added to Satpy) resamplers show up
+            # drawing attention to ask a developer to test and enable them.
+            resampling_method_name = configuration[0] if configuration else resampling_method
+            self.ui.resamplingMethodComboBox.addItem(resampling_method_name, userData=resampling_method)
+
+            # Check, whether current item is approved for detected geometry
+            # (area or swath). Disable if not and make sure the first enabled
+            # item is preselected or - if an attempt is to be made to do so,
+            # preselect the previously selected item, if it is enabled.
+            item_index = self.ui.resamplingMethodComboBox.count() - 1
+            if not configuration or geometry_definition not in configuration:
+                item = cb_model.item(item_index)
+                item.setEnabled(False)
+            else:
+                if first_enabled_item_index < 0:
+                    first_enabled_item_index = item_index
+                    self.ui.resamplingMethodComboBox.setCurrentIndex(first_enabled_item_index)
+                if resampling_method == previous_resampling_method:
+                    self.ui.resamplingMethodComboBox.setCurrentIndex(item_index)
+
+        self._set_opts_disabled(self.ui.resamplingMethodComboBox.currentData() == "none")
+        self.ui.resamplingMethodComboBox.blockSignals(False)
+
+    def _update_activation_of_projection_combobox(self):
+        if self.ui.resamplingMethodComboBox.currentData() != "none":
+            self._set_opts_disabled(False)
+        else:
+            self._set_opts_disabled(True)
+            self._reset_fields()
+
+    def _update_resampling_info(self):
+        area_def_name = self.ui.projectionComboBox.currentText()
+        area_def = AreaDefinitionsManager.area_def_by_name(area_def_name)
+
+        resampler = self.ui.resamplingMethodComboBox.currentData()
+        if not resampler or resampler.lower() == "none":
+            # gracefully interpret capitalization variants of 'None' as:
+            # "do not resample"
+            self.resampling_info = None
+        else:
+            self.resampling_info = {
+                "resampler": resampler,
+                "area_id": area_def.area_id,
+                "projection": area_def.proj_str,
+                "radius_of_influence": self.ui.radiusOfInfluenceSpinBox.value(),
+                "shape": (self.ui.resamplingShapeRowSpinBox.value(), self.ui.resamplingShapeColumnSpinBox.value()),
+            }
+
+    def _set_opts_disabled(self, is_disabled):
+        self.ui.radiusOfInfluenceSpinBox.setDisabled(is_disabled)
+        # The user should not change the projection nor the resampling shape,
+        # thus:
+        self.ui.projectionComboBox.setDisabled(True)  # instead of 'is_disabled'
+        self.ui.resamplingShapeRowSpinBox.setDisabled(True)  # instead of 'is_disabled'
+        self.ui.resamplingShapeColumnSpinBox.setDisabled(True)  # instead of 'is_disabled'
+
+    def _reset_fields(self):
+        self.ui.resamplingMethodComboBox.setCurrentIndex(0)
+        self.ui.radiusOfInfluenceSpinBox.setValue(5000)
+        self.ui.projectionComboBox.setCurrentIndex(self.parent().document.current_projection_index())
+        self._set_opts_disabled(True)
+
+    def _update_resampling_shape_spin_boxes(self):
+        area_def_name = self.ui.projectionComboBox.currentText()
+        area_def = AreaDefinitionsManager.area_def_by_name(area_def_name)
+        self.ui.resamplingShapeRowSpinBox.setValue(area_def.shape[0])
+        self.ui.resamplingShapeColumnSpinBox.setValue(area_def.shape[1])
+
+    def _update_grouping_mode_combobox(self):
+        reader = self.get_reader()
+        geometry_definition: str = config.get(f"data_reading.{reader}.geometry_definition", "AreaDefinition")
+
+        self.ui.groupingModeComboBox.blockSignals(True)
+
+        cb_model = self.ui.groupingModeComboBox.model()
+
+        if geometry_definition == "SwathDefinition":
+            cb_model.item(GroupingMode.KEEP_SEPARATE.value).setEnabled(True)
+            cb_model.item(GroupingMode.MERGE_ALL.value).setEnabled(True)
+        else:
+            cb_model.item(GroupingMode.KEEP_SEPARATE.value).setEnabled(False)
+            cb_model.item(GroupingMode.MERGE_ALL.value).setEnabled(False)
+            self.ui.groupingModeComboBox.setCurrentIndex(GroupingMode.BY_GROUP_KEYS.value)
+
+        self.ui.groupingModeComboBox.blockSignals(False)
 
-    # def dragEnterEvent(self, event):
-    #     if event.mimeData().hasUrls:
-    #         event.accept()
-    #
-    #     else:
-    #         event.ignore()
-    #
-    # def dragMoveEvent(self, event):
-    #     if event.mimeData().hasUrls:
-    #         event.setDropAction(QtCore.Qt.CopyAction)
-    #         event.accept()
-    #
-    #     else:
-    #         event.ignore()
-    #
-    # def dropEvent(self, event):
-    #     if event.mimeData().hasUrls:
-    #         event.setDropAction(QtCore.Qt.CopyAction)
-    #         event.accept()
-    #
-    #         filePaths = [
-    #             str(url.toLocalFile())
-    #             for url in event.mimeData().urls()
-    #         ]
-    #
-    #         self.dropped.emit(filePaths)
-    #
-    #     else:
-    #         event.ignore()
 
-    # self.widget().clear()
-    # for x in self.doc.asListing():
-    #     self.widget().addItem(x['name'])
+def _to_Qt_CheckState(value: bool):
+    return QtCore.Qt.Checked if value else QtCore.Qt.Unchecked
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/Cloud Amount Default.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/Cloud Amount Default.cmap`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -251,8 +251,8 @@
 <color r="0.0117647058823529" g="0.701960784313725" b="1" a="1" />
 <color r="0.0117647058823529" g="0.701960784313725" b="1" a="1" />
 <color r="0.0117647058823529" g="0.701960784313725" b="1" a="1" />
 <color r="0.0117647058823529" g="0.701960784313725" b="1" a="1" />
 <color r="0.0117647058823529" g="0.701960784313725" b="1" a="1" />
 <color r="1" g="1" b="0" a="1" />
 <color r="1" g="1" b="0" a="1" />
-</colorMap>
+</colorMap>
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/Cloud Top Height.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/Cloud Top Height.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/ACTP.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/ACTP.cmap`

 * *Files 1% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/ACTP.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/ACTP.cmap`

```diff
@@ -1,24 +1,24 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!--
         This_software_was_developed_and_/_or_modified_by_Raytheon_Company,
         pursuant_to_Contract_DG133W-05-CQ-1067_with_the_US_Government.
-        
+
         U.S._EXPORT_CONTROLLED_TECHNICAL_DATA
         This_software_product_contains_export-restricted_data_whose
         export/transfer/disclosure_is_restricted_by_U.S._law._Dissemination
         to_non-U.S._persons_whether_in_the_United_States_or_abroad_requires
         an_export_license_or_other_authorization.
-        
+
         Contractor_Name:________Raytheon_Company
         Contractor_Address:_____6825_Pine_Street,_Suite_340
         ________________________Mail_Stop_B8
         ________________________Omaha,_NE_68106
         ________________________402.291.0100
-        
+
         See_the_AWIPS_II_Master_Rights_File_("Master_Rights_File.pdf")_for
         further_licensing_information.
     -->
 <!-- TOWRdocs Header
          Level 2 product colormap for cloud phase
     -->
 <!-- TOWRdocs Description
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/ADP.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/ADP.cmap`

 * *Files 5% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/ADP.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/ADP.cmap`

```diff
@@ -1,31 +1,31 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!-- TOWRdocs Header
          Aerosol detection, experimental color map, ADP.cmap.
     -->
 <!-- TOWRdocs Description
-         This experimental  color map is a modification of the baseline. It is intended to depict 
-         3 operationally relevant paramenters: smoke, dust, and dust+smoke. It reduces the parameters  
+         This experimental  color map is a modification of the baseline. It is intended to depict
+         3 operationally relevant paramenters: smoke, dust, and dust+smoke. It reduces the parameters
          and color combinations shown in the baseline color legend when ADP products are displayed.
          Aerosol is defined in the algorithm Product User Guidance as the presence of either dust or smoke.
-         The baseline color map contains several combinations of aerosol, dust, and smoke (e.g. 
+         The baseline color map contains several combinations of aerosol, dust, and smoke (e.g.
          "Aerosol+smoke"), which appear redundant. Some pixel combinations are repeated, for example, to
          assign a single pixel color where the baseline colormap has two different pixels assigned for
-         a category that can be combined, such as dust,  and aerosol plus dust.  
+         a category that can be combined, such as dust,  and aerosol plus dust.
 
          The aerosol detection product contains the fields Aerosol, Smoke, and Dust. The goes-r plugin
          ingests these fields as a bitset, in the order of Aerosol, Dust, and Smoke (0=none detected, 1
          = detected).  These are assigned the physicalElement of ADP, which is the styleRule parameter
          in the styleRules file for GOES-R Level 2 products where this colormap is referenced in this
          repository. The dataMapping entries in the style rule file refer to the pixels in this colormap.
 
      -->
 <!-- TOWRdocs Status
-         New experimental color map, modified from the baseline ADP color map. This colormap file has the 
-         same name as the baseline aerosol detection colormap, ADP.cmap. 
+         New experimental color map, modified from the baseline ADP color map. This colormap file has the
+         same name as the baseline aerosol detection colormap, ADP.cmap.
     -->
 <!-- TOWRdocs POC
          Lee Byerle
     -->
 <colorMap>
   <!--Clear or Aerosol bit not included since redundant w/Dust or Smoke -->
   <color r="0.0" g="0.0" b="0.0" a="0.0"/>
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/CSM.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/CSM.cmap`

 * *Files 8% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/CSM.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/CSM.cmap`

```diff
@@ -1,24 +1,24 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!--
         This_software_was_developed_and_/_or_modified_by_Raytheon_Company,
         pursuant_to_Contract_DG133W-05-CQ-1067_with_the_US_Government.
-        
+
         U.S._EXPORT_CONTROLLED_TECHNICAL_DATA
         This_software_product_contains_export-restricted_data_whose
         export/transfer/disclosure_is_restricted_by_U.S._law._Dissemination
         to_non-U.S._persons_whether_in_the_United_States_or_abroad_requires
         an_export_license_or_other_authorization.
-        
+
         Contractor_Name:________Raytheon_Company
         Contractor_Address:_____6825_Pine_Street,_Suite_340
         ________________________Mail_Stop_B8
         ________________________Omaha,_NE_68106
         ________________________402.291.0100
-        
+
         See_the_AWIPS_II_Master_Rights_File_("Master_Rights_File.pdf")_for
         further_licensing_information.
     -->
 <!-- TOWRdocs Header
          Level 2 product colormap for clear sky mask
     -->
 <!-- TOWRdocs Description
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/Dust.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/Dust.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/Dust.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/Dust.cmap`

```diff
@@ -1,24 +1,24 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!--
         This_software_was_developed_and_/_or_modified_by_Raytheon_Company,
         pursuant_to_Contract_DG133W-05-CQ-1067_with_the_US_Government.
-        
+
         U.S._EXPORT_CONTROLLED_TECHNICAL_DATA
         This_software_product_contains_export-restricted_data_whose
         export/transfer/disclosure_is_restricted_by_U.S._law._Dissemination
         to_non-U.S._persons_whether_in_the_United_States_or_abroad_requires
         an_export_license_or_other_authorization.
-        
+
         Contractor_Name:________Raytheon_Company
         Contractor_Address:_____6825_Pine_Street,_Suite_340
         ________________________Mail_Stop_B8
         ________________________Omaha,_NE_68106
         ________________________402.291.0100
-        
+
         See_the_AWIPS_II_Master_Rights_File_("Master_Rights_File.pdf")_for
         further_licensing_information.
     -->
 <!-- TOWRdocs Header
          Level 2 product colormap for Dust
     -->
 <!-- TOWRdocs Description
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/FSC.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/FSC.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/FSC.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/FSC.cmap`

```diff
@@ -1,24 +1,24 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!--
         This_software_was_developed_and_/_or_modified_by_Raytheon_Company,
         pursuant_to_Contract_DG133W-05-CQ-1067_with_the_US_Government.
-        
+
         U.S._EXPORT_CONTROLLED_TECHNICAL_DATA
         This_software_product_contains_export-restricted_data_whose
         export/transfer/disclosure_is_restricted_by_U.S._law._Dissemination
         to_non-U.S._persons_whether_in_the_United_States_or_abroad_requires
         an_export_license_or_other_authorization.
-        
+
         Contractor_Name:________Raytheon_Company
         Contractor_Address:_____6825_Pine_Street,_Suite_340
         ________________________Mail_Stop_B8
         ________________________Omaha,_NE_68106
         ________________________402.291.0100
-        
+
         See_the_AWIPS_II_Master_Rights_File_("Master_Rights_File.pdf")_for
         further_licensing_information.
     -->
 <!-- TOWRdocs Header
          Level 2 product colormap for fire products
     -->
 <!-- TOWRdocs Description
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/GOES-SST-35.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/GOES-SST-35.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/RRQPE.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/RRQPE.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/RRQPE.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/RRQPE.cmap`

```diff
@@ -1,24 +1,24 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!--
         This_software_was_developed_and_/_or_modified_by_Raytheon_Company,
         pursuant_to_Contract_DG133W-05-CQ-1067_with_the_US_Government.
-        
+
         U.S._EXPORT_CONTROLLED_TECHNICAL_DATA
         This_software_product_contains_export-restricted_data_whose
         export/transfer/disclosure_is_restricted_by_U.S._law._Dissemination
         to_non-U.S._persons_whether_in_the_United_States_or_abroad_requires
         an_export_license_or_other_authorization.
-        
+
         Contractor_Name:________Raytheon_Company
         Contractor_Address:_____6825_Pine_Street,_Suite_340
         ________________________Mail_Stop_B8
         ________________________Omaha,_NE_68106
         ________________________402.291.0100
-        
+
         See_the_AWIPS_II_Master_Rights_File_("Master_Rights_File.pdf")_for
         further_licensing_information.
     -->
 <!-- TOWRdocs Header
          Level 2 product colormap for rainfall rate/QPF
     -->
 <!-- TOWRdocs Description
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/RRQPE1.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/RRQPE1.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/RRQPE1.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/RRQPE1.cmap`

```diff
@@ -1,21 +1,21 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!-- TOWRdocs Header
          RRQPE Level 2 product colormap, created for the ABI rain rate product
     -->
 <!-- TOWRdocs Description
          New colormap not in 17.x baseline that improves depiction of rain rate.
-         
+
          The current default color scale for the RR/QPE product highlights a lot of areas that have essentially no
          useful data, based upon the accuracy of the product design (around 0.3 inches/hr). Further, the default
          AWIPS RR/QPE product scale has a max value of 1.6 inches.  This value is too low as some readings in the product
          are in excess of 3 inches.
          This new color maps is intended for use with inches per hour, and removes data below 0.1 inches.  It also
          shows colors that are more familiar to forecasters from radar QPE products, and is intended
-         for extending the maximum value to 3.0 inches.  
+         for extending the maximum value to 3.0 inches.
          Contributors: Satellite Enhancement Team/Frank Alsheimer
     -->
 <!-- TOWRdocs Status
          Updated color map for derived products, not currently in the base line.
     -->
 <!-- TOWRdocs POC
          Lee Byerle 2/6/2018
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/Smoke.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/Smoke.cmap`

 * *Files 4% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/Smoke.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/Smoke.cmap`

```diff
@@ -1,24 +1,24 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!--
         This_software_was_developed_and_/_or_modified_by_Raytheon_Company,
         pursuant_to_Contract_DG133W-05-CQ-1067_with_the_US_Government.
-        
+
         U.S._EXPORT_CONTROLLED_TECHNICAL_DATA
         This_software_product_contains_export-restricted_data_whose
         export/transfer/disclosure_is_restricted_by_U.S._law._Dissemination
         to_non-U.S._persons_whether_in_the_United_States_or_abroad_requires
         an_export_license_or_other_authorization.
-        
+
         Contractor_Name:________Raytheon_Company
         Contractor_Address:_____6825_Pine_Street,_Suite_340
         ________________________Mail_Stop_B8
         ________________________Omaha,_NE_68106
         ________________________402.291.0100
-        
+
         See_the_AWIPS_II_Master_Rights_File_("Master_Rights_File.pdf")_for
         further_licensing_information.
     -->
 <!-- TOWRdocs Header
          Level 2 product colormap for Smoke
     -->
 <!-- TOWRdocs Description
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/VTRSB.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/VTRSB.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/VTRSB.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/VTRSB.cmap`

```diff
@@ -1,24 +1,24 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!--
         This_software_was_developed_and_/_or_modified_by_Raytheon_Company,
         pursuant_to_Contract_DG133W-05-CQ-1067_with_the_US_Government.
-        
+
         U.S._EXPORT_CONTROLLED_TECHNICAL_DATA
         This_software_product_contains_export-restricted_data_whose
         export/transfer/disclosure_is_restricted_by_U.S._law._Dissemination
         to_non-U.S._persons_whether_in_the_United_States_or_abroad_requires
         an_export_license_or_other_authorization.
-        
+
         Contractor_Name:________Raytheon_Company
         Contractor_Address:_____6825_Pine_Street,_Suite_340
         ________________________Mail_Stop_B8
         ________________________Omaha,_NE_68106
         ________________________402.291.0100
-        
+
         See_the_AWIPS_II_Master_Rights_File_("Master_Rights_File.pdf")_for
         further_licensing_information.
     -->
 <!-- TOWRdocs Header
          Level 2 product colormap for GOES-R prods
     -->
 <!-- TOWRdocs Description
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-cape-10.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-cape-10.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-cape-10.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-cape-10.cmap`

```diff
@@ -1,15 +1,15 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!-- TOWRdocs Header
          CAPE color map (color-cape-10).
     -->
 <!-- TOWRdocs Description
-         Updates GOES-R baseline colormap, and may be applied to CAPE, K-Index, Showalter, and Total Totals. 
-		 This is a 10 bits per pixel linear enhancement. It has 5 main colors, ranging from brown to blue to yellow to 
-		 red to magenta, for increasing instability. In general, this is based on the traditional GOES Sounder 
+         Updates GOES-R baseline colormap, and may be applied to CAPE, K-Index, Showalter, and Total Totals.
+		 This is a 10 bits per pixel linear enhancement. It has 5 main colors, ranging from brown to blue to yellow to
+		 red to magenta, for increasing instability. In general, this is based on the traditional GOES Sounder
 		 stability enhancement from CIMSS. Readout precisions range approximately from 0.05 to 0.1K with suggested
 		 display ranges (e.g. 0-5000 for CAPE, -70 to 70 or -60 to 60 for LI, KI, SI, and TT).
 	     Contributors: Tim Schmit and Jordan Gerth
     -->
 <!-- TOWRdocs Status
          Updated color map for derived products, not currently in the base line.
     -->
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-li-10.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-li-10.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-li-10.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-li-10.cmap`

```diff
@@ -1,14 +1,14 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!-- TOWRdocs Header
          Lifted Index (LI) color map (color-li-10).
     -->
 <!-- TOWRdocs Description
-         Updates GOES-R baseline colormap for LI. This is a 10 bits per pixel linear enhancement. It has 4 main colors, ranging 
-		 from brown to tan blue to yellow to red, for increasing instability. In general, this is based on the traditional 
+         Updates GOES-R baseline colormap for LI. This is a 10 bits per pixel linear enhancement. It has 4 main colors, ranging
+		 from brown to tan blue to yellow to red, for increasing instability. In general, this is based on the traditional
 		 GOES Sounder LI enhancement from CIMSS.
          Contributors: Tim Schmit and Jordan Gerth
     -->
 <!-- TOWRdocs Status
          Updated color map for derived products, not currently in the base line.
     -->
 <!-- TOWRdocs POC
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-pw10-10.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-pw10-10.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-pw10-10.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-pw10-10.cmap`

```diff
@@ -1,13 +1,13 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!-- TOWRdocs Header
          Precipitable Water (TPW) color map (color-pw10-10).
     -->
 <!-- TOWRdocs Description
-         Updates GOES-R baseline colormap for TPW. This is a 10 bits per pixel linear enhancement. The main colors range 
+         Updates GOES-R baseline colormap for TPW. This is a 10 bits per pixel linear enhancement. The main colors range
 		 from brown to blue to green to yellow to red to magenta to pink, for increasing amounts of moisture. In general,
 		 this is based on the traditional GOES Sounder TPW enhancement from CIMSS.
 		 If 80 mm is used for these 10 bit enhancements, thats a read-out precision of approximately 0.08 mm. (80/(2^10)).
          Contributors: Tim Schmit and Jordan Gerth
     -->
 <!-- TOWRdocs Status
          Updated color map for derived products, not currently in the base line.
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-pw8-10.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-pw8-10.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-pw8-10.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/color-pw8-10.cmap`

```diff
@@ -1,13 +1,13 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!-- TOWRdocs Header
          Precipitable Water (TPW) color map (color-pw8-10).
     -->
 <!-- TOWRdocs Description
-         Updates GOES-R baseline colormap for TPW. This is a 10 bits per pixel linear enhancement. The main colors range 
+         Updates GOES-R baseline colormap for TPW. This is a 10 bits per pixel linear enhancement. The main colors range
 		 from brown to blue to green to yellow to red to magenta to pink to red, for increasing amounts of moisture.
 		 In general, this is based on the traditional GOES Sounder TPW enhancement from CIMSS.
 		 If 80 mm is used for these 10 bit enhancements, thats a read-out precision of approximately 0.08 mm. (80/(2^10)).
          Contributors: Tim Schmit and Jordan Gerth
     -->
 <!-- TOWRdocs Status
          Updated color map for derived products, not currently in the base line.
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/CIRA (IR Default).cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/CIRA (IR Default).cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/IR WV.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/IR WV.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/IR_Color_Clouds_Summer.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/IR_Color_Clouds_Summer.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/IR_Color_Clouds_Summer.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/IR_Color_Clouds_Summer.cmap`

```diff
@@ -1,19 +1,19 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!-- TOWRdocs Header
      Window IR color map (IR_Color_Clouds_Summer, IR Pixel Flag off)
 -->
 <!-- TOWRdocs Description
-     This color table is designed primarily for the Window IR bands (bands 13 and 14), but can be applied to any IR band.  It works fine over regions with hot ground.  The warmer end, which is typically cloud free areas and relatively low clouds, is represented by a range of grayscale where the warmest brightness temperatures are darkest.  At around -25 C cool (blue) colors begin, and as the temps cool further, the colors begin to "warm", and the transition between colors is gradual and smooth. This colormap is intended for used when the IRPixel flag is turned off/commented out in the styleRules folder file goesrCMI-ImageryStyleRules.xml. Training Team Approved. Contributor: Dan Lindsey 
+     This color table is designed primarily for the Window IR bands (bands 13 and 14), but can be applied to any IR band.  It works fine over regions with hot ground.  The warmer end, which is typically cloud free areas and relatively low clouds, is represented by a range of grayscale where the warmest brightness temperatures are darkest.  At around -25 C cool (blue) colors begin, and as the temps cool further, the colors begin to "warm", and the transition between colors is gradual and smooth. This colormap is intended for used when the IRPixel flag is turned off/commented out in the styleRules folder file goesrCMI-ImageryStyleRules.xml. Training Team Approved. Contributor: Dan Lindsey
 -->
 <!-- TOWRdocs Status
-     New experimental IR color map for GOES-R channels. This color table is not part of the AWIPS baseline. 
-     The color map can be added to styleRules in common_static, or it can be used to modify an existing display.  
-     For the latter, right-click the title of the product at the bottom of the CAVE screen.  Select "Change Color Map."  
-     Under "Set Color Table Range" press the entry that appears and there will be a pull-down of options. 
+     New experimental IR color map for GOES-R channels. This color table is not part of the AWIPS baseline.
+     The color map can be added to styleRules in common_static, or it can be used to modify an existing display.
+     For the latter, right-click the title of the product at the bottom of the CAVE screen.  Select "Change Color Map."
+     Under "Set Color Table Range" press the entry that appears and there will be a pull-down of options.
      Select "site" and then your site 3-letter identifier, then "Sat", then "IR" (or "VIS" for Vis color tables), and select the desired color map.
 -->
 <!-- TOWRdocs POC
      Lee Byerle
 -->
 <colorMap>
   <color a="1.0" b="0.0" g="0.0" r="0.0"/>
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/IR_Color_Clouds_Winter.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/IR_Color_Clouds_Winter.cmap`

 * *Files 1% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/IR_Color_Clouds_Winter.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/IR_Color_Clouds_Winter.cmap`

```diff
@@ -1,19 +1,19 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!-- TOWRdocs Header
-     Window IR color map (IR_Color_Clouds_Winter, IR Pixel flag off) 
+     Window IR color map (IR_Color_Clouds_Winter, IR Pixel flag off)
 -->
 <!-- TOWRdocs Description
-     This color table is designed primarily for the Window IR bands (bands 13 and 14), but can be applied to any IR band.  It works best over areas where the ground is not particularly hot (e.g., over the ocean or land areas during the winter).  The warmer end, which is typically cloud free areas and relatively low clouds, is represented by a range of grayscale where the warmest brightness temperatures are darkest.  At around -25 C cool (blue) colors begin, and as the temps cool further, the colors begin to "warm", and the transition between colors is gradual and smooth. This colormap is intended for used when the IRPixel flag is turned off/commented out in the styleRules folder file goesrCMI-ImageryStyleRules.xml. Training Team Approved. Contributor: Dan Lindsey 
+     This color table is designed primarily for the Window IR bands (bands 13 and 14), but can be applied to any IR band.  It works best over areas where the ground is not particularly hot (e.g., over the ocean or land areas during the winter).  The warmer end, which is typically cloud free areas and relatively low clouds, is represented by a range of grayscale where the warmest brightness temperatures are darkest.  At around -25 C cool (blue) colors begin, and as the temps cool further, the colors begin to "warm", and the transition between colors is gradual and smooth. This colormap is intended for used when the IRPixel flag is turned off/commented out in the styleRules folder file goesrCMI-ImageryStyleRules.xml. Training Team Approved. Contributor: Dan Lindsey
 -->
 <!-- TOWRdocs Status
-     New experimental IR color map for GOES-R channels. This table is not part of the AWIPS baseline. 
-     The color map can be added to styleRules in common_static, or it can be used to modify an existing display.  
-     For the latter, right-click the title of the product at the bottom of the CAVE screen.  Select "Change Color Map."  
-     Under "Set Color Table Range" press the entry that appears and there will be a pull-down of options.  
+     New experimental IR color map for GOES-R channels. This table is not part of the AWIPS baseline.
+     The color map can be added to styleRules in common_static, or it can be used to modify an existing display.
+     For the latter, right-click the title of the product at the bottom of the CAVE screen.  Select "Change Color Map."
+     Under "Set Color Table Range" press the entry that appears and there will be a pull-down of options.
      Select "site" and then your site 3-letter identifier, then "Sat", then "IR" (or "VIS" for Vis color tables), and select the desired color map.
 -->
 <!-- TOWRdocs POC
      Lee Byerle
 -->
 <colorMap>
   <color a="1.0" b="0.0" g="0.0" r="0.0"/>
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/Rainbow_11_bit.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/Rainbow_11_bit.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/Rainbow_11_bit.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/Rainbow_11_bit.cmap`

```diff
@@ -1,33 +1,33 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!-- TOWRdocs Header
          11-bit IR/VIS/NIR color map (Rainbow_11_bit, IR Pixel Flag off)
     -->
 <!-- TOWRdocs Description
-         The 11-bit "Rainbow" enhancement was designed to be applicable to any 
-	 infrared band, including the water vapor bands. The "warmer" colors 
+         The 11-bit "Rainbow" enhancement was designed to be applicable to any
+	 infrared band, including the water vapor bands. The "warmer" colors
 	 represent the hot temperatures with "cooler" colors representing the
 	 colder temperatures. The sequence and discretization of the colors is
 	 a function of a continuous path on the classical Red-Green-Blue (RGB)
 	 color cube. To support a higher bit depth, more transition colors are
 	 used than traditionally through choosing midpoints on the face of the
 	 cube between breakpoints on the same axis. This enhancement is also
 	 suitable for visible and near-infrared imagery.
 
          The color map can be added to styleRules in common_static, or it can be used to modify
-         an existing display. For the latter, right-click the title of the product at the bottom 
+         an existing display. For the latter, right-click the title of the product at the bottom
          of the CAVE screen.  Select "Change Color Map." Under "Set Color Table Range" press the
          entry that appears and there will be a pull-down of options. Select "site" and then your
-         site 3-letter identifier, then "Sat", then "IR" (or "VIS" for Vis color tables), and 
+         site 3-letter identifier, then "Sat", then "IR" (or "VIS" for Vis color tables), and
          select the desired color map. Training Team Approved.
          Contributors: Tim Schmit and Jordan Gerth.
     -->
 <!-- TOWRdocs Status
          New experimental IR/VIS/NIR color map for GOES-R channels. This color map does not
-         appear in the current AWIPS baseline.  
+         appear in the current AWIPS baseline.
    -->
 <!-- TOWRdocs POC
          Lee Byerle
     -->
 <colorMap xmlns:ns2="group" xmlns:ns3="http://www.example.org/productType">
   <color r="0.498039215686" g="0.0" b="0.0" a="1.0"/>
   <color r="0.398431372549" g="0.0" b="0.0" a="1.0"/>
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/WV_Dry_Yellow.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/WV_Dry_Yellow.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/WV_Dry_Yellow.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/WV_Dry_Yellow.cmap`

```diff
@@ -1,25 +1,25 @@
 <?xml version="1.0" encoding="utf-8"?>
 <!-- TOWRdocs Header
          11-bit WV color map (WV_Dry_Yellow, use for IR Pixel Flag Off)
     -->
 <!-- TOWRdocs Description
          The 11-bit "WV_Dry_Yellow" enhancement was designed for CH 10 (7.3 um),
-         as well as the other water vapor bands. 
+         as well as the other water vapor bands.
 
          The color map can be added to styleRules in common_static, or it can be used to modify
-         an existing display. For the latter, right-click the title of the product at the bottom 
+         an existing display. For the latter, right-click the title of the product at the bottom
          of the CAVE screen.  Select "Change Color Map." Under "Set Color Table Range" press the
          entry that appears and there will be a pull-down of options. Select "site" and then your
-         site 3-letter identifier, then "Sat", then "IR" (or "VIS" for Vis color tables), and 
+         site 3-letter identifier, then "Sat", then "IR" (or "VIS" for Vis color tables), and
          select the desired color map. Training Team Approved. Contributor: Dan Lindsey.
     -->
 <!-- TOWRdocs Status
          New experimental WV color map for GOES-R channels. This color map does not
-         appear in the current AWIPS baseline.  
+         appear in the current AWIPS baseline.
    -->
 <!-- TOWRdocs POC
          Lee Byerle
     -->
 <colorMap>
   <color a="1.0" b="0.0" g="0.0" r="0.0"/>
   <color a="1.0" b="0.0" g="0.0" r="0.0"/>
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/dust_and_moisture_split_window.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/dust_and_moisture_split_window.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/dust_and_moisture_split_window.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/dust_and_moisture_split_window.cmap`

```diff
@@ -3,18 +3,18 @@
      Dust and moisture split window color map
 -->
 <!-- TOWRdocs Description
      - Set as the default for the 10.3-12.3 difference.
 - This color table is specifically designed for the GOES-16 10.3-12.3 difference product, and is most useful for detecting blowing dust, volcanic ash, and regions of low-level water vapor pooling.  Values that are negative (especially below about -1.0) are colored shades of brown and are uniquely associated with blowing dust or volcanic ash.  Values near zero (white) are typically associated with optically thick clouds and snow cover.  Values above about +10 are generally optically thin clouds, such as thin cirrus.  Values between about +1 and +10 are of interest when attempting to diagnose regions of low-level water vapor pooling.  In *clear skies*, larger positive values in this range mean deeper low-level water vapor for a given temperature lapse rate.  Steeper lapse rates and deeper moisture will cause values to increase.
 -->
 <!-- TOWRdocs Status
-     New color table that is not part of the AWIPS baseline, for use with GOES-16. 
-     The color map can be added to styleRules in common_static, or it can be used to modify an existing display.  
-     For the latter, right-click the title of the product at the bottom of the CAVE screen.  Select "Change Color Map."  
-     Under "Set Color Table Range" press the entry that appears and there will be a pull-down of options. 
+     New color table that is not part of the AWIPS baseline, for use with GOES-16.
+     The color map can be added to styleRules in common_static, or it can be used to modify an existing display.
+     For the latter, right-click the title of the product at the bottom of the CAVE screen.  Select "Change Color Map."
+     Under "Set Color Table Range" press the entry that appears and there will be a pull-down of options.
      Select "site" and then your site 3-letter identifier, then "GOES-R", then "IR" (or "VIS" for Vis color tables), and select the desired color map.
 -->
 <!-- TOWRdocs POC
      Lee Byerle
 -->
 <colorMap>
   <color a="0.0" b="0.0" g="0.0" r="0.0"/>
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/enhanced-rainbow-11.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/enhanced-rainbow-11.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/enhanced-rainbow-11.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/enhanced-rainbow-11.cmap`

```diff
@@ -3,17 +3,17 @@
      enhanced-rainbow-11 color map
 -->
 <!-- TOWRdocs Description
      3/27/2017 Color table added by SET team (contact: Jordan Gerth)
 -->
 <!-- TOWRdocs Status
      New color table that is not part of the AWIPS baseline, for use with GOES-R imagery.
-     The color map can be added to styleRules in common_static, or it can be used to modify an existing display.  
-     For the latter, right-click the title of the product at the bottom of the CAVE screen.  Select "Change Color Map."  
-     Under "Set Color Table Range" press the entry that appears and there will be a pull-down of options. 
+     The color map can be added to styleRules in common_static, or it can be used to modify an existing display.
+     For the latter, right-click the title of the product at the bottom of the CAVE screen.  Select "Change Color Map."
+     Under "Set Color Table Range" press the entry that appears and there will be a pull-down of options.
      Select "site" and then your site 3-letter identifier, then "Sat", then "IR" (or "VIS" for Vis color tables), and select the desired color map.
 -->
 <!-- TOWRdocs POC
      Lee Byerle
 -->
 <colorMap xmlns:ns2="group" xmlns:ns3="http://www.example.org/productType">
   <color r="0.498039215686" g="0.0" b="0.0" a="1.0"/>
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/enhanced-rainbow_warmer_yellow.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/enhanced-rainbow_warmer_yellow.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/enhanced-rainbow_warmer_yellow.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/enhanced-rainbow_warmer_yellow.cmap`

```diff
@@ -5,17 +5,17 @@
 <!-- TOWRdocs Description
      New color table. Values between +55 and +127 C vary from black (cooler) to yellow (hotter)
 to help identify fires. This table is not recommended for fire detection in the warm season when the background is relatively hot.
      3/27/2017 Color table added by SET team (contact: Jordan Gerth)
 -->
 <!-- TOWRdocs Status
      New color table that is not part of the AWIPS baseline, for use with GOES-R imagery.
-     The color map can be added to styleRules in common_static, or it can be used to modify an existing display.  
-     For the latter, right-click the title of the product at the bottom of the CAVE screen.  Select "Change Color Map."  
-     Under "Set Color Table Range" press the entry that appears and there will be a pull-down of options. 
+     The color map can be added to styleRules in common_static, or it can be used to modify an existing display.
+     For the latter, right-click the title of the product at the bottom of the CAVE screen.  Select "Change Color Map."
+     Under "Set Color Table Range" press the entry that appears and there will be a pull-down of options.
      Select "site" and then your site 3-letter identifier, then "GOES-R", then "IR" (or "VIS" for Vis color tables), and select the desired color map.
 -->
 <!-- TOWRdocs POC
      Lee Byerle
 -->
 <colorMap>
   <color a="1.0" b="0.0" g="1.0" r="1.0"/>
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/fire_detection_3.9.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/fire_detection_3.9.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/fogdiff_blue.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/fogdiff_blue.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/ramsdis_IR_12bit.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/ramsdis_IR_12bit.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/ramsdis_IR_12bit.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/ramsdis_IR_12bit.cmap`

```diff
@@ -3,18 +3,18 @@
      RAMSDIS Legacy IR Color Map
 -->
 <!-- TOWRdocs Description
      12-bit version of legacy table similar to the former AWIPS-1 and AWIPS-2 CIRA (IR Default) color table.
      Contributor: Dan Lindsey
 -->
 <!-- TOWRdocs Status
-     Added IR color map for GOES-R channels. This color table is not part of the AWIPS baseline. 
-     The color map can be added to styleRules in common_static, or it can be used to modify an existing display.  
-     For the latter, right-click the title of the product at the bottom of the CAVE screen.  Select "Change Color Map."  
-     Under "Set Color Table Range" press the entry that appears and there will be a pull-down of options. 
+     Added IR color map for GOES-R channels. This color table is not part of the AWIPS baseline.
+     The color map can be added to styleRules in common_static, or it can be used to modify an existing display.
+     For the latter, right-click the title of the product at the bottom of the CAVE screen.  Select "Change Color Map."
+     Under "Set Color Table Range" press the entry that appears and there will be a pull-down of options.
      Select "site" and then your site 3-letter identifier, then "Sat", then "IR", and select the desired color map.
 
 	 1/10/2017 updated name of file to add 12bit
 -->
 <!-- TOWRdocs POC
      Lee Byerle 1/10/2017
 -->
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/ramsdis_WV_12bit.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/ramsdis_WV_12bit.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/IR/ramsdis_WV_12bit.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/IR/ramsdis_WV_12bit.cmap`

```diff
@@ -3,18 +3,18 @@
      RAMSDIS Legacy Water Vapor (WV) Color Map
 -->
 <!-- TOWRdocs Description
      12-bit version of legacy table similar to the former AWIPS-1 and AWIPS-2 RAMSDIS WV color table.
      Contributor: Dan Lindsey
 -->
 <!-- TOWRdocs Status
-     Added WV color map for GOES-R channels. This color table is not part of the AWIPS baseline. 
-     The color map can be added to styleRules in common_static, or it can be used to modify an existing display.  
-     For the latter, right-click the title of the product at the bottom of the CAVE screen.  Select "Change Color Map."  
-     Under "Set Color Table Range" press the entry that appears and there will be a pull-down of options. 
+     Added WV color map for GOES-R channels. This color table is not part of the AWIPS baseline.
+     The color map can be added to styleRules in common_static, or it can be used to modify an existing display.
+     For the latter, right-click the title of the product at the bottom of the CAVE screen.  Select "Change Color Map."
+     Under "Set Color Table Range" press the entry that appears and there will be a pull-down of options.
      Select "site" and then your site 3-letter identifier, then "Sat", then "IR", and select the desired color map.
 
 	 1/10/2017 updated name of file to add 12bit
 
 -->
 <!-- TOWRdocs POC
      Lee Byerle 1/10/2017
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Lifted Index/Lifted Index - New CIMSS Table.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Lifted Index/Lifted Index - New CIMSS Table.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Lifted Index/Lifted Index Default.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Lifted Index/Lifted Index Default.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Precip/Blended Total Precip Water.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Precip/Blended Total Precip Water.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Precip/Percent of Normal TPW.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Precip/Percent of Normal TPW.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Precip/Precip Water - New CIMSS Table.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Precip/Precip Water - New CIMSS Table.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Precip/Precip Water - Polar.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Precip/Precip Water - Polar.cmap`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -251,8 +251,8 @@
 <color r="0.619607843137255" g="0.619607843137255" b="0.619607843137255" a="1" />
 <color r="0.654901960784314" g="0.654901960784314" b="0.654901960784314" a="1" />
 <color r="0.686274509803922" g="0.686274509803922" b="0.686274509803922" a="1" />
 <color r="1" g="1" b="1" a="1" />
 <color r="1" g="1" b="1" a="1" />
 <color r="1" g="1" b="1" a="1" />
 <color r="1" g="1" b="1" a="0" />
-</colorMap>
+</colorMap>
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Precip/Precip Water Default.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Precip/Precip Water Default.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Skin Temp/Skin Temp - New CIMSS Table.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Skin Temp/Skin Temp - New CIMSS Table.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/Skin Temp/Skin Temp Default.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/Skin Temp/Skin Temp Default.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/VIS/CA (Low Light Vis).cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/VIS/CA (Low Light Vis).cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/VIS/Linear.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/VIS/Linear.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/VIS/VIS_gray_sq-root-12.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/VIS/VIS_gray_sq-root-12.cmap`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/VIS/VIS_gray_sq-root-12.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/VIS/VIS_gray_sq-root-12.cmap`

```diff
@@ -3,34 +3,34 @@
 This is a colormap file that is read via JaXB to marshel the ColorMap class.
 ======================-->
 <!-- TOWRdocs Header
          12-bit, square-root, VIS/NIR color map.
     -->
 <!-- TOWRdocs Description
          12-bit, square-root color map which can be applied to both the vis and near-infrared
-         (NIR) reflective spectral bands. 
-	 
-         It is a special case of the power law stretch (n=1/2). It compresses the 
-         upper end of the reflectance factor spectrum and stretches the lower end. This method 
-         has been used for decades when displaying GOES visible data. For example, this is 
+         (NIR) reflective spectral bands.
+
+         It is a special case of the power law stretch (n=1/2). It compresses the
+         upper end of the reflectance factor spectrum and stretches the lower end. This method
+         has been used for decades when displaying GOES visible data. For example, this is
          applied 'up stream' for the GOES visible band in AWIPS 1. The idea of this enhancement
-         is to brighten the darker land features, while leaving the brighter clouds mostly 
+         is to brighten the darker land features, while leaving the brighter clouds mostly
          unchanged. For the ABI on GOES-R, this enhancement can be applied to both the visbile
-         and near-infrared (reflective) spectral bands.    
+         and near-infrared (reflective) spectral bands.
 
          The color map can be added to styleRules in common_static, or it can be used to modify
-         an existing display. For the latter, right-click the title of the product at the bottom 
+         an existing display. For the latter, right-click the title of the product at the bottom
          of the CAVE screen.  Select "Change Color Map." Under "Set Color Table Range" press the
          entry that appears and there will be a pull-down of options. Select "site" and then your
-         site 3-letter identifier, then "Sat", then "IR" (or "VIS" for Vis color tables), and 
+         site 3-letter identifier, then "Sat", then "IR" (or "VIS" for Vis color tables), and
          select the desired color map. Training Team Approved. Contributors: Tim Schmit and Jordan Gerth.
 
     -->
 <!-- TOWRdocs Status
-         12-bit VIS/NIR color map which is not currently in the AWIPS baseline. 
+         12-bit VIS/NIR color map which is not currently in the AWIPS baseline.
     -->
 <!-- TOWRdocs POC
          Lee Byerle
     -->
 <colorMap>
   <color r="0.0" g="0.0" b="0.0" a="1.0"/>
   <color r="0.015625" g="0.015625" b="0.015625" a="1.0"/>
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/VIS/ZA (Vis Default).cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/VIS/ZA (Vis Default).cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/WV/Gray Scale Water Vapor.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/WV/Gray Scale Water Vapor.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/WV/NSSL VAS (WV Alternate).cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/WV/NSSL VAS (WV Alternate).cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/GOES-R/WV/SLC WV.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/GOES-R/WV/SLC WV.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/Low Cloud Base.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/Low Cloud Base.cmap`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -251,8 +251,8 @@
 <color r="1" g="0" b="0" a="1" />
 <color r="1" g="0" b="0" a="1" />
 <color r="1" g="0" b="0" a="1" />
 <color r="1" g="0" b="0" a="1" />
 <color r="1" g="0" b="0" a="1" />
 <color r="1" g="0" b="0" a="1" />
 <color r="1" g="0" b="0" a="1" />
-</colorMap>
+</colorMap>
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/Rain Rate.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/Rain Rate.cmap`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -251,8 +251,8 @@
 	<color r="0.937254901960784" g="0.937254901960784" b="0.937254901960784" a="1" />
 	<color r="0.949019607843137" g="0.949019607843137" b="0.949019607843137" a="1" />
 	<color r="0.956862745098039" g="0.956862745098039" b="0.956862745098039" a="1" />
 	<color r="0.968627450980392" g="0.968627450980392" b="0.968627450980392" a="1" />
 	<color r="0.980392156862745" g="0.980392156862745" b="0.980392156862745" a="1" />
 	<color r="0.988235294117647" g="0.988235294117647" b="0.988235294117647" a="1" />
 	<color r="1" g="1" b="1" a="1" />
-</colorMap>
+</colorMap>
```

### Comparing `uwsift-1.2.3/uwsift/data/colormaps/OAX/prob_severe.cmap` & `uwsift-2.0.0b0/uwsift/data/colormaps/OAX/prob_severe.cmap`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/fonts/Andale Mono.ttf` & `uwsift-2.0.0b0/uwsift/data/fonts/Andale Mono.ttf`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/cfName.def` & `uwsift-2.0.0b0/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/cfName.def`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/modelName.def` & `uwsift-2.0.0b0/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/modelName.def`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/name.def` & `uwsift-2.0.0b0/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/name.def`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/paramId.def` & `uwsift-2.0.0b0/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/paramId.def`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/shortName.def` & `uwsift-2.0.0b0/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/shortName.def`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/units.def` & `uwsift-2.0.0b0/uwsift/data/grib_definitions/grib2/localConcepts/kwbc/units.def`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.README.html` & `uwsift-2.0.0b0/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.README.html`

 * *Files 2% similar despite different names*

```diff
@@ -11,22 +11,22 @@
 <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
 <link rel="alternate" type="application/rss+xml" title="Natural Earth RSS Feed" href="http://www.naturalearthdata.com/feed/" />
 <link rel="pingback" href="http://www.naturalearthdata.com/xmlrpc.php" />
 <script type="text/javascript" src="http://www.naturalearthdata.com/wp-content/themes/NEV/includes/js/suckerfish.js"></script>
 <!--[if lt IE 7]>
     <script src="http://ie7-js.googlecode.com/svn/version/2.0(beta3)/IE7.js" type="text/javascript"></script>
     <script defer="defer" type="text/javascript" src="http://www.naturalearthdata.com/wp-content/themes/NEV/includes/js/pngfix.js"></script>
-<![endif]--> 
+<![endif]-->
 <link rel="stylesheet" href="http://www.naturalearthdata.com/wp-content/themes/NEV/style.css" type="text/css" media="screen" />
 
 <meta name='Admin Management Xtended WordPress plugin' content='2.1.1' />
 <link rel="alternate" type="application/rss+xml" title="Natural Earth &raquo; Admin 0 &#8211; Countries Comments Feed" href="http://www.naturalearthdata.com/downloads/110m-cultural-vectors/110m-admin-0-countries/feed/" />
 <link rel='stylesheet' id='sociable-front-css-css'  href='http://www.naturalearthdata.com/wp-content/plugins/sociable/sociable.css?ver=2.9.2' type='text/css' media='' />
 <link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://www.naturalearthdata.com/xmlrpc.php?rsd" />
-<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.naturalearthdata.com/wp-includes/wlwmanifest.xml" /> 
+<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.naturalearthdata.com/wp-includes/wlwmanifest.xml" />
 <link rel='index' title='Natural Earth' href='http://www.naturalearthdata.com' />
 <link rel='start' title='Welcome to the Natural Earth Blog' href='http://www.naturalearthdata.com/blog/miscellaneous/test/' />
 <link rel='prev' title='Admin 0 &#8211; Details' href='http://www.naturalearthdata.com/downloads/110m-cultural-vectors/110m-admin-0-details/' />
 <link rel='next' title='Rivers, Lake Centerlines' href='http://www.naturalearthdata.com/downloads/110m-physical-vectors/110m-rivers-lake-centerlines/' />
 <meta name="generator" content="WordPress 2.9.2" />
 
 <!-- All in One SEO Pack 1.6.10.2 by Michael Torbert of Semper Fi Web Design[309,457] -->
@@ -52,41 +52,41 @@
 <script>
      jQuery.noConflict();
 </script>
 <script type="text/javascript" charset="utf-8">
 	$(function(){
 		var tabContainers = $('div#maintabdiv > div');
 		tabContainers.hide().filter('#comments').show();
-		
+
 		$('div#maintabdiv ul#tabnav a').click(function () {
 				tabContainers.hide();
 				tabContainers.filter(this.hash).show();
 				$('div#maintabdiv ul#tabnav a').removeClass('current');
 				$(this).addClass('current');
 				return false;
 			}).filter('#comments').click();
-		
-		
+
+
 	});
 </script>
 
 		<script type="text/javascript" language="javascript" src="http://www.naturalearthdata.com/dataTables/media/js/jquery.dataTables.js"></script>
 		<script type="text/javascript" charset="utf-8">
 			$(document).ready(function() {
 				$('#ne_table').dataTable();
 			} );
 		</script>
 
 </head>
 <body>
 <div id="page">
 <div id="header">
-	<div id="headerimg">		
-        <h1><a href="http://www.naturalearthdata.com/"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/nev_logo.png" alt="Natural Earth title="Natural Earth" /></a></h1> 
-        <div class="description">Free vector and raster map data at 1:10m, 1:50m, and 1:110m scales</div> 
+	<div id="headerimg">
+        <h1><a href="http://www.naturalearthdata.com/"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/nev_logo.png" alt="Natural Earth title="Natural Earth" /></a></h1>
+        <div class="description">Free vector and raster map data at 1:10m, 1:50m, and 1:110m scales</div>
         <div class="header_search"><form method="get" id="searchform" action="http://www.naturalearthdata.com/">
 <label class="hidden" for="s">Search for:</label>
 <div><input type="text" value="" name="s" id="s" />
 <input type="submit" id="searchsubmit" value="Search" />
 </div>
 </form>
 </div>
@@ -98,44 +98,44 @@
    pageLanguage: 'en'
  }, 'google_translate_element');
 }
 </script>
 <script src="http://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script>
 </div>-->
 	</div>
-    
+
 </div>
 
 <div id="pagemenu" style="align:bottom;">
     <ul id="page-list" class="clearfix"><li class="page_item page-item-4"><a href="http://www.naturalearthdata.com" title="Home">Home</a></li>
 <li class="page_item page-item-10"><a href="http://www.naturalearthdata.com/features/" title="Features">Features</a></li>
 <li class="page_item page-item-12"><a href="http://www.naturalearthdata.com/downloads/" title="Downloads">Downloads</a></li>
 <li class="page_item page-item-6 current_page_parent"><a href="http://www.naturalearthdata.com/blog/" title="Blog">Blog</a></li>
 <li class="page_item page-item-14"><a href="http://www.naturalearthdata.com/forums" title="Forums">Forums</a></li>
 <li class="page_item page-item-366"><a href="http://www.naturalearthdata.com/corrections" title="Corrections">Corrections</a></li>
 <li class="page_item page-item-16"><a href="http://www.naturalearthdata.com/about/" title="About">About</a></li>
-</ul>    
+</ul>
 </div>
 
 <hr />	<div id="main">
 	<div id="content" class="narrowcolumn">
 
-				
+
 									&laquo; <a href="http://www.naturalearthdata.com/downloads/110m-cultural-vectors/">1:110m Cultural Vectors</a>&nbsp;
 						   <div class="post" id="post-1556">
        		<h2>Admin 0 &#8211; Countries</h2>
 			<div class="entry">
 				<div class="downloadPromoBlock">
 <div style="float: left; width: 170px;"><img class="alignleft size-thumbnail wp-image-92" title="home_image_3" src="http://www.naturalearthdata.com/wp-content/uploads/2009/09/countries_thumnail.png" alt="countries_thumb" width="150" height="97" /></div>
 <div style="float: left; width: 410px;"><em>There are 247 countries in the world. Greenland as separate from Denmark.</em></p>
 <div class="download-link-div">
-	<a class="download-link" rel="nofollow" title="Downloaded 11103 times (Shapefile, geoDB, or TIFF format)" onclick="if (window.urchinTracker) urchinTracker ('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries.zip');" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries.zip" onclick="javascript:pageTracker._trackPageview('/downloads/http///download/110m/cultural/ne_110m_admin_0_countries.zip');">Download countries</a> <span class="download-link-span">(184.01 KB) version 2.0.0</span> 
+	<a class="download-link" rel="nofollow" title="Downloaded 11103 times (Shapefile, geoDB, or TIFF format)" onclick="if (window.urchinTracker) urchinTracker ('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries.zip');" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries.zip" onclick="javascript:pageTracker._trackPageview('/downloads/http///download/110m/cultural/ne_110m_admin_0_countries.zip');">Download countries</a> <span class="download-link-span">(184.01 KB) version 2.0.0</span>
 </div>
 <div class="download-link-div">
-	<a class="download-link" rel="nofollow" title="Downloaded 4 times (Shapefile, geoDB, or TIFF format)" onclick="if (window.urchinTracker) urchinTracker ('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries_lakes.zip');" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries_lakes.zip" onclick="javascript:pageTracker._trackPageview('/downloads/http///download/110m/cultural/ne_110m_admin_0_countries_lakes.zip');">Download without boundary lakes</a> <span class="download-link-span">(185.75 KB) version 2.0.0</span> 
+	<a class="download-link" rel="nofollow" title="Downloaded 4 times (Shapefile, geoDB, or TIFF format)" onclick="if (window.urchinTracker) urchinTracker ('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries_lakes.zip');" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries_lakes.zip" onclick="javascript:pageTracker._trackPageview('/downloads/http///download/110m/cultural/ne_110m_admin_0_countries_lakes.zip');">Download without boundary lakes</a> <span class="download-link-span">(185.75 KB) version 2.0.0</span>
 </div>
 <p><span id="more-1556"></span></div>
 </div>
 <div class="downloadMainBlock">
 <p><img class="alignnone size-full wp-image-1896" title="countries_banner" src="http://www.naturalearthdata.com/wp-content/uploads/2009/09/countries_banner1.png" alt="countries_banner" width="580" height="150" /></p>
 <p><strong>About</strong></p>
 <p>Countries distinguish between metropolitan (homeland) and independent and semi-independent portions of sovereign states. If you want to see the dependent overseas regions broken out (like in ISO codes, see France for example), use <a href="http://www.naturalearthdata.com/downloads/10m-political-vectors/10m-admin-0-nitty-gritty/" >map units</a> instead.</p>
@@ -144,27 +144,27 @@
 <p><strong>Disclaimer</strong></p>
 <p>Natural Earth Vector draws boundaries of countries according to defacto status. We show who actually controls the situation on the ground. Please feel free to mashup our disputed areas (link) theme to match your particular political outlook.</p>
 <p><strong>Known Problems</strong></p>
 <p>None.</p>
 <p><strong>Version History</strong></p>
 	<ul>
 					<li>
-									<a rel="nofollow" title="Download version 2.0.0 of ne_110m_admin_0_countries.zip" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries.zip" onclick="javascript:pageTracker._trackPageview('/downloads/http///download/110m/cultural/ne_110m_admin_0_countries.zip');">2.0.0</a>								
+									<a rel="nofollow" title="Download version 2.0.0 of ne_110m_admin_0_countries.zip" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries.zip" onclick="javascript:pageTracker._trackPageview('/downloads/http///download/110m/cultural/ne_110m_admin_0_countries.zip');">2.0.0</a>
 							</li>
 					<li>
-									1.4.0								
+									1.4.0
 							</li>
 					<li>
-									1.3.0								
+									1.3.0
 							</li>
 					<li>
-									1.1.0								
+									1.1.0
 							</li>
 					<li>
-									1.0.0								
+									1.0.0
 							</li>
 			</ul>
 
 <p><a href="https://github.com/nvkelso/natural-earth-vector/blob/master/CHANGELOG" onclick="javascript:pageTracker._trackPageview('/outbound/article/https://github.com/nvkelso/natural-earth-vector/blob/master/CHANGELOG');">The master changelog is available on Github </a>
 </div>
 
 <div class="sociable">
@@ -181,19 +181,19 @@
 	<li><a rel="nofollow"  target="_blank" href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fwww.naturalearthdata.com%2Fdownloads%2F110m-cultural-vectors%2F110m-admin-0-countries%2F&amp;title=Admin%200%20-%20Countries" onclick="javascript:pageTracker._trackPageview('/outbound/article/http://www.stumbleupon.com/submit?url=http%3A%2F%2Fwww.naturalearthdata.com%2Fdownloads%2F110m-cultural-vectors%2F110m-admin-0-countries%2F&amp;title=Admin%200%20-%20Countries');" title="StumbleUpon"><img src="http://www.naturalearthdata.com/wp-content/plugins/sociable/images/services-sprite.gif" title="StumbleUpon" alt="StumbleUpon" style="width: 16px; height: 16px; background: transparent url(http://www.naturalearthdata.com/wp-content/plugins/sociable/images/services-sprite.png) no-repeat; background-position:-217px -55px" class="sociable-hovers" /></a></li>
 	<li><a rel="nofollow"  target="_blank" href="mailto:?subject=Admin%200%20-%20Countries&amp;body=http%3A%2F%2Fwww.naturalearthdata.com%2Fdownloads%2F110m-cultural-vectors%2F110m-admin-0-countries%2F" title="email"><img src="http://www.naturalearthdata.com/wp-content/plugins/sociable/images/services-sprite.gif"  title="email" alt="email" style="width: 16px; height: 16px; background: transparent url(http://www.naturalearthdata.com/wp-content/plugins/sociable/images/services-sprite.png) no-repeat; background-position:-325px -1px" class="sociable-hovers" /></a></li>
 	<li><a rel="nofollow"  target="_blank" href="http://www.linkedin.com/shareArticle?mini=true&amp;url=http%3A%2F%2Fwww.naturalearthdata.com%2Fdownloads%2F110m-cultural-vectors%2F110m-admin-0-countries%2F&amp;title=Admin%200%20-%20Countries&amp;source=Natural+Earth+Free+vector+and+raster+map+data+at+1%3A10m%2C+1%3A50m%2C+and+1%3A110m+scales&amp;summary=%0D%0A%0D%0AThere%20are%20247%20countries%20in%20the%20world.%20Greenland%20as%20separate%20from%20Denmark.%0D%0A%0D%0A%5Bdrain%20file%20183%20show%20nev_download%5D%0D%0A%0D%0A%5Bdrain%20file%20361%20show%20nev_download%5D%0D%0A%0D%0A%0D%0A%0D%0A%0D%0A%0D%0A%0D%0A%0D%0AAbout%0D%0A%0D%0ACountries%20distinguish%20between%20metropolitan%20%28homeland%29%20and%20independent%20an" onclick="javascript:pageTracker._trackPageview('/outbound/article/http://www.linkedin.com/shareArticle?mini=true&amp;url=http%3A%2F%2Fwww.naturalearthdata.com%2Fdownloads%2F110m-cultural-vectors%2F110m-admin-0-countries%2F&amp;title=Admin%200%20-%20Countries&amp;source=Natural+Earth+Free+vector+and+raster+map+data+at+1%3A10m%2C+1%3A50m%2C+and+1%3A110m+scales&amp;summary=%0D%0A%0D%0AThere%20are%20247%20countries%20in%20the%20world.%20Greenland%20as%20separate%20from%20Denmark.%0D%0A%0D%0A%5Bdrain%20file%20183%20show%20nev_download%5D%0D%0A%0D%0A%5Bdrain%20file%20361%20show%20nev_download%5D%0D%0A%0D%0A%0D%0A%0D%0A%0D%0A%0D%0A%0D%0A%0D%0AAbout%0D%0A%0D%0ACountries%20distinguish%20between%20metropolitan%20%28homeland%29%20and%20independent%20an');" title="LinkedIn"><img src="http://www.naturalearthdata.com/wp-content/plugins/sociable/images/services-sprite.gif" title="LinkedIn" alt="LinkedIn" style="width: 16px; height: 16px; background: transparent url(http://www.naturalearthdata.com/wp-content/plugins/sociable/images/services-sprite.png) no-repeat; background-position:-1px -37px" class="sociable-hovers" /></a></li>
 	<li class="sociablelast"><a rel="nofollow"  target="_blank" href="http://reddit.com/submit?url=http%3A%2F%2Fwww.naturalearthdata.com%2Fdownloads%2F110m-cultural-vectors%2F110m-admin-0-countries%2F&amp;title=Admin%200%20-%20Countries" onclick="javascript:pageTracker._trackPageview('/outbound/article/http://reddit.com/submit?url=http%3A%2F%2Fwww.naturalearthdata.com%2Fdownloads%2F110m-cultural-vectors%2F110m-admin-0-countries%2F&amp;title=Admin%200%20-%20Countries');" title="Reddit"><img src="http://www.naturalearthdata.com/wp-content/plugins/sociable/images/services-sprite.gif" title="Reddit" alt="Reddit" style="width: 16px; height: 16px; background: transparent url(http://www.naturalearthdata.com/wp-content/plugins/sociable/images/services-sprite.png) no-repeat; background-position:-55px -55px" class="sociable-hovers" /></a></li>
 </ul>
 </div>
 
-				
+
 			</div>
 
 		</div>
-		
+
 
 		</div>
 
 
 	<div id="sidebar">
     	<ul><li id='text-5' class='widget widget_text'><h2 class="widgettitle">Stay up to Date</h2>
 			<div class="textwidget"> Know when a new version of Natural Earth is released by subscribing to our <a href="http://www.naturalearthdata.com/updates/" class="up-to-date-link" >announcement list</a>.</div>
@@ -232,17 +232,17 @@
         </div>
         <div class="footer-ad-box">
         	<a href="http://www.shadedrelief.com" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/shaded_relief.png" alt="Shaded Relief" /></a>
         </div>
         <div class="footer-ad-box">
         	<a href="http://www.xnrproductions.com " target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/xnr.png" alt="XNR Productions" /></a>
         </div>
-        
+
         <p style="clear:both;"></p>
-        
+
        <div class="footer-ad-box">
         	<a href="http://www.freac.fsu.edu" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/fsu.png" alt="Florida State University - FREAC" /></a>
         </div>
         <div class="footer-ad-box">
         	<a href="http://www.springercartographics.com" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/scllc.png" alt="Springer Cartographics LLC" /></a>
         </div>
         <div class="footer-ad-box">
@@ -250,51 +250,51 @@
         </div>
         <div class="footer-ad-box">
         	<a href="http://www.redgeographics.com" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/redgeo.png" alt="Red Geographics" /></a>
         </div>
         <div class="footer-ad-box">
         	<a href="http://kelsocartography.com/blog " target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/kelso.png" alt="Kelso Cartography" /></a>
         </div>
-        
+
         <p style="clear:both;"></p>
         <div class="footer-ad-box">
         	<a href="http://www.avenza.com" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/avenza.png" alt="Avenza Systems Inc." /></a>
         </div>
         <div class="footer-ad-box">
         	<a href="http://www.stamen.com" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/stamen_ne_logo.png" alt="Stamen Design" /></a>
         </div>
 
 
     </div>
     <p style="clear:both;"></p>
 	<span id="footerleft">
 		&copy; 2012. Natural Earth. All rights reserved.
 	</span>
-    <span id="footerright"> 
-    	<!-- Please help promote WordPress and simpleX. Do not remove -->   
+    <span id="footerright">
+    	<!-- Please help promote WordPress and simpleX. Do not remove -->
 		<div>Powered by <a href="http://wordpress.org/">WordPress</a></div>
         <div><a href="http://www.naturalearthdata.com/wp-admin">Staff Login &raquo;</a></div>
     </span>
 </div>
 </div>
-		
+
 <!-- Google Analytics for WordPress | http://yoast.com/wordpress/google-analytics/ -->
 <script type="text/javascript">
 	var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
 	document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
 </script>
 <script type="text/javascript">
 	try {
 		var pageTracker = _gat._getTracker("UA-10168306-1");
 	} catch(err) {}
 </script>
 <script src="http://www.naturalearthdata.com/wp-content/plugins/google-analytics-for-wordpress/custom_se.js" type="text/javascript"></script>
 <script type="text/javascript">
 	try {
-		// Cookied already: 
+		// Cookied already:
 		pageTracker._trackPageview();
 	} catch(err) {}
 </script>
 <!-- End of Google Analytics code -->
 
 </body>
-</html>
+</html>
```

### Comparing `uwsift-1.2.3/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.dbf` & `uwsift-2.0.0b0/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.dbf`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp` & `uwsift-2.0.0b0/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shx` & `uwsift-2.0.0b0/uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shx`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.README.html` & `uwsift-2.0.0b0/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.README.html`

 * *Files 1% similar despite different names*

```diff
@@ -11,22 +11,22 @@
 <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
 <link rel="alternate" type="application/rss+xml" title="Natural Earth RSS Feed" href="http://www.naturalearthdata.com/feed/" />
 <link rel="pingback" href="http://www.naturalearthdata.com/xmlrpc.php" />
 <script type="text/javascript" src="http://www.naturalearthdata.com/wp-content/themes/NEV/includes/js/suckerfish.js"></script>
 <!--[if lt IE 7]>
     <script src="http://ie7-js.googlecode.com/svn/version/2.0(beta3)/IE7.js" type="text/javascript"></script>
     <script defer="defer" type="text/javascript" src="http://www.naturalearthdata.com/wp-content/themes/NEV/includes/js/pngfix.js"></script>
-<![endif]--> 
+<![endif]-->
 <link rel="stylesheet" href="http://www.naturalearthdata.com/wp-content/themes/NEV/style.css" type="text/css" media="screen" />
 
 <meta name='Admin Management Xtended WordPress plugin' content='2.1.1' />
 <link rel="alternate" type="application/rss+xml" title="Natural Earth &raquo; Admin 0 &#8211; Countries Comments Feed" href="http://www.naturalearthdata.com/downloads/50m-cultural-vectors/50m-admin-0-countries-2/feed/" />
 <link rel='stylesheet' id='sociable-front-css-css'  href='http://www.naturalearthdata.com/wp-content/plugins/sociable/sociable.css?ver=2.9.2' type='text/css' media='' />
 <link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://www.naturalearthdata.com/xmlrpc.php?rsd" />
-<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.naturalearthdata.com/wp-includes/wlwmanifest.xml" /> 
+<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.naturalearthdata.com/wp-includes/wlwmanifest.xml" />
 <link rel='index' title='Natural Earth' href='http://www.naturalearthdata.com' />
 <link rel='start' title='Welcome to the Natural Earth Blog' href='http://www.naturalearthdata.com/blog/miscellaneous/test/' />
 <link rel='prev' title='Admin 0 &#8211; Details' href='http://www.naturalearthdata.com/downloads/50m-cultural-vectors/50m-admin-0-details/' />
 <link rel='next' title='Lakes + Reservoirs' href='http://www.naturalearthdata.com/downloads/110m-physical-vectors/110mlakes-reservoirs/' />
 <meta name="generator" content="WordPress 2.9.2" />
 
 <!-- All in One SEO Pack 1.6.10.2 by Michael Torbert of Semper Fi Web Design[309,457] -->
@@ -52,41 +52,41 @@
 <script>
      jQuery.noConflict();
 </script>
 <script type="text/javascript" charset="utf-8">
 	$(function(){
 		var tabContainers = $('div#maintabdiv > div');
 		tabContainers.hide().filter('#comments').show();
-		
+
 		$('div#maintabdiv ul#tabnav a').click(function () {
 				tabContainers.hide();
 				tabContainers.filter(this.hash).show();
 				$('div#maintabdiv ul#tabnav a').removeClass('current');
 				$(this).addClass('current');
 				return false;
 			}).filter('#comments').click();
-		
-		
+
+
 	});
 </script>
 
 		<script type="text/javascript" language="javascript" src="http://www.naturalearthdata.com/dataTables/media/js/jquery.dataTables.js"></script>
 		<script type="text/javascript" charset="utf-8">
 			$(document).ready(function() {
 				$('#ne_table').dataTable();
 			} );
 		</script>
 
 </head>
 <body>
 <div id="page">
 <div id="header">
-	<div id="headerimg">		
-        <h1><a href="http://www.naturalearthdata.com/"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/nev_logo.png" alt="Natural Earth title="Natural Earth" /></a></h1> 
-        <div class="description">Free vector and raster map data at 1:10m, 1:50m, and 1:110m scales</div> 
+	<div id="headerimg">
+        <h1><a href="http://www.naturalearthdata.com/"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/nev_logo.png" alt="Natural Earth title="Natural Earth" /></a></h1>
+        <div class="description">Free vector and raster map data at 1:10m, 1:50m, and 1:110m scales</div>
         <div class="header_search"><form method="get" id="searchform" action="http://www.naturalearthdata.com/">
 <label class="hidden" for="s">Search for:</label>
 <div><input type="text" value="" name="s" id="s" />
 <input type="submit" id="searchsubmit" value="Search" />
 </div>
 </form>
 </div>
@@ -98,44 +98,44 @@
    pageLanguage: 'en'
  }, 'google_translate_element');
 }
 </script>
 <script src="http://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script>
 </div>-->
 	</div>
-    
+
 </div>
 
 <div id="pagemenu" style="align:bottom;">
     <ul id="page-list" class="clearfix"><li class="page_item page-item-4"><a href="http://www.naturalearthdata.com" title="Home">Home</a></li>
 <li class="page_item page-item-10"><a href="http://www.naturalearthdata.com/features/" title="Features">Features</a></li>
 <li class="page_item page-item-12"><a href="http://www.naturalearthdata.com/downloads/" title="Downloads">Downloads</a></li>
 <li class="page_item page-item-6 current_page_parent"><a href="http://www.naturalearthdata.com/blog/" title="Blog">Blog</a></li>
 <li class="page_item page-item-14"><a href="http://www.naturalearthdata.com/forums" title="Forums">Forums</a></li>
 <li class="page_item page-item-366"><a href="http://www.naturalearthdata.com/corrections" title="Corrections">Corrections</a></li>
 <li class="page_item page-item-16"><a href="http://www.naturalearthdata.com/about/" title="About">About</a></li>
-</ul>    
+</ul>
 </div>
 
 <hr />	<div id="main">
 	<div id="content" class="narrowcolumn">
 
-				
+
 									&laquo; <a href="http://www.naturalearthdata.com/downloads/50m-cultural-vectors/">1:50m Cultural Vectors</a>&nbsp;
 						   <div class="post" id="post-1541">
        		<h2>Admin 0 &#8211; Countries</h2>
 			<div class="entry">
 				<div class="downloadPromoBlock">
 <div style="float: left; width: 170px;"><img class="alignleft size-thumbnail wp-image-92" title="home_image_3" src="http://www.naturalearthdata.com/wp-content/uploads/2009/09/countries_thumnail.png" alt="countries_thumb" width="150" height="97" /></div>
 <div style="float: left; width: 410px;"><em>There are 247 countries in the world. Greenland as separate from Denmark. Most users will want this file instead of sovereign states.</em></p>
 <div class="download-link-div">
-	<a class="download-link" rel="nofollow" title="Downloaded 7306 times (Shapefile, geoDB, or TIFF format)" onclick="if (window.urchinTracker) urchinTracker ('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_0_countries.zip');" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_0_countries.zip" onclick="javascript:pageTracker._trackPageview('/downloads/http///download/50m/cultural/ne_50m_admin_0_countries.zip');">Download countries</a> <span class="download-link-span">(798.39 KB) version 2.0.0</span> 
+	<a class="download-link" rel="nofollow" title="Downloaded 7306 times (Shapefile, geoDB, or TIFF format)" onclick="if (window.urchinTracker) urchinTracker ('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_0_countries.zip');" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_0_countries.zip" onclick="javascript:pageTracker._trackPageview('/downloads/http///download/50m/cultural/ne_50m_admin_0_countries.zip');">Download countries</a> <span class="download-link-span">(798.39 KB) version 2.0.0</span>
 </div>
 <div class="download-link-div">
-	<a class="download-link" rel="nofollow" title="Downloaded 3 times (Shapefile, geoDB, or TIFF format)" onclick="if (window.urchinTracker) urchinTracker ('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_0_countries_lakes.zip');" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_0_countries_lakes.zip" onclick="javascript:pageTracker._trackPageview('/downloads/http///download/50m/cultural/ne_50m_admin_0_countries_lakes.zip');">Download without boundary lakes</a> <span class="download-link-span">(854.08 KB) version 2.0.0</span> 
+	<a class="download-link" rel="nofollow" title="Downloaded 3 times (Shapefile, geoDB, or TIFF format)" onclick="if (window.urchinTracker) urchinTracker ('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_0_countries_lakes.zip');" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_0_countries_lakes.zip" onclick="javascript:pageTracker._trackPageview('/downloads/http///download/50m/cultural/ne_50m_admin_0_countries_lakes.zip');">Download without boundary lakes</a> <span class="download-link-span">(854.08 KB) version 2.0.0</span>
 </div>
 <p><span id="more-1541"></span></div>
 </div>
 <div class="downloadMainBlock">
 <p><img class="alignnone size-full wp-image-1896" title="countries_banner" src="http://www.naturalearthdata.com/wp-content/uploads/2009/09/countries_banner1.png" alt="countries_banner" width="580" height="150" /></p>
 <p><strong>About</strong></p>
 <p>Countries distinguish between metropolitan (homeland) and independent and semi-independent portions of sovereign states. If you want to see the dependent overseas regions broken out (like in ISO codes, see France for example), use <a href="http://www.naturalearthdata.com/downloads/10m-political-vectors/10m-admin-0-nitty-gritty/" >map units</a> instead.</p>
@@ -144,27 +144,27 @@
 <p><strong>Disclaimer</strong></p>
 <p>Natural Earth Vector draws boundaries of countries according to defacto status. We show who actually controls the situation on the ground. Please feel free to mashup our disputed areas (link) theme to match your particular political outlook.</p>
 <p><strong>Known Problems</strong></p>
 <p>None.</p>
 <p><strong>Version History</strong></p>
 	<ul>
 					<li>
-									<a rel="nofollow" title="Download version 2.0.0 of ne_50m_admin_0_countries.zip" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_0_countries.zip" onclick="javascript:pageTracker._trackPageview('/downloads/http///download/50m/cultural/ne_50m_admin_0_countries.zip');">2.0.0</a>								
+									<a rel="nofollow" title="Download version 2.0.0 of ne_50m_admin_0_countries.zip" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_0_countries.zip" onclick="javascript:pageTracker._trackPageview('/downloads/http///download/50m/cultural/ne_50m_admin_0_countries.zip');">2.0.0</a>
 							</li>
 					<li>
-									1.4.0								
+									1.4.0
 							</li>
 					<li>
-									1.3.0								
+									1.3.0
 							</li>
 					<li>
-									1.1.0								
+									1.1.0
 							</li>
 					<li>
-									1.0.0								
+									1.0.0
 							</li>
 			</ul>
 
 <p><a href="https://github.com/nvkelso/natural-earth-vector/blob/master/CHANGELOG" onclick="javascript:pageTracker._trackPageview('/outbound/article/https://github.com/nvkelso/natural-earth-vector/blob/master/CHANGELOG');">The master changelog is available on Github </a>
 </div>
 
 <div class="sociable">
@@ -181,19 +181,19 @@
 	<li><a rel="nofollow"  target="_blank" href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fwww.naturalearthdata.com%2Fdownloads%2F50m-cultural-vectors%2F50m-admin-0-countries-2%2F&amp;title=Admin%200%20-%20Countries" onclick="javascript:pageTracker._trackPageview('/outbound/article/http://www.stumbleupon.com/submit?url=http%3A%2F%2Fwww.naturalearthdata.com%2Fdownloads%2F50m-cultural-vectors%2F50m-admin-0-countries-2%2F&amp;title=Admin%200%20-%20Countries');" title="StumbleUpon"><img src="http://www.naturalearthdata.com/wp-content/plugins/sociable/images/services-sprite.gif" title="StumbleUpon" alt="StumbleUpon" style="width: 16px; height: 16px; background: transparent url(http://www.naturalearthdata.com/wp-content/plugins/sociable/images/services-sprite.png) no-repeat; background-position:-217px -55px" class="sociable-hovers" /></a></li>
 	<li><a rel="nofollow"  target="_blank" href="mailto:?subject=Admin%200%20-%20Countries&amp;body=http%3A%2F%2Fwww.naturalearthdata.com%2Fdownloads%2F50m-cultural-vectors%2F50m-admin-0-countries-2%2F" title="email"><img src="http://www.naturalearthdata.com/wp-content/plugins/sociable/images/services-sprite.gif"  title="email" alt="email" style="width: 16px; height: 16px; background: transparent url(http://www.naturalearthdata.com/wp-content/plugins/sociable/images/services-sprite.png) no-repeat; background-position:-325px -1px" class="sociable-hovers" /></a></li>
 	<li><a rel="nofollow"  target="_blank" href="http://www.linkedin.com/shareArticle?mini=true&amp;url=http%3A%2F%2Fwww.naturalearthdata.com%2Fdownloads%2F50m-cultural-vectors%2F50m-admin-0-countries-2%2F&amp;title=Admin%200%20-%20Countries&amp;source=Natural+Earth+Free+vector+and+raster+map+data+at+1%3A10m%2C+1%3A50m%2C+and+1%3A110m+scales&amp;summary=%0D%0A%0D%0AThere%20are%20247%20countries%20in%20the%20world.%20Greenland%20as%20separate%20from%20Denmark.%20Most%20users%20will%20want%20this%20file%20instead%20of%20sovereign%20states.%0D%0A%0D%0A%5Bdrain%20file%20114%20show%20nev_download%5D%0D%0A%0D%0A%5Bdrain%20file%20350%20show%20nev_download%5D%0D%0A%0D%0A%0D%0A%0D%0A%0D%0A%0D%0A%0D%0A%0D%0AAbout%0D%0A%0D%0ACountries%20di" onclick="javascript:pageTracker._trackPageview('/outbound/article/http://www.linkedin.com/shareArticle?mini=true&amp;url=http%3A%2F%2Fwww.naturalearthdata.com%2Fdownloads%2F50m-cultural-vectors%2F50m-admin-0-countries-2%2F&amp;title=Admin%200%20-%20Countries&amp;source=Natural+Earth+Free+vector+and+raster+map+data+at+1%3A10m%2C+1%3A50m%2C+and+1%3A110m+scales&amp;summary=%0D%0A%0D%0AThere%20are%20247%20countries%20in%20the%20world.%20Greenland%20as%20separate%20from%20Denmark.%20Most%20users%20will%20want%20this%20file%20instead%20of%20sovereign%20states.%0D%0A%0D%0A%5Bdrain%20file%20114%20show%20nev_download%5D%0D%0A%0D%0A%5Bdrain%20file%20350%20show%20nev_download%5D%0D%0A%0D%0A%0D%0A%0D%0A%0D%0A%0D%0A%0D%0A%0D%0AAbout%0D%0A%0D%0ACountries%20di');" title="LinkedIn"><img src="http://www.naturalearthdata.com/wp-content/plugins/sociable/images/services-sprite.gif" title="LinkedIn" alt="LinkedIn" style="width: 16px; height: 16px; background: transparent url(http://www.naturalearthdata.com/wp-content/plugins/sociable/images/services-sprite.png) no-repeat; background-position:-1px -37px" class="sociable-hovers" /></a></li>
 	<li class="sociablelast"><a rel="nofollow"  target="_blank" href="http://reddit.com/submit?url=http%3A%2F%2Fwww.naturalearthdata.com%2Fdownloads%2F50m-cultural-vectors%2F50m-admin-0-countries-2%2F&amp;title=Admin%200%20-%20Countries" onclick="javascript:pageTracker._trackPageview('/outbound/article/http://reddit.com/submit?url=http%3A%2F%2Fwww.naturalearthdata.com%2Fdownloads%2F50m-cultural-vectors%2F50m-admin-0-countries-2%2F&amp;title=Admin%200%20-%20Countries');" title="Reddit"><img src="http://www.naturalearthdata.com/wp-content/plugins/sociable/images/services-sprite.gif" title="Reddit" alt="Reddit" style="width: 16px; height: 16px; background: transparent url(http://www.naturalearthdata.com/wp-content/plugins/sociable/images/services-sprite.png) no-repeat; background-position:-55px -55px" class="sociable-hovers" /></a></li>
 </ul>
 </div>
 
-				
+
 			</div>
 
 		</div>
-		
+
 
 		</div>
 
 
 	<div id="sidebar">
     	<ul><li id='text-5' class='widget widget_text'><h2 class="widgettitle">Stay up to Date</h2>
 			<div class="textwidget"> Know when a new version of Natural Earth is released by subscribing to our <a href="http://www.naturalearthdata.com/updates/" class="up-to-date-link" >announcement list</a>.</div>
@@ -232,17 +232,17 @@
         </div>
         <div class="footer-ad-box">
         	<a href="http://www.shadedrelief.com" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/shaded_relief.png" alt="Shaded Relief" /></a>
         </div>
         <div class="footer-ad-box">
         	<a href="http://www.xnrproductions.com " target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/xnr.png" alt="XNR Productions" /></a>
         </div>
-        
+
         <p style="clear:both;"></p>
-        
+
        <div class="footer-ad-box">
         	<a href="http://www.freac.fsu.edu" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/fsu.png" alt="Florida State University - FREAC" /></a>
         </div>
         <div class="footer-ad-box">
         	<a href="http://www.springercartographics.com" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/scllc.png" alt="Springer Cartographics LLC" /></a>
         </div>
         <div class="footer-ad-box">
@@ -250,51 +250,51 @@
         </div>
         <div class="footer-ad-box">
         	<a href="http://www.redgeographics.com" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/redgeo.png" alt="Red Geographics" /></a>
         </div>
         <div class="footer-ad-box">
         	<a href="http://kelsocartography.com/blog " target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/kelso.png" alt="Kelso Cartography" /></a>
         </div>
-        
+
         <p style="clear:both;"></p>
         <div class="footer-ad-box">
         	<a href="http://www.avenza.com" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/avenza.png" alt="Avenza Systems Inc." /></a>
         </div>
         <div class="footer-ad-box">
         	<a href="http://www.stamen.com" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/stamen_ne_logo.png" alt="Stamen Design" /></a>
         </div>
 
 
     </div>
     <p style="clear:both;"></p>
 	<span id="footerleft">
 		&copy; 2012. Natural Earth. All rights reserved.
 	</span>
-    <span id="footerright"> 
-    	<!-- Please help promote WordPress and simpleX. Do not remove -->   
+    <span id="footerright">
+    	<!-- Please help promote WordPress and simpleX. Do not remove -->
 		<div>Powered by <a href="http://wordpress.org/">WordPress</a></div>
         <div><a href="http://www.naturalearthdata.com/wp-admin">Staff Login &raquo;</a></div>
     </span>
 </div>
 </div>
-		
+
 <!-- Google Analytics for WordPress | http://yoast.com/wordpress/google-analytics/ -->
 <script type="text/javascript">
 	var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
 	document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
 </script>
 <script type="text/javascript">
 	try {
 		var pageTracker = _gat._getTracker("UA-10168306-1");
 	} catch(err) {}
 </script>
 <script src="http://www.naturalearthdata.com/wp-content/plugins/google-analytics-for-wordpress/custom_se.js" type="text/javascript"></script>
 <script type="text/javascript">
 	try {
-		// Cookied already: 
+		// Cookied already:
 		pageTracker._trackPageview();
 	} catch(err) {}
 </script>
 <!-- End of Google Analytics code -->
 
 </body>
-</html>
+</html>
```

### Comparing `uwsift-1.2.3/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.dbf` & `uwsift-2.0.0b0/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.dbf`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp` & `uwsift-2.0.0b0/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shx` & `uwsift-2.0.0b0/uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shx`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.README.html` & `uwsift-2.0.0b0/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.README.html`

 * *Files 3% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
 <link rel="alternate" type="application/rss+xml" title="Natural Earth RSS Feed" href="http://www.naturalearthdata.com/feed/" />
 <link rel="pingback" href="http://www.naturalearthdata.com/xmlrpc.php" />
 <script type="text/javascript" src="http://www.naturalearthdata.com/wp-content/themes/NEV/includes/js/suckerfish.js"></script>
 <!--[if lt IE 7]>
     <script src="http://ie7-js.googlecode.com/svn/version/2.0(beta3)/IE7.js" type="text/javascript"></script>
     <script defer="defer" type="text/javascript" src="http://www.naturalearthdata.com/wp-content/themes/NEV/includes/js/pngfix.js"></script>
-<![endif]--> 
+<![endif]-->
 <link rel="stylesheet" href="http://www.naturalearthdata.com/wp-content/themes/NEV/style.css" type="text/css" media="screen" />
 
 
             <script type="text/javascript">//<![CDATA[
             // Google Analytics for WordPress by Yoast v4.3.2 | http://yoast.com/wordpress/google-analytics/
             var _gaq = _gaq || [];
             _gaq.push(['_setAccount', 'UA-10168306-1']);
@@ -33,15 +33,15 @@
 
                 var s = document.getElementsByTagName('script')[0];
                 s.parentNode.insertBefore(ga, s);
             })();
             //]]></script>
 			<link rel='stylesheet' id='bbp-child-bbpress-css'  href='http://www.naturalearthdata.com/wp-content/themes/NEV/css/bbpress.css?ver=2.2.4' type='text/css' media='screen' />
 <link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://www.naturalearthdata.com/xmlrpc.php?rsd" />
-<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.naturalearthdata.com/wp-includes/wlwmanifest.xml" /> 
+<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.naturalearthdata.com/wp-includes/wlwmanifest.xml" />
 <link rel='prev' title='Populated Places' href='http://www.naturalearthdata.com/downloads/50m-cultural-vectors/50m-populated-places/' />
 <link rel='next' title='Admin 0 &#8211; Breakaway, disputed areas' href='http://www.naturalearthdata.com/downloads/50m-cultural-vectors/50m-admin-0-breakaway-disputed-areas/' />
 <meta name="generator" content="WordPress 3.5.1" />
 <link rel='shortlink' href='http://www.naturalearthdata.com/?p=1518' />
 
 <!-- All in One SEO Pack 1.6.15.3 by Michael Torbert of Semper Fi Web Design[299,455] -->
 <meta name="description" content="Internal administrative boundaries. About Internal, first-order administrative boundaries and polygons for just the United" />
@@ -51,15 +51,15 @@
 		<script type="text/javascript">
 			/* <![CDATA[ */
 			var ajaxurl = 'http://www.naturalearthdata.com/wp-admin/admin-ajax.php';
 
 						/* ]]> */
 		</script>
 
-	
+
 	<!-- begin gallery scripts -->
     <link rel="stylesheet" href="http://www.naturalearthdata.com/wp-content/plugins/featured-content-gallery/css/jd.gallery.css.php" type="text/css" media="screen" charset="utf-8"/>
 	<link rel="stylesheet" href="http://www.naturalearthdata.com/wp-content/plugins/featured-content-gallery/css/jd.gallery.css" type="text/css" media="screen" charset="utf-8"/>
 	<script type="text/javascript" src="http://www.naturalearthdata.com/wp-content/plugins/featured-content-gallery/scripts/mootools.v1.11.js"></script>
 	<script type="text/javascript" src="http://www.naturalearthdata.com/wp-content/plugins/featured-content-gallery/scripts/jd.gallery.js.php"></script>
 	<script type="text/javascript" src="http://www.naturalearthdata.com/wp-content/plugins/featured-content-gallery/scripts/jd.gallery.transitions.js"></script>
 	<!-- end gallery scripts -->
@@ -74,41 +74,41 @@
 <script>
      jQuery.noConflict();
 </script>
 <script type="text/javascript" charset="utf-8">
 	$(function(){
 		var tabContainers = $('div#maintabdiv > div');
 		tabContainers.hide().filter('#comments').show();
-		
+
 		$('div#maintabdiv ul#tabnav a').click(function () {
 				tabContainers.hide();
 				tabContainers.filter(this.hash).show();
 				$('div#maintabdiv ul#tabnav a').removeClass('current');
 				$(this).addClass('current');
 				return false;
 			}).filter('#comments').click();
-		
-		
+
+
 	});
 </script>
 
 		<script type="text/javascript" language="javascript" src="http://www.naturalearthdata.com/dataTables/media/js/jquery.dataTables.js"></script>
 		<script type="text/javascript" charset="utf-8">
 			$(document).ready(function() {
 				$('#ne_table').dataTable();
 			} );
 		</script>
 
 </head>
 <body>
 <div id="page">
 <div id="header">
-	<div id="headerimg">		
-        <h1><a href="http://www.naturalearthdata.com/"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/nev_logo.png" alt="Natural Earth title="Natural Earth" /></a></h1> 
-        <div class="description">Free vector and raster map data at 1:10m, 1:50m, and 1:110m scales</div> 
+	<div id="headerimg">
+        <h1><a href="http://www.naturalearthdata.com/"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/nev_logo.png" alt="Natural Earth title="Natural Earth" /></a></h1>
+        <div class="description">Free vector and raster map data at 1:10m, 1:50m, and 1:110m scales</div>
         <div class="header_search"><form method="get" id="searchform" action="http://www.naturalearthdata.com/">
 <label class="hidden" for="s">Search for:</label>
 <div><input type="text" value="" name="s" id="s" />
 <input type="submit" id="searchsubmit" value="Search" />
 </div>
 </form>
 </div>
@@ -120,51 +120,51 @@
    pageLanguage: 'en'
  }, 'google_translate_element');
 }
 </script>
 <script src="http://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script>
 </div>-->
 	</div>
-    
+
 </div>
 
 <div id="pagemenu" style="align:bottom;">
     <ul id="page-list" class="clearfix"><li class="page_item page-item-4"><a href="http://www.naturalearthdata.com/">Home</a></li>
 <li class="page_item page-item-10"><a href="http://www.naturalearthdata.com/features/">Features</a></li>
 <li class="page_item page-item-12"><a href="http://www.naturalearthdata.com/downloads/">Downloads</a></li>
 <li class="page_item page-item-6 current_page_parent"><a href="http://www.naturalearthdata.com/blog/">Blog</a></li>
 <li class="page_item page-item-14"><a href="http://www.naturalearthdata.com/forums">Forums</a></li>
 <li class="page_item page-item-366"><a href="http://www.naturalearthdata.com/corrections">Corrections</a></li>
 <li class="page_item page-item-16"><a href="http://www.naturalearthdata.com/about/">About</a></li>
-</ul>    
+</ul>
 </div>
 
 <hr />	<div id="main">
 	<div id="content" class="narrowcolumn">
 
-				
+
 									&laquo; <a href="http://www.naturalearthdata.com/downloads/50m-cultural-vectors/">1:50m Cultural Vectors</a>&nbsp;
 						   							&laquo; <a href="http://www.naturalearthdata.com/downloads/">Downloads</a>&nbsp;
 						   <div class="post" id="post-1518">
        		<h2>Admin 1 &#8211; States, provinces</h2>
 			<div class="entry">
 				<div class="downloadPromoBlock">
 <div style="float: left; width: 170px;"><img class="alignnone size-full wp-image-1914" title="states_thumb" src="http://www.naturalearthdata.com/wp-content/uploads/2009/09/states_thumb.png" alt="states_thumb" width="150" height="97" /></div>
 <div style="float: left; width: 410px;"><em>Internal administrative boundaries.</em></p>
 <div class="download-link-div">
-	<a class="download-link" rel="nofollow" title="Downloaded 3385 times (Shapefile, geoDB, or TIFF format)" onclick="if (window.urchinTracker) urchinTracker ('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lakes_shp.zip');" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lakes_shp.zip" onclick="javascript:_gaq.push(['_trackEvent','download','http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lakes_shp.zip']);">Download states and provinces (shp)</a> <span class="download-link-span">(449.64 KB) version 2.0.0</span> 
+	<a class="download-link" rel="nofollow" title="Downloaded 3385 times (Shapefile, geoDB, or TIFF format)" onclick="if (window.urchinTracker) urchinTracker ('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lakes_shp.zip');" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lakes_shp.zip" onclick="javascript:_gaq.push(['_trackEvent','download','http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lakes_shp.zip']);">Download states and provinces (shp)</a> <span class="download-link-span">(449.64 KB) version 2.0.0</span>
 </div>
 <div class="download-link-div">
-	<a class="download-link" rel="nofollow" title="Downloaded 2920 times (Shapefile, geoDB, or TIFF format)" onclick="if (window.urchinTracker) urchinTracker ('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lakes_shp.zip');" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lakes_shp.zip" onclick="javascript:_gaq.push(['_trackEvent','download','http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lakes_shp.zip']);">Download without large lakes (shp)</a> <span class="download-link-span">(449.64 KB) version 2.0.0</span> 
+	<a class="download-link" rel="nofollow" title="Downloaded 2920 times (Shapefile, geoDB, or TIFF format)" onclick="if (window.urchinTracker) urchinTracker ('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lakes_shp.zip');" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lakes_shp.zip" onclick="javascript:_gaq.push(['_trackEvent','download','http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lakes_shp.zip']);">Download without large lakes (shp)</a> <span class="download-link-span">(449.64 KB) version 2.0.0</span>
 </div>
 <div class="download-link-div">
-	<a class="download-link" rel="nofollow" title="Downloaded 2192 times (Shapefile, geoDB, or TIFF format)" onclick="if (window.urchinTracker) urchinTracker ('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lines.zip');" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lines.zip" onclick="javascript:_gaq.push(['_trackEvent','download','http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lines.zip']);">Download boundary lines</a> <span class="download-link-span">(98.56 KB) version 2.0.0</span> 
+	<a class="download-link" rel="nofollow" title="Downloaded 2192 times (Shapefile, geoDB, or TIFF format)" onclick="if (window.urchinTracker) urchinTracker ('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lines.zip');" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lines.zip" onclick="javascript:_gaq.push(['_trackEvent','download','http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lines.zip']);">Download boundary lines</a> <span class="download-link-span">(98.56 KB) version 2.0.0</span>
 </div>
 <div class="download-link-div">
-	<a class="download-link" rel="nofollow" title="Downloaded 242 times (Shapefile, geoDB, or TIFF format)" onclick="if (window.urchinTracker) urchinTracker ('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_shp_scale_rank.zip');" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_shp_scale_rank.zip" onclick="javascript:_gaq.push(['_trackEvent','download','http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_shp_scale_rank.zip']);">Download scale ranks</a> <span class="download-link-span">(432.77 KB) version 2.0.0</span> 
+	<a class="download-link" rel="nofollow" title="Downloaded 242 times (Shapefile, geoDB, or TIFF format)" onclick="if (window.urchinTracker) urchinTracker ('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_shp_scale_rank.zip');" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_shp_scale_rank.zip" onclick="javascript:_gaq.push(['_trackEvent','download','http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_shp_scale_rank.zip']);">Download scale ranks</a> <span class="download-link-span">(432.77 KB) version 2.0.0</span>
 </div>
 <p><span id="more-1518"></span></div>
 </div>
 <div class="downloadMainBlock">
 <p><img class="alignnone size-full wp-image-1916" title="states_banner" src="http://www.naturalearthdata.com/wp-content/uploads/2009/09/states_banner.png" alt="states_banner" width="580" height="150" /></p>
 <p><strong>About</strong></p>
 <p>Internal, first-order administrative boundaries and polygons for just the United States and Canada at this time. For more detailed breakdowns for most countries in the world, see 10m admin-1. Boundary lines do not duplicate each other. Boundary lines are only present on internal boundaries (not coastlines or admin-0 boundaries).</p>
@@ -175,32 +175,32 @@
 <p><strong>Issues</strong></p>
 <p>All countries with 10m admin-1 with scale ranks 1 to 5 should be present in this file. Currently only the United States and Canada are represented.</p>
 <p><em>Boundaries should perfectly match the following 50m NEV themes:</em></p>
 <p>Coastline, lake shoreline, admin-0 country boundary, river and lake centerlines.</p>
 <p><strong>Version History</strong></p>
 	<ul>
 					<li>
-									<a rel="nofollow" title="Download version 2.0.0 of ne_50m_admin_1_states_provinces_lakes_shp.zip" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lakes_shp.zip" onclick="javascript:_gaq.push(['_trackEvent','download','http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lakes_shp.zip']);">2.0.0</a>								
+									<a rel="nofollow" title="Download version 2.0.0 of ne_50m_admin_1_states_provinces_lakes_shp.zip" href="http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lakes_shp.zip" onclick="javascript:_gaq.push(['_trackEvent','download','http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_1_states_provinces_lakes_shp.zip']);">2.0.0</a>
 							</li>
 					<li>
-									1.3.0								
+									1.3.0
 							</li>
 					<li>
-									1.0.0								
+									1.0.0
 							</li>
 			</ul>
 
 <p><a href="https://github.com/nvkelso/natural-earth-vector/blob/master/CHANGELOG" onclick="javascript:_gaq.push(['_trackEvent','outbound-article','http://github.com/nvkelso/natural-earth-vector/blob/master/CHANGELOG']);">The master changelog is available on Github </a>
 </div>
 
-				
+
 			</div>
 
 		</div>
-		
+
 
 		</div>
 
 
 	<div id="sidebar">
     	<ul><li id='text-5' class='widget widget_text'><h2 class="widgettitle">Stay up to Date</h2>
 			<div class="textwidget"> Know when a new version of Natural Earth is released by subscribing to our <a href="http://www.naturalearthdata.com/updates/"  class="up-to-date-link" >announcement list</a>.</div>
@@ -216,101 +216,101 @@
 </div></div>
 		</li></ul><ul><li id='text-4' class='widget widget_text'><h2 class="widgettitle">Thank You</h2>
 			<div class="textwidget">Our data downloads are generously hosted by Florida State University.</div>
 		</li></ul><ul><li id='bbp_topics_widget-3' class='widget widget_display_topics'><h2 class="widgettitle">Recent Forum Topics</h2>
 
 			<ul>
 
-				
+
 					<li>
 						<a class="bbp-forum-title" href="http://www.naturalearthdata.com/forums/topic/osm-style-slippy-map/" title="OSM Style SLIPPY Map">OSM Style SLIPPY Map</a>
 
-						
-						
+
+
 					</li>
 
-				
+
 					<li>
 						<a class="bbp-forum-title" href="http://www.naturalearthdata.com/forums/topic/within-this-on-the-8/" title="Within this on the">Within this on the</a>
 
-						
-						
+
+
 					</li>
 
-				
+
 					<li>
 						<a class="bbp-forum-title" href="http://www.naturalearthdata.com/forums/topic/these-handpicked-brand/" title="These handpicked brand">These handpicked brand</a>
 
-						
-						
+
+
 					</li>
 
-				
+
 					<li>
 						<a class="bbp-forum-title" href="http://www.naturalearthdata.com/forums/topic/within-this-on-the-3/" title="Within this on the">Within this on the</a>
 
-						
-						
+
+
 					</li>
 
-				
+
 					<li>
 						<a class="bbp-forum-title" href="http://www.naturalearthdata.com/forums/topic/within-this-on-thewar/" title="Within this on thewar">Within this on thewar</a>
 
-						
-						
+
+
 					</li>
 
-				
+
 					<li>
 						<a class="bbp-forum-title" href="http://www.naturalearthdata.com/forums/topic/within-this-on-the-2/" title="Within this on the">Within this on the</a>
 
-						
-						
+
+
 					</li>
 
-				
+
 					<li>
 						<a class="bbp-forum-title" href="http://www.naturalearthdata.com/forums/topic/iso-3166-1-countries/" title="ISO 3166-1 countries">ISO 3166-1 countries</a>
 
-						
-						
+
+
 					</li>
 
-				
+
 					<li>
 						<a class="bbp-forum-title" href="http://www.naturalearthdata.com/forums/topic/striped-hook-up-inside/" title="striped hook up inside">striped hook up inside</a>
 
-						
-						
+
+
 					</li>
 
-				
+
 					<li>
 						<a class="bbp-forum-title" href="http://www.naturalearthdata.com/forums/topic/multipart-ocean-features/" title="Multipart Ocean Features?">Multipart Ocean Features?</a>
 
-						
-						
+
+
 					</li>
 
-				
+
 					<li>
 						<a class="bbp-forum-title" href="http://www.naturalearthdata.com/forums/topic/what-code-page-was-used-for-the-vector-data/" title="What Code Page was used for the vector data?">What Code Page was used for the vector data?</a>
 
-						
-						
+
+
 					</li>
 
-				
+
 			</ul>
 
 			</li></ul><ul><li id='bbpresswptweaks_login_links_widget-3' class='widget bbpresswptweaks_login_links_widget'><h2 class="widgettitle">Forum Login</h2>
 <div class="bbp-template-notice">
 					<a href="http://www.naturalearthdata.com/wp-login.php?redirect_to=/downloads/50m-cultural-vectors/50m-admin-1-states-provinces/" rel="nofollow">Log in</a>
-					- or - 
+					- or -
 					<a href="http://www.naturalearthdata.com/wp-login.php?action=register" rel="nofollow">Register</a>
 				</div></li></ul>	</div>
 
 </div>
 
 <hr />
 <div id="footer">
@@ -331,17 +331,17 @@
         </div>
         <div class="footer-ad-box">
         	<a href="http://www.shadedrelief.com" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/shaded_relief.png" alt="Shaded Relief" /></a>
         </div>
         <div class="footer-ad-box">
         	<a href="http://www.xnrproductions.com " target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/xnr.png" alt="XNR Productions" /></a>
         </div>
-        
+
         <p style="clear:both;"></p>
-        
+
        <div class="footer-ad-box">
         	<a href="http://www.freac.fsu.edu" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/fsu.png" alt="Florida State University - FREAC" /></a>
         </div>
         <div class="footer-ad-box">
         	<a href="http://www.springercartographics.com" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/scllc.png" alt="Springer Cartographics LLC" /></a>
         </div>
         <div class="footer-ad-box">
@@ -349,32 +349,32 @@
         </div>
         <div class="footer-ad-box">
         	<a href="http://www.redgeographics.com" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/redgeo.png" alt="Red Geographics" /></a>
         </div>
         <div class="footer-ad-box">
         	<a href="http://kelsocartography.com/blog " target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/kelso.png" alt="Kelso Cartography" /></a>
         </div>
-        
+
         <p style="clear:both;"></p>
         <div class="footer-ad-box">
         	<a href="http://www.avenza.com" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/avenza.png" alt="Avenza Systems Inc." /></a>
         </div>
         <div class="footer-ad-box">
         	<a href="http://www.stamen.com" target="_blank"><img src="http://www.naturalearthdata.com/wp-content/themes/NEV/images/stamen_ne_logo.png" alt="Stamen Design" /></a>
         </div>
 
 
     </div>
     <p style="clear:both;"></p>
 	<span id="footerleft">
 		&copy; 2013. Natural Earth. All rights reserved.
 	</span>
-    <span id="footerright"> 
-    	<!-- Please help promote WordPress and simpleX. Do not remove -->   
+    <span id="footerright">
+    	<!-- Please help promote WordPress and simpleX. Do not remove -->
 		<div>Powered by <a href="http://wordpress.org/">WordPress</a></div>
         <div><a href="http://www.naturalearthdata.com/wp-admin">Staff Login &raquo;</a></div>
     </span>
 </div>
 </div>
-		
+
 </body>
-</html>
+</html>
```

### Comparing `uwsift-1.2.3/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.dbf` & `uwsift-2.0.0b0/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.dbf`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.shp` & `uwsift-2.0.0b0/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.shp`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.shx` & `uwsift-2.0.0b0/uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.shx`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/data/shadedrelief.jpg` & `uwsift-2.0.0b0/uwsift/data/shadedrelief.jpg`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/model/__init__.py` & `uwsift-2.0.0b0/uwsift/model/__init__.py`

 * *Files identical despite different names*

### Comparing `uwsift-1.2.3/uwsift/project/organize_data_bands.py` & `uwsift-2.0.0b0/uwsift/project/organize_data_bands.py`

 * *Files 20% similar despite different names*

```diff
@@ -20,41 +20,47 @@
 REQUIRES
 
 
 :author: David Hoese <david.hoese@ssec.wisc.edu>
 :copyright: 2014 by University of Wisconsin Regents, see AUTHORS for more details
 :license: GPLv3, see LICENSE for more details
 """
-__docformat__ = 'reStructuredText'
-__author__ = 'davidh'
+__docformat__ = "reStructuredText"
+__author__ = "davidh"
 
 import logging
 import os
 import re
 import sys
 from glob import glob
 
 LOG = logging.getLogger(__name__)
 
-FILENAME_RE = r'HS_H08_(?P<date>\d{8})_(?P<time>\d{4})_(?P<band>B\d{2})_FLDK_(?P<res>R\d+)\.(?P<ext>.+)'
+FILENAME_RE = r"HS_H08_(?P<date>\d{8})_(?P<time>\d{4})_(?P<band>B\d{2})_FLDK_(?P<res>R\d+)\.(?P<ext>.+)"
 fn_re = re.compile(FILENAME_RE)
 
 
 def main():
     import argparse
+
     parser = argparse.ArgumentParser(description="Regenerate or generate mirrored AHI data structure")
-    parser.add_argument("base_ahi_dir", default="/odyssey/isis/tmp/davidh/sift_data/ahi",
-                        help="Base AHI directory for the geotiff data files "
-                             "(next child directory is the full dated directory)")
-    parser.add_argument('-v', '--verbose', dest='verbosity', action="count",
-                        default=int(os.environ.get("VERBOSITY", 2)),
-                        help='each occurrence increases verbosity 1 level through '
-                             'ERROR-WARNING-Info-DEBUG (default Info)')
-    parser.add_argument("--overwrite", action="store_true",
-                        help="Overwrite existing hardlinks")
+    parser.add_argument(
+        "base_ahi_dir",
+        default="/odyssey/isis/tmp/davidh/sift_data/ahi",
+        help="Base AHI directory for the geotiff data files " "(next child directory is the full dated directory)",
+    )
+    parser.add_argument(
+        "-v",
+        "--verbose",
+        dest="verbosity",
+        action="count",
+        default=int(os.environ.get("VERBOSITY", 2)),
+        help="each occurrence increases verbosity 1 level through " "ERROR-WARNING-Info-DEBUG (default Info)",
+    )
+    parser.add_argument("--overwrite", action="store_true", help="Overwrite existing hardlinks")
     args = parser.parse_args()
 
     levels = [logging.ERROR, logging.WARN, logging.INFO, logging.DEBUG]
     level = levels[min(3, args.verbosity)]
     logging.basicConfig(level=level)
 
     if not os.path.isdir(args.base_ahi_dir):
@@ -71,16 +77,16 @@
         nfo = m.groupdict()
         link_path = os.path.join(nfo["band"], tif_fn)
         if os.path.exists(link_path) and not args.overwrite:
             LOG.debug("Link '%s' already exists, skipping...", link_path)
             continue
         link_dir = os.path.dirname(link_path)
         if not os.path.isdir(link_dir):
-            LOG.info("Creating directory for link: %s", link_dir)
+            LOG.debug("Creating directory for link: %s", link_dir)
             os.makedirs(link_dir)
-        LOG.info("Creating hardlink '%s' -> '%s'", link_path, tif_file)
+        LOG.debug("Creating hardlink '%s' -> '%s'", link_path, tif_file)
         os.link(tif_file, link_path)
     LOG.info("Done mirroring files")
 
 
 if __name__ == "__main__":
     sys.exit(main())
```

### Comparing `uwsift-1.2.3/uwsift/queue.py` & `uwsift-2.0.0b0/uwsift/queue.py`

 * *Files 3% similar despite different names*

```diff
@@ -15,21 +15,21 @@
 REQUIRES
 
 
 :author: R.K.Garcia <rayg@ssec.wisc.edu>
 :copyright: 2014 by University of Wisconsin Regents, see AUTHORS for more details
 :license: GPLv3, see LICENSE for more details
 """
-__author__ = 'rayg'
-__docformat__ = 'reStructuredText'
+__author__ = "rayg"
+__docformat__ = "reStructuredText"
 
 import logging
 from collections import OrderedDict
 
-from PyQt5.QtCore import QObject, pyqtSignal, QThread
+from PyQt5.QtCore import QObject, QThread, pyqtSignal
 
 LOG = logging.getLogger(__name__)
 
 # keys for status dictionaries
 TASK_DOING = ("activity", str)
 TASK_PROGRESS = ("progress", float)  # 0.0 - 1.0 progress
 
@@ -37,25 +37,23 @@
 TheQueue = None
 
 
 class Worker(QThread):
     """
     Worker thread use by TaskQueue
     """
-    queue = None
-    depth = 0
 
     # worker id, sequence of dictionaries listing update information to be propagated to view
     workerDidMakeProgress = pyqtSignal(int, list)
     # task-key, ok: False if exception occurred else True
     workerDidCompleteTask = pyqtSignal(str, bool)
 
     def __init__(self, myid: int):
         super(Worker, self).__init__()
-        self.queue = OrderedDict()
+        self.queue: OrderedDict = OrderedDict()
         self.depth = 0
         self.id = myid
 
     def add(self, key, task_iterable):
         # FUTURE: replace queued task if key matches
         self.queue[key] = task_iterable
         self.depth = len(self.queue)
@@ -91,54 +89,57 @@
 class TaskQueue(QObject):
     """
     Global background task queue for loading, rendering, et cetera.
     Includes state updates and GUI links.
     Eventually will include thread pools and multiprocess pools.
     Two threads for interactive tasks (high priority), one thread for background tasks (low priority): 0, 1, 2
     """
-    process_pool = None  # process pool for background activity
-    workers = None  # thread pool for background activity
-    _interactive_round_robin = 0  # swaps between 0 and 1 for interactive tasks
-    _last_status = None  # list of last status reports for different workers
-    _completion_futures = None  # dictionary of id(task) : completion(bool)
 
     didMakeProgress = pyqtSignal(list)  # sequence of dictionaries listing update information to be propagated to view
 
     # started : inherited
     # finished : inherited
     # terminated : inherited
 
-    def __init__(self, process_pool=None, worker_count=3):
+    def __init__(self, process_pool=None):
         super(TaskQueue, self).__init__()
-        self._interactive_round_robin = 0
-        self.process_pool = process_pool
-        self._completion_futures = {}
-        self._last_status = []
-        self.workers = []
+        self._interactive_round_robin = 0  # swaps between 0 and 1 for interactive tasks
+        self.process_pool = process_pool  # thread pool for background activity
+        self._completion_futures = {}  # dictionary of id(task) : completion(bool)
+        self._last_status = []  # list of last status reports for different workers
+        self.workers = []  # thread pool for background activity
         for id in range(3):
             worker = Worker(id)
             worker.workerDidMakeProgress.connect(self._did_progress)
             worker.workerDidCompleteTask.connect(self._did_complete_task)
             self.workers.append(worker)
             self._last_status.append(None)
 
         global TheQueue
-        assert (TheQueue is None)
+        assert TheQueue is None  # nosec B101
         TheQueue = self
 
     @property
     def depth(self):
         return sum([x.depth for x in self.workers])
 
     @property
     def remaining(self):
         return sum([len(x.queue) for x in self.workers])
 
-    def add(self, key, task_iterable, description, interactive=False, and_then=None, use_process_pool=False,
-            use_thread_pool=False):
+    def add(
+        self,
+        key,
+        task_iterable,
+        description,
+        interactive=False,
+        and_then=None,
+        use_process_pool=False,
+        use_thread_pool=False,
+    ):
         """Add an iterable task which will yield progress information dictionaries.
 
         Expect behavior like this::
 
          for task in queue:
             for status_info in task:
                 update_display(status_info)
@@ -149,15 +150,15 @@
                 and the new one deferred to the end
             task_iterable (iter): callable resulting in an iterable, or an iterable itself to be run on the background
 
         """
         if interactive:
             wdex = self._interactive_round_robin
             self._interactive_round_robin += 1
-            self._interactive_round_robin %= 2
+            self._interactive_round_robin %= 2  # TODO(nk) worker count is hardcoded: worker_count-1
         else:
             wdex = 2
         if callable(and_then):
             self._completion_futures[key] = and_then
         self.workers[wdex].add(key, task_iterable)
 
     def _did_progress(self, worker_id, worker_status):
@@ -175,15 +176,15 @@
         # FUTURE make this a more useful signal content, rather than relying on progress_ratio back-query
         for wdex, status in enumerate(self._last_status):
             if self.workers[wdex].isRunning() and status is not None:
                 self.didMakeProgress.emit(status)
                 return
 
         # otherwise this is a notification that we're finally at full idle
-        self.didMakeProgress.emit([{TASK_DOING: '', TASK_PROGRESS: 0.0}])
+        self.didMakeProgress.emit([{TASK_DOING: "", TASK_PROGRESS: 0.0}])
         # FUTURE: consider one progress bar per worker
 
     def _did_complete_task(self, task_key: str, succeeded: bool):
         # LOG.debug("background task complete!")
         todo = self._completion_futures.pop(task_key, None)
         if callable(todo):
             LOG.debug("completed task {}, and_then we do this...".format(succeeded))
@@ -197,14 +198,7 @@
             return 0.0
         elif depth == 1 and current_progress is not None:
             # show something other than 50% if there is only 1 job
             return current_progress
         else:
             depth, remaining = self.depth, self.remaining
             return float(depth - remaining) / depth
-
-
-def test_task():
-    for dex in range(10):
-        yield {TASK_DOING: 'test task', TASK_PROGRESS: float(dex) / 10.0}
-        TheQueue.sleep(1)
-    yield {TASK_DOING: 'test task', TASK_PROGRESS: 1.0}
```

### Comparing `uwsift-1.2.3/uwsift/satpy_compat.py` & `uwsift-2.0.0b0/uwsift/satpy_compat.py`

 * *Files 22% similar despite different names*

```diff
@@ -8,19 +8,24 @@
     def get_id_value(id_obj, key, default=None):
         return id_obj.get(key, default)
 
     def get_id_items(id_obj):
         return id_obj.items()
 
     def id_from_attrs(attrs):
-        return attrs['_satpy_id']
+        return attrs["_satpy_id"]
+
 except ImportError:
     import warnings
-    warnings.warn("Satpy <0.23.0 will not be supported in future versions. "
-                  "Please update your version of Satpy.", DeprecationWarning)
+
+    warnings.warn(
+        "Satpy <0.23.0 will not be supported in future versions. " "Please update your version of Satpy.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
     from satpy import DatasetID as DataID
 
     def get_id_value(id_obj, key, default=None):
         return getattr(id_obj, key, default)
 
     def get_id_items(id_obj):
         return id_obj._asdict().items()
```

### Comparing `uwsift-1.2.3/uwsift/tests/conftest.py` & `uwsift-2.0.0b0/uwsift/tests/conftest.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 import pytest
+from PyQt5.QtTest import QTest
+
 from uwsift.__main__ import Main, create_app
 from uwsift.util.default_paths import USER_CONFIG_DIR
-from PyQt5.QtTest import QTest
 
 
 @pytest.fixture(scope="session")
 def window(tmp_path_factory):
     """Provides the SIFT GUI to tests."""
     vispy_app, qt_app = create_app()  # noqa
     d = tmp_path_factory.mktemp("tmp")
```

### Comparing `uwsift-1.2.3/uwsift/tests/view/test_export_image.py` & `uwsift-2.0.0b0/uwsift/tests/view/test_export_image.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,94 +1,93 @@
-from uwsift.view import export_image
-from numpy.testing import assert_array_equal
-from PIL import Image
-from matplotlib import pyplot as plt
-from collections import namedtuple
-from PyQt5.QtCore import Qt
-import pytest
 import datetime
 import os
+from collections import namedtuple
+
 import imageio
+import pytest
+from matplotlib import pyplot as plt
+from numpy.testing import assert_array_equal
+from PIL import Image
+from PyQt5.QtCore import Qt
+
+from uwsift.view import export_image
 
 
-def _get_mock_doc():
-    """Mock Document class for testing."""
+def _get_mock_model():
+    """Mock LayerModel class for testing."""
 
-    class MockPrez():
+    class MockPrez:
         def __init__(self):
-            self.colormap = 'Rainbow (IR Default)'
+            self.colormap = "Rainbow (IR Default)"
             self.climits = (0, 1)
 
-    class MockDocBasicLayer():
+    class MockDataset:
         def __init__(self):
-            self.attrs = {}
-            self.attrs['unit_conversion'] = ('unit', lambda t: t, lambda t: t)
-            self.attrs['timeline'] = datetime.datetime(2000, 1, 1, 0, 0, 0, 0)
-            self.attrs['display_name'] = 'name'
+            self.info = {}
+            self.info["unit_conversion"] = ("unit", lambda t: t, lambda t: t)
+            self.info["timeline"] = datetime.datetime(2000, 1, 1, 0, 0, 0, 0)
+            self.info["display_name"] = "name"
 
-        def __getitem__(self, item):
-            return self.attrs[item]
-
-    class MockDoc():
+    class MockModel:
         def __init__(self):
-            self.layer = MockDocBasicLayer()
-            self.prez = MockPrez()
-
-        def __getitem__(self, item):
-            return self.layer
+            self.moc_prez = MockPrez()
+            self.moc_dataset = MockDataset()
 
-        def colormap_for_uuid(self, u):
-            return 'Rainbow (IR Default)'
+        def get_dataset_presentation_by_uuid(self, u):
+            return self.moc_prez
 
-        def prez_for_uuid(self, u):
-            return MockPrez()
+        def get_dataset_by_uuid(self, u):
+            return self.moc_dataset
 
-    return MockDoc()
+    return MockModel()
 
 
 def _get_mock_sd(fr, fn):
     """Mock ScreenshotDialog class for testing."""
 
     class MockScreenshotDialog:
         def __init__(self, frame_range, filename):
             self.info = {
-                'frame_range': frame_range,
-                'include_footer': True,
-                'filename': filename,
-                'colorbar': True,
-                'font_size': 10,
-                'loop': True,
+                "frame_range": frame_range,
+                "include_footer": True,
+                "filename": filename,
+                "colorbar": True,
+                "font_size": 10,
+                "loop": True,
             }
 
         def get_info(self):
             return self.info
 
     return MockScreenshotDialog(fr, fn)
 
 
 def _get_mock_sgm(frame_order):
     """Mock SceneGraphManager class for testing."""
 
-    class MockLayerSet:
-        def __init__(self, fo):
-            self.frame_order = fo
+    class MockAnimationController:
+        def __init__(self):
+            self._frame_order = frame_order
 
-        def top_layer_uuid(self):
-            return 1
+        def get_current_frame_index(self):
+            return max(self._frame_order, 1) if self._frame_order else 0
+
+        def get_frame_uuids(self):
+            return self._frame_order if self._frame_order else []
 
     class MockSGM:
-        def __init__(self, fo):
-            self.layer_set = MockLayerSet(fo)
+        def __init__(self):
+            self.animation_controller = MockAnimationController()
 
         def get_screenshot_array(self, fr):
             if fr is None:
                 return None
             return [[fr[1], fr[1] - fr[0]]]
 
-    return MockSGM(frame_order)
+    return MockSGM()
 
 
 def _get_mock_writer():
     """Mock Writer class for testing."""
 
     class MockWriter:
         def __init__(self):
@@ -99,143 +98,151 @@
 
         def close(self):
             pass
 
     return MockWriter()
 
 
-@pytest.mark.parametrize("size,mode,exp", [
-    ((100, 100), 'vertical', [0.1, 1.2]),
-    ((100, 100), 'horizontal', [1.2, 0.1]),
-])
+@pytest.mark.parametrize(
+    "size,mode,exp",
+    [
+        ((100, 100), "vertical", [0.1, 1.2]),
+        ((100, 100), "horizontal", [1.2, 0.1]),
+    ],
+)
 def test_create_colorbar(size, mode, exp, monkeypatch, window):
     """Test colorbar is created correctly given dimensions and the colorbar append direction."""
-    monkeypatch.setattr(window.export_image, 'doc', _get_mock_doc())
-    monkeypatch.setattr(window.export_image.sgm.main_canvas, 'dpi', 100)
+    monkeypatch.setattr(window.export_image, "model", _get_mock_model())
+    monkeypatch.setattr(window.export_image.sgm.main_canvas, "dpi", 100)
 
     res = window.export_image._create_colorbar(mode, None, size)
 
     assert_array_equal(res.get_size_inches(), exp)
     assert res.dpi == 100
 
 
-@pytest.mark.parametrize("mode,cbar_size,exp", [
-    (None, (0, 0), (100, 100)),
-    ('vertical', (10, 120), (108, 100)),
-    ('horizontal', (110, 10), (100, 109)),
-])
+@pytest.mark.parametrize(
+    "mode,cbar_size,exp",
+    [
+        (None, (0, 0), (100, 100)),
+        ("vertical", (10, 120), (108, 100)),
+        ("horizontal", (110, 10), (100, 109)),
+    ],
+)
 def test_append_colorbar(mode, cbar_size, exp, monkeypatch, window):
     """Test colorbar is appended to the appropriate location given the colorbar append direction."""
-    monkeypatch.setattr(window.export_image, 'doc', _get_mock_doc())
-    monkeypatch.setattr(window.export_image.sgm.main_canvas, 'dpi', 100)
-    monkeypatch.setattr(window.export_image, '_create_colorbar', lambda x, y, z: plt.figure(figsize=cbar_size))
+    monkeypatch.setattr(window.export_image, "model", _get_mock_model())
+    monkeypatch.setattr(window.export_image.sgm.main_canvas, "dpi", 100)
+    monkeypatch.setattr(window.export_image, "_create_colorbar", lambda x, y, z: plt.figure(figsize=cbar_size))
 
-    im = Image.new('RGBA', (100, 100))
+    im = Image.new("RGBA", (100, 100))
     res = window.export_image._append_colorbar(mode, im, None)
 
     assert res.size == exp
 
 
-@pytest.mark.parametrize("size,fs,exp", [
-    ((100, 100), 10, (100, 110))
-])
+@pytest.mark.parametrize("size,fs,exp", [((100, 100), 10, (100, 110))])
 def test_add_screenshot_footer(size, fs, exp, window):
     """Test screenshot footer is appended correctly."""
-    im = Image.new('RGBA', size)
-    res = window.export_image._add_screenshot_footer(im, 'text', font_size=fs)
+    im = Image.new("RGBA", size)
+    res = window.export_image._add_screenshot_footer(im, "text", font_size=fs)
     assert res.size == exp
 
 
-@pytest.mark.parametrize("range,exp", [
-    (None, None),
-    ((None, 2), (0, 1)),
-    ((1, None), (0, 0)),
-    ((1, 5), (0, 4)),
-])
+@pytest.mark.parametrize(
+    "range,exp",
+    [
+        (None, None),
+        ((None, 2), (0, 1)),
+        ((1, None), (0, 0)),
+        ((1, 5), (0, 4)),
+    ],
+)
 def test_convert_frame_range(range, exp, window):
     """Test frame range is converted correctly."""
     res = window.export_image._convert_frame_range(range)
     assert res == exp
 
 
-@pytest.mark.parametrize("info,isgif,exp", [
-    ({'fps': None, 'filename': None, 'loop': 0}, True, {'duration': [0.1, 0.1], 'loop': 0}),
-    ({'fps': None, 'filename': None, 'loop': 0}, False, {'fps': 10}),
-    ({'fps': 1, 'filename': None}, True, {'fps': 1, 'loop': 0}),
-    ({'fps': 1, 'filename': None}, False, {'fps': 1})
-])
+@pytest.mark.parametrize(
+    "info,isgif,exp",
+    [
+        ({"fps": None, "filename": None, "loop": 0}, True, {"duration": [0.1, 0.1], "loop": 0}),
+        ({"fps": None, "filename": None, "loop": 0}, False, {"fps": 10}),
+        ({"fps": 1, "filename": None}, True, {"fps": 1, "loop": 0}),
+        ({"fps": 1, "filename": None}, False, {"fps": 1}),
+    ],
+)
 def test_get_animation_parameters(info, isgif, exp, monkeypatch, window):
     """Test animation parameters are calculated correctly."""
-    monkeypatch.setattr(window.export_image, 'doc', _get_mock_doc())
-    monkeypatch.setattr(export_image, 'is_gif_filename', lambda x: isgif)
+    monkeypatch.setattr(window.export_image, "model", _get_mock_model())
+    monkeypatch.setattr(export_image, "is_gif_filename", lambda x: isgif)
 
-    im = Image.new('RGBA', (100, 100))
+    im = Image.new("RGBA", (100, 100))
 
     res = window.export_image._get_animation_parameters(info, [(0, im), (0, im)])
     assert res == exp
 
 
-@pytest.mark.parametrize('fn,exp', [
-    ('test.gif', True),
-    ('test.png', False)
-])
+@pytest.mark.parametrize("fn,exp", [("test.gif", True), ("test.png", False)])
 def test_is_gif_filename(fn, exp):
     """Test that gif file names are recognized."""
     res = export_image.is_gif_filename(fn)
     assert res == exp
 
 
-@pytest.mark.parametrize('fn,exp', [
-    ('test.gif', True),
-    ('test.m4v', True),
-    ('test.mp4', True),
-    ('test.png', False)
-])
+@pytest.mark.parametrize("fn,exp", [("test.gif", True), ("test.m4v", True), ("test.mp4", True), ("test.png", False)])
 def test_is_video_filename(fn, exp):
     """Test that video file names are recognized."""
     res = export_image.is_video_filename(fn)
     assert res == exp
 
 
-@pytest.mark.parametrize('uuids,base,exp', [
-    ([0, 0], 'test.gif', ([0, 0], ["test.gif"])),
-    ([0, 0], 'test.png', ([0, 0], ["test_001.png", "test_002.png"])),
-    (None, 'test.gif', ([None], ['test.gif']))
-])
+@pytest.mark.parametrize(
+    "uuids,base,exp",
+    [
+        ([0, 0], "test.gif", ([0, 0], ["test.gif"])),
+        ([0, 0], "test.png", ([0, 0], ["test_001.png", "test_002.png"])),
+        (None, "test.gif", ([None], ["test.gif"])),
+    ],
+)
 def test_create_filenames(uuids, base, exp, monkeypatch, window):
     """Test file names are created correctly."""
-    monkeypatch.setattr(window.export_image, 'doc', _get_mock_doc())
+    monkeypatch.setattr(window.export_image, "model", _get_mock_model())
     res = window.export_image._create_filenames(uuids, base)
     assert res == exp
 
 
-@pytest.mark.parametrize('fr,fn,overwrite,exp', [
-    ([1, 2], 'test.gif', True, 1),
-    ([1, 2], 'test.m4v', True, 1),
-    (None, 'test.m4v', True, 0),
-    ([1, 2], 'test.gif', False, 0)
-])
+@pytest.mark.parametrize(
+    "fr,fn,overwrite,exp",
+    [
+        ([1, 2], "test.gif", True, 1),
+        ([1, 2], "test.m4v", True, 1),
+        (None, "test.m4v", True, 0),
+        ([1, 2], "test.gif", False, 0),
+    ],
+)
 def test_save_screenshot(fr, fn, overwrite, exp, monkeypatch, window):
     """Test screenshot is saved correctly given the frame range and filename."""
     writer = _get_mock_writer()
-    IFormat = namedtuple('IFormat', 'name')
+    IFormat = namedtuple("IFormat", "name")
 
-    monkeypatch.setattr(window.export_image, '_screenshot_dialog', _get_mock_sd(fr, fn))
-    monkeypatch.setattr(window.export_image, '_convert_frame_range', lambda x: fr)
-    monkeypatch.setattr(window.export_image, 'sgm', _get_mock_sgm(fr))
-    monkeypatch.setattr(window.export_image, '_create_filenames', lambda x, y: ([1], [fn]))
-    monkeypatch.setattr(window.export_image, 'doc', _get_mock_doc())
-    monkeypatch.setattr(window.export_image, '_overwrite_dialog', lambda: overwrite)
-    monkeypatch.setattr(window.export_image, '_append_colorbar', lambda x, y, z: y)
-    monkeypatch.setattr(window.export_image, '_add_screenshot_footer', lambda x, y, font_size=10: x)
-    monkeypatch.setattr(window.export_image, '_get_animation_parameters', lambda x, y: {'loop': True})
-    monkeypatch.setattr(export_image, 'get_imageio_format', lambda x: IFormat(name='test'))
-    monkeypatch.setattr(os.path, 'isfile', lambda x: True)
-    monkeypatch.setattr(Image, 'fromarray', lambda x: x)
-    monkeypatch.setattr(imageio, 'get_writer', lambda x, y: writer)
+    monkeypatch.setattr(window.export_image, "_screenshot_dialog", _get_mock_sd(fr, fn))
+    monkeypatch.setattr(window.export_image, "_convert_frame_range", lambda x: fr)
+    monkeypatch.setattr(window.export_image, "sgm", _get_mock_sgm(fr))
+    monkeypatch.setattr(window.export_image, "_create_filenames", lambda x, y: ([1], [fn]))
+    monkeypatch.setattr(window.export_image, "model", _get_mock_model())
+    monkeypatch.setattr(window.export_image, "_overwrite_dialog", lambda: overwrite)
+    monkeypatch.setattr(window.export_image, "_append_colorbar", lambda x, y, z: y)
+    monkeypatch.setattr(window.export_image, "_add_screenshot_footer", lambda x, y, font_size=10: x)
+    monkeypatch.setattr(window.export_image, "_get_animation_parameters", lambda x, y: {"loop": True})
+    monkeypatch.setattr(export_image, "get_imageio_format", lambda x: IFormat(name="test"))
+    monkeypatch.setattr(os.path, "isfile", lambda x: True)
+    monkeypatch.setattr(Image, "fromarray", lambda x: x)
+    monkeypatch.setattr(imageio, "get_writer", lambda x, y: writer)
 
     window.export_image._save_screenshot()
 
     assert len(writer.data) == exp
 
 
 def test_cmd_open_export_image_dialog(qtbot, window):
@@ -253,51 +260,51 @@
     """Assert changing no options results in the default screenshot settings."""
     window.export_image.take_screenshot()
     qtbot.waitUntil(lambda: window.export_image._screenshot_dialog is not None)
 
     res = window.export_image._screenshot_dialog.get_info()
 
     # only look at the filename
-    res['filename'] = os.path.split(res['filename'])[-1]
+    res["filename"] = os.path.split(res["filename"])[-1]
 
     exp = {
-        'frame_range': None,
-        'include_footer': True,
-        'loop': True,
-        'filename': export_image.ExportImageDialog.default_filename,
-        'fps': None,
-        'font_size': 11,
-        'colorbar': None
+        "frame_range": None,
+        "include_footer": True,
+        "loop": True,
+        "filename": export_image.ExportImageDialog.default_filename,
+        "fps": None,
+        "font_size": 11,
+        "colorbar": None,
     }
 
     assert res == exp
 
 
 def test_export_image_dialog_info(qtbot, window):
     """Test changing the options in the export image GUI."""
     window.export_image.take_screenshot()
     qtbot.waitUntil(lambda: window.export_image._screenshot_dialog is not None)
 
     qtbot.keyClick(window.export_image._screenshot_dialog.ui.saveAsLineEdit, Qt.Key_A, Qt.ControlModifier)
     qtbot.keyClick(window.export_image._screenshot_dialog.ui.saveAsLineEdit, Qt.Key_Backspace)
-    qtbot.keyClicks(window.export_image._screenshot_dialog.ui.saveAsLineEdit, 'test.png')
+    qtbot.keyClicks(window.export_image._screenshot_dialog.ui.saveAsLineEdit, "test.png")
     qtbot.keyClick(window.export_image._screenshot_dialog.ui.footerFontSizeSpinBox, Qt.Key_A, Qt.ControlModifier)
     qtbot.keyClick(window.export_image._screenshot_dialog.ui.footerFontSizeSpinBox, Qt.Key_Backspace)
-    qtbot.keyClicks(window.export_image._screenshot_dialog.ui.footerFontSizeSpinBox, '20')
+    qtbot.keyClicks(window.export_image._screenshot_dialog.ui.footerFontSizeSpinBox, "20")
     qtbot.mouseClick(window.export_image._screenshot_dialog.ui.frameRangeRadio, Qt.LeftButton)
     qtbot.mouseClick(window.export_image._screenshot_dialog.ui.colorbarVerticalRadio, Qt.LeftButton)
     qtbot.mouseClick(window.export_image._screenshot_dialog.ui.includeFooterCheckbox, Qt.LeftButton)
 
     res = window.export_image._screenshot_dialog.get_info()
-    res['filename'] = os.path.split(res['filename'])[-1]
+    res["filename"] = os.path.split(res["filename"])[-1]
 
     exp = {
-        'frame_range': [1, 1],
-        'include_footer': False,
-        'loop': True,
-        'filename': 'test.png',
-        'fps': None,
-        'font_size': 20,
-        'colorbar': 'vertical'
+        "frame_range": [1, 1],
+        "include_footer": False,
+        "loop": True,
+        "filename": "test.png",
+        "fps": None,
+        "font_size": 20,
+        "colorbar": "vertical",
     }
 
     assert res == exp
```

### Comparing `uwsift-1.2.3/uwsift/tests/view/test_rgb_config.py` & `uwsift-2.0.0b0/uwsift/tests/view/test_rgb_config.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,51 +1,79 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
 """Test RGB configuration changes through the UI."""
+from unittest import mock
+
+from PyQt5.QtWidgets import QMainWindow
+
 from uwsift.common import Info, Kind
 from uwsift.model.composite_recipes import CompositeRecipe
-from uwsift.view.rgb_config import RGBLayerConfigPane
+from uwsift.model.layer_model import LayerModel
 from uwsift.ui.pov_main_ui import Ui_MainWindow
-
-from PyQt5.QtWidgets import QMainWindow
+from uwsift.view.rgb_config import RGBLayerConfigPane
 
 
 class _PaneWrapper(QMainWindow):
-    def __init__(self):
+    def __init__(self, model):
         super().__init__()
         self.ui = Ui_MainWindow()
         self.ui.setupUi(self)
-        self.pane = RGBLayerConfigPane(self.ui, self.ui.layersPaneWidget)
+        self.pane = RGBLayerConfigPane(self.ui, None, model)
 
 
 def test_slider_change(qtbot):
-    widget = _PaneWrapper()
+    doc = mock.MagicMock()
+    workspace = mock.MagicMock()
+    doc._workspace = workspace
+
+    layer_model = LayerModel(doc)
+    widget = _PaneWrapper(layer_model)
     pane = widget.pane
 
-    pane.family_added("family1", {
+    layer_one = mock.MagicMock()
+    layer_two = mock.MagicMock()
+
+    layer_one.uuid = "some_uuid"
+    layer_one.kind = Kind.IMAGE
+    layer_one.descriptor = "layer_one"
+    layer_one.info = {
         Info.VALID_RANGE: (0.0, 150.0),
         Info.KIND: Kind.IMAGE,
         Info.DISPLAY_FAMILY: "family1",
         Info.UNIT_CONVERSION: (lambda x, inverse=False: x, lambda x, inverse=False: x),
-    })
-    pane.family_added("family2", {
+    }
+    layer_one.valid_range = (0.0, 150.0)
+
+    layer_two.uuid = "some_other_uuid"
+    layer_two.kind = Kind.IMAGE
+    layer_two.descriptor = "layer_two"
+    layer_two.info = {
         Info.VALID_RANGE: (0.0, 150.0),
         Info.KIND: Kind.IMAGE,
         Info.DISPLAY_FAMILY: "family2",
         Info.UNIT_CONVERSION: (lambda x, inverse=False: x, lambda x, inverse=False: x),
-    })
-    rgb_recipe = CompositeRecipe("my_rgb",
-                                 input_ids=["family1", "family2", None],
-                                 color_limits=((0.0, 90.0), (0.0, 90.0), (None, None)))
-    pane.selection_did_change(rgb_recipe)
+    }
+    layer_two.valid_range = (0.0, 150.0)
+
+    layer_model.layers.append(layer_one)
+    layer_model.layers.append(layer_two)
+
+    pane.layer_added(layer_one)
+    pane.layer_added(layer_two)
+
+    rgb_recipe = CompositeRecipe.from_rgb(
+        name="my_rgb", r="some_uuid", g="some_other_uuid", b=None, color_limits=((0.0, 90.0), (0.0, 90.0), (None, None))
+    )
+    rgb_layer = mock.MagicMock()
+    rgb_layer.recipe = rgb_recipe
+
+    pane.selection_did_change([rgb_layer])
 
     # Set the red min value manually
-    with qtbot.waitSignal(pane.didChangeRGBComponentLimits) as blocker:
+    with qtbot.waitSignal(pane.didChangeRGBColorLimits) as blocker:
         pane.sliders[0][0].setValue(8)
     recipe = blocker.args[0]
-    clims_r = blocker.args[1][0]
-    clims_g = blocker.args[1][1]
-    clims_b = blocker.args[1][2]
+    color = blocker.args[1]
+    clim = blocker.args[2]
     assert recipe is rgb_recipe
-    assert clims_r == ((8.0 / 100.0) * 150, 90.0)
-    assert clims_g == (0.0, 90.0)
-    assert clims_b == (None, None)
+    assert color == "r"
+    assert clim == ((8.0 / 100.0) * 150, 90.0)
```

### Comparing `uwsift-1.2.3/uwsift/tests/view/test_tile_calculator.py` & `uwsift-2.0.0b0/uwsift/tests/view/test_tile_calculator.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,214 +1,298 @@
-import pytest
 import numpy as np
-from uwsift.view.tile_calculator import (TileCalculator,
-                                         calc_pixel_size,
-                                         get_reference_points,
-                                         _calc_extent_component,
-                                         clip,
-                                         calc_view_extents,
-                                         max_tiles_available,
-                                         calc_tile_slice,
-                                         visible_tiles,
-                                         calc_tile_fraction,
-                                         calc_stride,
-                                         calc_overview_stride,
-                                         calc_vertex_coordinates,
-                                         calc_texture_coordinates)
-from uwsift.common import Point, Box, ViewBox, Resolution
+import pytest
+
+from uwsift.common import Box, Point, Resolution, ViewBox
+from uwsift.view.tile_calculator import (
+    TileCalculator,
+    _calc_extent_component,
+    calc_overview_stride,
+    calc_pixel_size,
+    calc_stride,
+    calc_texture_coordinates,
+    calc_tile_fraction,
+    calc_tile_slice,
+    calc_vertex_coordinates,
+    calc_view_extents,
+    clip,
+    get_reference_points,
+    max_tiles_available,
+    visible_tiles,
+)
 
 
 @pytest.fixture(params=[False, True], autouse=True)
 def disable_jit(request, monkeypatch):
     """Runs the tests with jit enabled and disabled."""
     if request.param:
-        monkeypatch.setattr('uwsift.view.tile_calculator.calc_tile_slice', calc_tile_slice.py_func)
-        monkeypatch.setattr('uwsift.view.tile_calculator.get_reference_points', get_reference_points.py_func)
-        monkeypatch.setattr('uwsift.view.tile_calculator.calc_pixel_size', calc_pixel_size.py_func)
-        monkeypatch.setattr('uwsift.view.tile_calculator._calc_extent_component', _calc_extent_component.py_func)
-        monkeypatch.setattr('uwsift.view.tile_calculator.clip', clip.py_func)
-        monkeypatch.setattr('uwsift.view.tile_calculator.calc_view_extents', calc_view_extents.py_func)
-        monkeypatch.setattr('uwsift.view.tile_calculator.max_tiles_available', max_tiles_available.py_func)
-        monkeypatch.setattr('uwsift.view.tile_calculator.visible_tiles', visible_tiles.py_func)
-        monkeypatch.setattr('uwsift.view.tile_calculator.calc_tile_fraction', calc_tile_fraction.py_func)
-        monkeypatch.setattr('uwsift.view.tile_calculator.calc_stride', calc_stride.py_func)
-        monkeypatch.setattr('uwsift.view.tile_calculator.calc_overview_stride', calc_overview_stride.py_func)
-        monkeypatch.setattr('uwsift.view.tile_calculator.calc_vertex_coordinates', calc_vertex_coordinates.py_func)
-        monkeypatch.setattr('uwsift.view.tile_calculator.calc_texture_coordinates', calc_texture_coordinates.py_func)
-
-
-@pytest.mark.parametrize("tc_params,vg,etiles,stride,exp", [
-    (["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
-     ViewBox(200000, -300000, 500000, -6000, 500, 400), (1, 1, 1, 1), (2, 2),
-     Box(bottom=3, left=7, top=-2, right=3))
-])
+        monkeypatch.setattr("uwsift.view.tile_calculator.calc_tile_slice", calc_tile_slice.py_func)
+        monkeypatch.setattr("uwsift.view.tile_calculator.get_reference_points", get_reference_points.py_func)
+        monkeypatch.setattr("uwsift.view.tile_calculator.calc_pixel_size", calc_pixel_size.py_func)
+        monkeypatch.setattr("uwsift.view.tile_calculator._calc_extent_component", _calc_extent_component.py_func)
+        monkeypatch.setattr("uwsift.view.tile_calculator.clip", clip.py_func)
+        monkeypatch.setattr("uwsift.view.tile_calculator.calc_view_extents", calc_view_extents.py_func)
+        monkeypatch.setattr("uwsift.view.tile_calculator.max_tiles_available", max_tiles_available.py_func)
+        monkeypatch.setattr("uwsift.view.tile_calculator.visible_tiles", visible_tiles.py_func)
+        monkeypatch.setattr("uwsift.view.tile_calculator.calc_tile_fraction", calc_tile_fraction.py_func)
+        monkeypatch.setattr("uwsift.view.tile_calculator.calc_stride", calc_stride.py_func)
+        monkeypatch.setattr("uwsift.view.tile_calculator.calc_overview_stride", calc_overview_stride.py_func)
+        monkeypatch.setattr("uwsift.view.tile_calculator.calc_vertex_coordinates", calc_vertex_coordinates.py_func)
+        monkeypatch.setattr("uwsift.view.tile_calculator.calc_texture_coordinates", calc_texture_coordinates.py_func)
+
+
+@pytest.mark.parametrize(
+    "tc_params,vg,etiles,stride,exp",
+    [
+        (
+            ["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
+            ViewBox(200000, -300000, 500000, -6000, 500, 400),
+            (1, 1, 1, 1),
+            (2, 2),
+            Box(bottom=3, left=7, top=-2, right=3),
+        )
+    ],
+)
 def test_visible_tiles(tc_params, vg, etiles, stride, exp):
     """Test returned box of tiles to draw is correct given a visible world geometry and sampling."""
     tile_calc = TileCalculator(*tc_params)
     res = tile_calc.visible_tiles(vg, stride, etiles)
     assert res == exp
 
 
-@pytest.mark.parametrize("cp,ip,cs,exp", [
-    ([[10.0, 10.0], [20.0, 20.0]], [[10.0, 10.0], [20.0, 20.0]], (1, 1), (2.0, 2.0))
-])
+@pytest.mark.parametrize(
+    "cp,ip,cs,exp", [([[10.0, 10.0], [20.0, 20.0]], [[10.0, 10.0], [20.0, 20.0]], (1, 1), (2.0, 2.0))]
+)
 def test_calc_pixel_size(cp, ip, cs, exp):
     """Test calculated pixel size is correct given image data."""
     res = calc_pixel_size(np.array(cp), np.array(ip), cs)
     assert res == exp
 
 
-@pytest.mark.parametrize("ic,iv,exp", [
-    ([[0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 1.0]],
-     [[1.0, 3.0, 7.0, 2.0], [3.0, 2.0, 8.0, 5.0]], (0, 1))
-])
+@pytest.mark.parametrize(
+    "ic,iv,exp", [([[0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 1.0]], [[1.0, 3.0, 7.0, 2.0], [3.0, 2.0, 8.0, 5.0]], (0, 1))]
+)
 def test_get_reference_points(ic, iv, exp):
     """Test returned image reference point indexes are correct."""
     res = get_reference_points(np.array(ic), np.array(iv))
     assert res == exp
 
 
-@pytest.mark.parametrize("ic,iv", [
-    ([[0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 1.0]],
-     [[0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 1.0]])
-])
+@pytest.mark.parametrize(
+    "ic,iv", [([[0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 1.0]], [[0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 1.0]])]
+)
 def test_get_reference_points_bad_points(ic, iv):
     """Test that error is thrown if given invalid mesh points."""
     with pytest.raises(ValueError):
         get_reference_points(np.array(ic), np.array(iv))
 
 
-@pytest.mark.parametrize("cp,ip,num_p,mpp,exp", [
-    (1.0, 500.0, 100, 3, (200, 500))
-])
+@pytest.mark.parametrize("cp,ip,num_p,mpp,exp", [(1.0, 500.0, 100, 3, (200, 500))])
 def test_calc_extent_component(cp, ip, num_p, mpp, exp):
     """Test bounding box extents are correct."""
     res = _calc_extent_component(cp, ip, num_p, mpp)
     assert res == exp
 
 
-@pytest.mark.parametrize("v,n,x,exp", [
-    (1.0, 2.0, 3.0, 2.0),
-    (0.0, 0.0, 0.0, 0.0)
-])
+@pytest.mark.parametrize("v,n,x,exp", [(1.0, 2.0, 3.0, 2.0), (0.0, 0.0, 0.0, 0.0)])
 def test_clip(v, n, x, exp):
     """Test clipped value is correct."""
     res = clip(v, n, x)
     assert res == exp
 
 
-@pytest.mark.parametrize("iebox,cp,ip,cs,dx,dy,exp", [
-    (Box(100.0, 100.0, 200.0, 300.0), [1.0, 1.0], [500.0, 350.0], (100, 70), 3.0, 2.5,
-     Box(bottom=175.0, left=200.0, top=200.0, right=300.0))
-])
+@pytest.mark.parametrize(
+    "iebox,cp,ip,cs,dx,dy,exp",
+    [
+        (
+            Box(100.0, 100.0, 200.0, 300.0),
+            [1.0, 1.0],
+            [500.0, 350.0],
+            (100, 70),
+            3.0,
+            2.5,
+            Box(bottom=175.0, left=200.0, top=200.0, right=300.0),
+        )
+    ],
+)
 def test_calc_view_extents(iebox, cp, ip, cs, dx, dy, exp):
     """Test calculated viewing box for image is correct."""
     res = calc_view_extents(iebox, np.array(cp), np.array(ip), cs, dx, dy)
     assert res == exp
 
 
-@pytest.mark.parametrize("iebox,cp,ip,cs,dx,dy", [
-    (Box(0.0, 0.0, 0.0, 0.0), [1.0, 1.0], [500.0, 500.0], (100, 100), 3.0, 3.0)
-])
+@pytest.mark.parametrize(
+    "iebox,cp,ip,cs,dx,dy", [(Box(0.0, 0.0, 0.0, 0.0), [1.0, 1.0], [500.0, 500.0], (100, 100), 3.0, 3.0)]
+)
 def test_calc_view_extents_bad_box(iebox, cp, ip, cs, dx, dy):
     """Test that error is thrown given zero-sized box."""
     with pytest.raises(ValueError):
         calc_view_extents(iebox, np.array(cp), np.array(ip), cs, dx, dy)
 
 
-@pytest.mark.parametrize("ims,ts,s,exp", [
-    (Point(20, 20), Point(2, 2), Point(2, 2), (5.0, 5.0))
-])
+@pytest.mark.parametrize("ims,ts,s,exp", [(Point(20, 20), Point(2, 2), Point(2, 2), (5.0, 5.0))])
 def test_max_tiles_available(ims, ts, s, exp):
     """Test the max number of tiles available is returned given image shape, tile shape, and stride."""
     res = max_tiles_available(ims[0], ims[1], ts[0], ts[1], s[0], s[1])
     assert res == exp
 
 
-@pytest.mark.parametrize("tc_params,tiy,tix,s,exp", [
-    (["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
-     10, 10, (2, 2), (slice(600, 650, 1), slice(600, 650, 1))),
-    (["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (500, 500)],
-     0, 0, (2, 2), (slice(0, 375, 1), slice(0, 375, 1)))
-])
+@pytest.mark.parametrize(
+    "tc_params,tiy,tix,s,exp",
+    [
+        (
+            ["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
+            10,
+            10,
+            (2, 2),
+            (slice(600, 650, 1), slice(600, 650, 1)),
+        ),
+        (
+            ["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (500, 500)],
+            0,
+            0,
+            (2, 2),
+            (slice(0, 375, 1), slice(0, 375, 1)),
+        ),
+    ],
+)
 def test_calc_tile_slice(tc_params, tiy, tix, s, exp, monkeypatch):
     """Test appropriate slice is returned given image data."""
     tile_calc = TileCalculator(*tc_params)
     res = tile_calc.calc_tile_slice(tiy, tix, s)
     assert res == exp
 
 
-@pytest.mark.parametrize("tc_params,tiy,tix,s,exp", [
-    (["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
-     -5, -5, (2, 2), (Resolution(-2.0, -2.0), Resolution(3.0, 3.0))),
-    (["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
-     1, 1, (2, 2), (Resolution(1.0, 1.0), Resolution(0.0, 0.0))),
-    (["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
-     5, 5, (2, 2), (Resolution(-2.0, -2.0), Resolution(0.0, 0.0))),
-])
+@pytest.mark.parametrize(
+    "tc_params,tiy,tix,s,exp",
+    [
+        (
+            ["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
+            -5,
+            -5,
+            (2, 2),
+            (Resolution(-2.0, -2.0), Resolution(3.0, 3.0)),
+        ),
+        (
+            ["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
+            1,
+            1,
+            (2, 2),
+            (Resolution(1.0, 1.0), Resolution(0.0, 0.0)),
+        ),
+        (
+            ["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
+            5,
+            5,
+            (2, 2),
+            (Resolution(-2.0, -2.0), Resolution(0.0, 0.0)),
+        ),
+    ],
+)
 def test_calc_tile_fraction(tc_params, tiy, tix, s, exp):
     """Test calculated fractional components of the specified tile are correct."""
     tile_calc = TileCalculator(*tc_params)
     res = tile_calc.calc_tile_fraction(tiy, tix, s)
     assert res == exp
 
 
-@pytest.mark.parametrize("tc_params,v,t,exp", [
-    (["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
-     ViewBox(dx=1, dy=1, bottom=1, top=1, right=1, left=1), None, Point(y=1, x=1)),
-    (["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
-     Resolution(dx=1, dy=1), None, Point(y=1, x=1)),
-    (["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
-     ViewBox(dx=1, dy=1, bottom=1, top=1, right=1, left=1), Resolution(100, 100), Point(y=1, x=1)),
-])
+@pytest.mark.parametrize(
+    "tc_params,v,t,exp",
+    [
+        (
+            ["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
+            ViewBox(dx=1, dy=1, bottom=1, top=1, right=1, left=1),
+            None,
+            Point(y=1, x=1),
+        ),
+        (
+            ["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
+            Resolution(dx=1, dy=1),
+            None,
+            Point(y=1, x=1),
+        ),
+        (
+            ["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
+            ViewBox(dx=1, dy=1, bottom=1, top=1, right=1, left=1),
+            Resolution(100, 100),
+            Point(y=1, x=1),
+        ),
+    ],
+)
 def test_calc_stride(tc_params, v, t, exp):
     """Test calculated stride value is correct given world geometry and sampling."""
     tile_calc = TileCalculator(*tc_params)
     res = tile_calc.calc_stride(v, t)
     assert res == exp
 
 
-@pytest.mark.parametrize("tc_params,ims,exp", [
-    (["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
-     None, (slice(0, 500, 10), slice(0, 500, 10))),
-    (["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
-     (100, 100), (slice(0, 100, 2), slice(0, 100, 2)))
-])
+@pytest.mark.parametrize(
+    "tc_params,ims,exp",
+    [
+        (
+            ["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
+            None,
+            (slice(0, 500, 10), slice(0, 500, 10)),
+        ),
+        (
+            ["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
+            (100, 100),
+            (slice(0, 100, 2), slice(0, 100, 2)),
+        ),
+    ],
+)
 def test_calc_overview_stride(tc_params, ims, exp):
     """Test calculated stride is correct given a valid image."""
     tile_calc = TileCalculator(*tc_params)
-    res = tile_calc.calc_overview_stride(ims)
+    res = tile_calc._calc_overview_stride(ims)
     assert res == exp
 
 
-@pytest.mark.parametrize("tc_params,tiy,tix,sy,sx,fr,ofr,tl,exp", [
-    (["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
-     10, 10, 2, 2, Resolution(dy=2, dx=2), Resolution(dy=2, dx=2), 1,
-     np.array([[-220000, 220000.],
-               [-180000, 220000.],
-               [-180000, 180000.],
-               [-220000, 220000.],
-               [-180000, 180000.],
-               [-220000, 180000.]]))
-])
+@pytest.mark.parametrize(
+    "tc_params,tiy,tix,sy,sx,fr,ofr,tl,exp",
+    [
+        (
+            ["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
+            10,
+            10,
+            2,
+            2,
+            Resolution(dy=2, dx=2),
+            Resolution(dy=2, dx=2),
+            1,
+            np.array(
+                [
+                    [-220000, 220000.0],
+                    [-180000, 220000.0],
+                    [-180000, 180000.0],
+                    [-220000, 220000.0],
+                    [-180000, 180000.0],
+                    [-220000, 180000.0],
+                ]
+            ),
+        )
+    ],
+)
 def test_calc_vertex_coordinates(tc_params, tiy, tix, sy, sx, fr, ofr, tl, exp):
     """Test vertex coordinates for a given tile are correct."""
     tile_calc = TileCalculator(*tc_params)
     res = tile_calc.calc_vertex_coordinates(tiy, tix, sy, sx, fr, ofr, tl)
     assert np.array_equal(res, exp)
 
 
-@pytest.mark.parametrize("tc_params,ti,fr,ofr,tl,exp", [
-    (["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
-     10, Resolution(dy=2, dx=2), Resolution(dy=2, dx=2), 1,
-     np.array([[0.625, 0.],
-               [0.75, 0.],
-               [0.75, 1.],
-               [0.625, 0.],
-               [0.75, 1.],
-               [0.625, 1.]]))
-])
+@pytest.mark.parametrize(
+    "tc_params,ti,fr,ofr,tl,exp",
+    [
+        (
+            ["test", (500, 500), Point(500000, -500000), Resolution(200, 200), (50, 50)],
+            10,
+            Resolution(dy=2, dx=2),
+            Resolution(dy=2, dx=2),
+            1,
+            np.array([[0.625, 0.0], [0.75, 0.0], [0.75, 1.0], [0.625, 0.0], [0.75, 1.0], [0.625, 1.0]]),
+        )
+    ],
+)
 def test_calc_texture_coordinates(tc_params, ti, fr, ofr, tl, exp):
     """Test texture coordinates for a given tile are correct."""
     tile_calc = TileCalculator(*tc_params)
     res = tile_calc.calc_texture_coordinates(ti, fr, ofr, tl)
     assert np.array_equal(res, exp)
```

### Comparing `uwsift-1.2.3/uwsift/tests/workspace/test_algebraic.py` & `uwsift-2.0.0b0/uwsift/tests/workspace/test_algebraic.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,40 +1,42 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
 """Tests for algebraic layer operations."""
 
-import numpy as np
 from datetime import datetime
-from uwsift.common import Info, Platform, Instrument
+
+import numpy as np
+
+from uwsift.common import Info, Instrument, Platform
 
 
 def _get_data_array_generator(data_arrs):
     """Help mimic what a real Satpy Scene would do."""
     yield from data_arrs
 
 
 def test_satpy_importer_basic(tmpdir, monkeypatch, mocker):
     """Basic import test using Satpy."""
-    from uwsift.workspace import Workspace
-    from uwsift.model.layer import DocBasicLayer
     from uuid import uuid1 as uuidgen
-    ws = Workspace(str(tmpdir))
+
+    from uwsift.workspace import CachingWorkspace
+
+    ws = CachingWorkspace(str(tmpdir))
     c01_attrs = {
-        Info.SHORT_NAME: 'C01',
-        Info.DATASET_NAME: 'C01',
+        Info.SHORT_NAME: "C01",
+        Info.DATASET_NAME: "C01",
         Info.CENTRAL_WAVELENGTH: 2.0,
         Info.UUID: uuidgen(),
     }
     c03_attrs = {
-        Info.SHORT_NAME: 'C03',
-        Info.DATASET_NAME: 'C03',
+        Info.SHORT_NAME: "C03",
+        Info.DATASET_NAME: "C03",
         Info.CENTRAL_WAVELENGTH: 4.0,
         Info.UUID: uuidgen(),
     }
-    doc = mocker.MagicMock()
     for ds in [c01_attrs, c03_attrs]:
         ds[Info.ORIGIN_X] = -5434894.885056
         ds[Info.ORIGIN_Y] = 5434894.885056
         ds[Info.CELL_HEIGHT] = 1000.0
         ds[Info.CELL_WIDTH] = 1000.0
         ds[Info.STANDARD_NAME] = "toa_bidirectional_reflectance"
         ds[Info.VALID_RANGE] = (0, 120)
@@ -42,26 +44,26 @@
         ds[Info.SHAPE] = (2, 2)
         ds[Info.PROJ] = "+proj=merc"
         ds[Info.FAMILY] = "family"
         ds[Info.CATEGORY] = "category"
         ds[Info.SERIAL] = "serial"
         ds[Info.PLATFORM] = Platform.GOES_16
         ds[Info.INSTRUMENT] = Instrument.ABI
+        ds[Info.GRID_ORIGIN] = "SE"
+        ds[Info.GRID_FIRST_INDEX_X] = 1
+        ds[Info.GRID_FIRST_INDEX_Y] = 1
 
-    c01 = DocBasicLayer(doc, c01_attrs)
-    c03 = DocBasicLayer(doc, c03_attrs)
     ops = "z = x - y"
-    ns = {'y': c03_attrs[Info.UUID], 'x': c01_attrs[Info.UUID]}
+    ns = {"y": c03_attrs[Info.UUID], "x": c01_attrs[Info.UUID]}
 
     def get_metadata(u):
-        return c01 if u == c01_attrs[Info.UUID] else c03
+        return c01_attrs if u == c01_attrs[Info.UUID] else c03_attrs
 
     def get_content(u):
         return np.ones((2, 2)) if u == c01_attrs[Info.UUID] else np.zeros((2, 2))
 
-    monkeypatch.setattr(ws, 'get_metadata', get_metadata)
-    monkeypatch.setattr(ws, 'get_content', get_content)
-    uuid, info, data = ws.create_algebraic_composite(
-        ops, ns, info={Info.SHORT_NAME: 'new', Info.DATASET_NAME: 'new'})
+    monkeypatch.setattr(ws, "get_metadata", get_metadata)
+    monkeypatch.setattr(ws, "get_content", get_content)
+    uuid, info, data = ws.create_algebraic_composite(ops, ns, info={Info.SHORT_NAME: "new", Info.DATASET_NAME: "new"})
 
     np.testing.assert_equal(data, 1)
     assert info.get(Info.STANDARD_NAME) == "unknown"
```

### Comparing `uwsift-1.2.3/uwsift/tests/workspace/test_importer.py` & `uwsift-2.0.0b0/uwsift/tests/workspace/test_importer.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
 """Tests for the importer functions and classes."""
 
 import os
-import yaml
-import xarray as xr
-import numpy as np
-import dask.array as da
 from datetime import datetime
+
+import dask.array as da
+import numpy as np
+import pytest
+import xarray as xr
+import yaml
+from pyresample.geometry import AreaDefinition
 from satpy import Scene
 from satpy.tests.utils import make_dataid
-from pyresample.geometry import AreaDefinition
-from uwsift.workspace.importer import available_satpy_readers, SatpyImporter
-from uwsift.common import Info, Kind, Platform, Instrument
 
-import pytest
+from uwsift.common import Info, Instrument, Platform
+from uwsift.workspace.importer import SatpyImporter, available_satpy_readers
 
 
 def test_available_satpy_readers_defaults():
     """Test default behavior of the satpy reader function."""
     readers = available_satpy_readers()
     assert isinstance(readers, list)
     assert len(readers) != 0
@@ -31,47 +32,53 @@
     assert isinstance(readers, list)
     assert len(readers) != 0
     assert isinstance(readers[0], dict)
 
 
 def test_available_satpy_readers_no_cache(tmpdir, monkeypatch):
     """Test loading the satpy readers when we know the cache is missing."""
-    p = tmpdir.join('satpy_available_readers.yaml')
-    monkeypatch.setattr('uwsift.workspace.importer.SATPY_READER_CACHE_FILE', str(p))
+    p = tmpdir.join("satpy_available_readers.yaml")
+    monkeypatch.setattr("uwsift.workspace.importer.SATPY_READER_CACHE_FILE", str(p))
     readers = available_satpy_readers()
     assert isinstance(readers, list)
     assert len(readers) != 0
     assert isinstance(readers[0], str)
 
 
 def test_available_satpy_readers_empty_cache(tmpdir, monkeypatch):
     """Test loading the satpy readers when the cache exists but is empty."""
-    p = tmpdir.join('satpy_available_readers.yaml')
-    with open(p, 'w') as cfile:
+    p = tmpdir.join("satpy_available_readers.yaml")
+    with open(p, "w") as cfile:
         yaml.dump({}, cfile)
-    monkeypatch.setattr('uwsift.workspace.importer.SATPY_READER_CACHE_FILE', str(p))
+    monkeypatch.setattr("uwsift.workspace.importer.SATPY_READER_CACHE_FILE", str(p))
     readers = available_satpy_readers()
     assert isinstance(readers, list)
     assert len(readers) != 0
     assert isinstance(readers[0], str)
 
 
 def test_available_satpy_readers_known_cache(tmpdir, monkeypatch):
     """Test loading the satpy readers when the cache exists."""
-    p = tmpdir.join('satpy_available_readers.yaml')
-    with open(p, 'w') as cfile:
+
+    # TODO: The next line of code skips the test.
+    #  If this test is no longer to be skipped, this line must be removed.
+    #  Adjustments may have to be made to make the test work correctly again.
+    pytest.skip("Satpy available readers caching is deactivated at the moment.")
+
+    p = tmpdir.join("satpy_available_readers.yaml")
+    with open(p, "w") as cfile:
         yaml.dump({}, cfile)
-    monkeypatch.setattr('uwsift.workspace.importer.SATPY_READER_CACHE_FILE', str(p))
-    monkeypatch.setattr('uwsift.workspace.importer._SATPY_READERS', None)
+    monkeypatch.setattr("uwsift.workspace.importer.SATPY_READER_CACHE_FILE", str(p))
+    monkeypatch.setattr("uwsift.workspace.importer._SATPY_READERS", None)
     # create the cache
     _ = available_satpy_readers()
     mod_time1 = os.stat(p).st_mtime
 
     # load from the cache
-    monkeypatch.setattr('uwsift.workspace.importer._SATPY_READERS', None)
+    monkeypatch.setattr("uwsift.workspace.importer._SATPY_READERS", None)
     _ = available_satpy_readers()
     mod_time2 = os.stat(p).st_mtime
     assert mod_time1 == mod_time2, "Cache was not reused"
 
     # force recreation of the cache
     readers = available_satpy_readers(force_cache_refresh=True)
     mod_time3 = os.stat(p).st_mtime
@@ -84,128 +91,76 @@
 def _get_data_array_generator(data_arrs):
     """Help mimic what a real Satpy Scene would do."""
     yield from data_arrs
 
 
 def _get_fake_g16_abi_c01_scene(mocker):
     attrs = {
-        'name': 'C01',
-        'wavelength': (1.0, 2.0, 3.0),
-        'area': AreaDefinition(
-            'test', 'test', 'test',
+        "name": "C01",
+        "wavelength": (1.0, 2.0, 3.0),
+        "area": AreaDefinition(
+            "test",
+            "test",
+            "test",
             {
-                'proj': 'geos',
-                'sweep': 'x',
-                'lon_0': -75,
-                'h': 35786023,
-                'ellps': 'GRS80',
-                'units': 'm',
-            }, 5, 5,
-            (-5434894.885056, -5434894.885056, 5434894.885056, 5434894.885056)
+                "proj": "geos",
+                "sweep": "x",
+                "lon_0": -75,
+                "h": 35786023,
+                "ellps": "GRS80",
+                "units": "m",
+            },
+            5,
+            5,
+            (-5434894.885056, -5434894.885056, 5434894.885056, 5434894.885056),
         ),
-        'start_time': datetime(2018, 9, 10, 17, 0, 31, 100000),
-        'end_time': datetime(2018, 9, 10, 17, 11, 7, 800000),
-        'standard_name': 'toa_bidirectional_reflectance',
-        'sensor': 'abi',
-        'platform_name': 'GOES-16',
-        'platform_shortname': 'G16',
+        "start_time": datetime(2018, 9, 10, 17, 0, 31, 100000),
+        "end_time": datetime(2018, 9, 10, 17, 11, 7, 800000),
+        "standard_name": "toa_bidirectional_reflectance",
+        "sensor": "abi",
+        "platform_name": "GOES-16",
+        "platform_shortname": "G16",
     }
-    data_arr = xr.DataArray(da.from_array(np.empty((5, 5), dtype=np.float64), chunks='auto'),
-                            attrs=attrs)
+    data_arr = xr.DataArray(da.from_array(np.empty((5, 5), dtype=np.float64), chunks="auto"), attrs=attrs)
     scn = Scene()
-    scn['C01'] = data_arr
+    scn["C01"] = data_arr
     scn.load = mocker.MagicMock()  # don't do anything on load
     return scn
 
 
 def _get_fake_g18_abi_c01_scene(mocker):
     scn = _get_fake_g16_abi_c01_scene(mocker)
-    scn['C01'].attrs['platform_name'] = 'GOES-18'
-    scn['C01'].attrs['platform_shortname'] = 'G18'
+    scn["C01"].attrs["platform_name"] = "GOES-18"
+    scn["C01"].attrs["platform_shortname"] = "G18"
     return scn
 
 
 def _get_fake_g18_abi_c01_scene_no_pname(mocker):
     # old versions of satpy didn't assign a proper platform_name
     scn = _get_fake_g18_abi_c01_scene(mocker)
-    scn['C01'].attrs['platform_name'] = None
+    scn["C01"].attrs["platform_name"] = None
     return scn
 
 
 @pytest.mark.parametrize(
     ["get_scene", "exp_platform"],
     [
         (_get_fake_g16_abi_c01_scene, Platform.GOES_16),
         (_get_fake_g18_abi_c01_scene, Platform.GOES_18),
         (_get_fake_g18_abi_c01_scene_no_pname, Platform.GOES_18),
-    ]
+    ],
 )
 def test_satpy_importer_basic(get_scene, exp_platform, tmpdir, monkeypatch, mocker):
     """Basic import test using Satpy."""
     db_sess = mocker.MagicMock()
     scn = get_scene(mocker)
-    imp = SatpyImporter(['/test/file.nc'], tmpdir, db_sess,
-                        scene=scn,
-                        reader='abi_l1b',
-                        dataset_ids=[make_dataid(name='C01')])
+    imp = SatpyImporter(
+        ["/test/file.nc"], tmpdir, db_sess, scene=scn, reader="abi_l1b", dataset_ids=[make_dataid(name="C01")]
+    )
     imp.merge_resources()
     assert imp.num_products == 1
     products = list(imp.merge_products())
     assert len(products) == 1
     assert products[0].info[Info.CENTRAL_WAVELENGTH] == 2.0
-    assert products[0].info[Info.STANDARD_NAME] == 'toa_bidirectional_reflectance'
+    assert products[0].info[Info.STANDARD_NAME] == "toa_bidirectional_reflectance"
     assert products[0].info[Info.PLATFORM] == exp_platform
     assert products[0].info[Info.INSTRUMENT] == Instrument.ABI
-
-
-def test_satpy_importer_contour_0_360(tmpdir, monkeypatch, mocker):
-    """Test import of grib contour data using Satpy."""
-    db_sess = mocker.MagicMock()
-    attrs = {
-        'name': 'gh',
-        'level': 125,
-        'area': AreaDefinition(
-            'test', 'test', 'test',
-            {
-                'proj': 'eqc',
-                'lon_0': 0,
-                'pm': 180,
-                'R': 6371229,
-            }, 240, 120,
-            (-20015806.220738243, -10007903.110369122, 20015806.220738243, 10007903.110369122)
-        ),
-        'start_time': datetime(2018, 9, 10, 17, 0, 31, 100000),
-        'end_time': datetime(2018, 9, 10, 17, 11, 7, 800000),
-        'model_time': datetime(2018, 9, 10, 17, 11, 7, 800000),
-        'standard_name': 'geopotential_height',
-    }
-    data_arr = xr.DataArray(da.from_array(np.random.random((120, 240)).astype(np.float64), chunks='auto'),
-                            attrs=attrs)
-    scn = Scene()
-    scn['gh'] = data_arr
-    scn.load = mocker.MagicMock()  # don't do anything on load
-
-    imp = SatpyImporter(['/test/file.nc'], tmpdir, db_sess,
-                        scene=scn,
-                        reader='grib',
-                        dataset_ids=[make_dataid(name='gh', level=125)])
-    imp.merge_resources()
-    assert imp.num_products == 1
-    products = list(imp.merge_products())
-    assert len(products) == 1
-    assert products[0].info[Info.STANDARD_NAME] == 'geopotential_height'
-    assert products[0].info[Info.KIND] == Kind.CONTOUR
-
-    query_mock = mocker.MagicMock(name='query')
-    filter1_mock = mocker.MagicMock(name='filter1')
-    filter2_mock = mocker.MagicMock(name='filter2')
-    db_sess.query.return_value = query_mock
-    query_mock.filter.return_value = filter1_mock
-    filter1_mock.filter.return_value = filter2_mock
-    filter2_mock.all.return_value = products
-    import_gen = imp.begin_import_products()
-    content_progresses = list(import_gen)
-    # image and contour content
-    assert len(content_progresses) == 2
-    # make sure data was swapped to -180/180 space
-    assert (content_progresses[0].data[:, :120] == data_arr.data[:, 120:].astype(np.float32)).all()
-    assert (content_progresses[0].data[:, 120:] == data_arr.data[:, :120].astype(np.float32)).all()
```

### Comparing `uwsift-1.2.3/uwsift/ui/__init__.py` & `uwsift-2.0.0b0/uwsift/ui/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,37 +13,43 @@
 REQUIRES
 
 
 :author: R.K.Garcia <rayg@ssec.wisc.edu>
 :copyright: 2014 by University of Wisconsin Regents, see AUTHORS for more details
 :license: GPLv3, see LICENSE for more details
 """
-__author__ = 'rayg'
-__docformat__ = 'reStructuredText'
+__author__ = "rayg"
+__docformat__ = "reStructuredText"
 
 import argparse
 import logging
 import sys
 import unittest
+from pathlib import Path
+
+# Expose path to qml files.
+QML_PATH = Path(__file__).parent.absolute()
 
 LOG = logging.getLogger(__name__)
 
 
 def main():
-    parser = argparse.ArgumentParser(
-        description="PURPOSE",
-        epilog="",
-        fromfile_prefix_chars='@')
-    parser.add_argument('-v', '--verbose', dest='verbosity', action="count", default=0,
-                        help='each occurrence increases verbosity 1 level through ERROR-WARNING-Info-DEBUG')
+    parser = argparse.ArgumentParser(description="PURPOSE", epilog="", fromfile_prefix_chars="@")
+    parser.add_argument(
+        "-v",
+        "--verbose",
+        dest="verbosity",
+        action="count",
+        default=0,
+        help="each occurrence increases verbosity 1 level through ERROR-WARNING-Info-DEBUG",
+    )
     # http://docs.python.org/2.7/library/argparse.html#nargs
     # parser.add_argument('--stuff', nargs='5', dest='my_stuff',
     #                    help="one or more random things")
-    parser.add_argument('pos_args', nargs='*',
-                        help="positional arguments don't have the '-' prefix")
+    parser.add_argument("pos_args", nargs="*", help="positional arguments don't have the '-' prefix")
     args = parser.parse_args()
 
     levels = [logging.ERROR, logging.WARN, logging.INFO, logging.DEBUG]
     logging.basicConfig(level=levels[min(3, args.verbosity)])
 
     if not args.pos_args:
         unittest.main()
@@ -51,9 +57,9 @@
 
     for pn in args.pos_args:
         pass
 
     return 0
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     sys.exit(main())
```

### Comparing `uwsift-1.2.3/uwsift/ui/change_colormap_dialog_ui.py` & `uwsift-2.0.0b0/uwsift/ui/change_colormap_dialog_ui.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,26 +1,18 @@
 # -*- coding: utf-8 -*-
 
 # Form implementation generated from reading ui file 'change_colormap_dialog.ui'
 #
-# Created by: PyQt4 UI code generator 4.11.4
+# Created by: PyQt5 UI code generator 5.15.7
 #
-# WARNING! All changes made in this file will be lost!
+# WARNING: Any manual changes made to this file will be lost when pyuic5 is
+# run again.  Do not edit this file unless you know what you are doing.
 
-from PyQt5 import QtCore, QtGui, QtWidgets
-
-try:
-    _encoding = QtWidgets.QApplication.UnicodeUTF8
 
-
-    def _translate(context, text, disambig):
-        return QtCore.QCoreApplication.translate(context, text, disambig, _encoding)
-except AttributeError:
-    def _translate(context, text, disambig):
-        return QtCore.QCoreApplication.translate(context, text, disambig)
+from PyQt5 import QtCore, QtGui, QtWidgets
 
 
 class Ui_changeColormapDialog(object):
     def setupUi(self, changeColormapDialog):
         changeColormapDialog.setObjectName("changeColormapDialog")
         changeColormapDialog.resize(351, 199)
         sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Fixed, QtWidgets.QSizePolicy.Fixed)
@@ -61,21 +53,22 @@
         self.gammaSpinBox.setObjectName("gammaSpinBox")
         self.gammaLabel = QtWidgets.QLabel(changeColormapDialog)
         self.gammaLabel.setGeometry(QtCore.QRect(200, 120, 56, 24))
         self.gammaLabel.setAlignment(QtCore.Qt.AlignLeading | QtCore.Qt.AlignLeft | QtCore.Qt.AlignVCenter)
         self.gammaLabel.setObjectName("gammaLabel")
 
         self.retranslateUi(changeColormapDialog)
-        self.buttons.accepted.connect(changeColormapDialog.accept)
-        self.buttons.rejected.connect(changeColormapDialog.reject)
+        self.buttons.accepted.connect(changeColormapDialog.accept)  # type: ignore
+        self.buttons.rejected.connect(changeColormapDialog.reject)  # type: ignore
         QtCore.QMetaObject.connectSlotsByName(changeColormapDialog)
 
     def retranslateUi(self, changeColormapDialog):
-        changeColormapDialog.setWindowTitle(_translate("changeColormapDialog", "Change Colormap", None))
-        self.vmin_edit.setToolTip(_translate("changeColormapDialog", "minimum color limit", None))
-        self.vmin_slider.setToolTip(_translate("changeColormapDialog", "minimum color limit", None))
-        self.vmax_slider.setToolTip(_translate("changeColormapDialog", "maximum color limit", None))
-        self.vmax_edit.setToolTip(_translate("changeColormapDialog", "maximum color limit", None))
-        self.gammaLabel.setText(_translate("changeColormapDialog", "Gamma: ", None))
+        _translate = QtCore.QCoreApplication.translate
+        changeColormapDialog.setWindowTitle(_translate("changeColormapDialog", "Change Colormap"))
+        self.vmin_edit.setToolTip(_translate("changeColormapDialog", "minimum color limit"))
+        self.vmin_slider.setToolTip(_translate("changeColormapDialog", "minimum color limit"))
+        self.vmax_slider.setToolTip(_translate("changeColormapDialog", "maximum color limit"))
+        self.vmax_edit.setToolTip(_translate("changeColormapDialog", "maximum color limit"))
+        self.gammaLabel.setText(_translate("changeColormapDialog", "Gamma: "))
 
 
 from uwsift.ui.custom_widgets import QNoScrollDoubleSpinBox
```

### Comparing `uwsift-1.2.3/uwsift/ui/custom_widgets.py` & `uwsift-2.0.0b0/uwsift/ui/custom_widgets.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,28 +1,30 @@
 from PyQt5.QtCore import Qt
-from PyQt5.QtWidgets import QComboBox, QSlider, QDoubleSpinBox, QWizardPage, QTableWidget, QListWidget
 from PyQt5.QtWebEngineWidgets import QWebEngineView
+from PyQt5.QtWidgets import (
+    QComboBox,
+    QDoubleSpinBox,
+    QListWidget,
+    QSlider,
+    QTableWidget,
+    QWizardPage,
+)
 
 
 class QNoScrollComboBox(QComboBox):
-    """Special subclass of QComboBox to stop it from taking focus on scroll over
-    """
+    """Special subclass of QComboBox to stop it from taking focus on scroll over"""
 
     def __init__(self, *args, **kwargs):
         super(QNoScrollComboBox, self).__init__(*args, **kwargs)
         self.setFocusPolicy(Qt.StrongFocus)
 
     def wheelEvent(self, ev):
         # If we want it to scroll when it has focus then uncomment
         # Currently not desired for Projection combo box, but may
         # be desired for RGB layer selector
-        # if not self.hasFocus():
-        #    ev.ignore()
-        # else:
-        #    super(QNoScrollComboBox, self).wheelEvent(ev)
         ev.ignore()
 
 
 class QNoScrollSlider(QSlider):
     def __init__(self, *args, **kwargs):
         super(QNoScrollSlider, self).__init__(*args, **kwargs)
         self.setFocusPolicy(Qt.StrongFocus)
@@ -93,7 +95,22 @@
                     break
             else:
                 return False
         return True
 
     def completeChangedSlot(self, *args, **kwargs):
         self.completeChanged.emit()
+
+
+class InitiallyIncompleteWizardPage(QWizardPage):
+    """QWizardPage that is only complete once 'page_complete' is set to True"""
+
+    def __init__(self, *args, **kwargs):
+        self.page_complete = False
+        super(InitiallyIncompleteWizardPage, self).__init__(*args, **kwargs)
+
+    # override default behaviour by allowing the code to disable 'Next'/'Finish' buttons
+    def isComplete(self):
+        return self.page_complete
+
+    def completeChangedSlot(self, *args, **kwargs):
+        self.completeChanged.emit()
```

### Comparing `uwsift-1.2.3/uwsift/ui/export_image_dialog.ui` & `uwsift-2.0.0b0/uwsift/ui/export_image_dialog.ui`

 * *Files 0% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/ui/export_image_dialog.ui` & `uwsift-2.0.0b0/uwsift/ui/export_image_dialog.ui`

```diff
@@ -255,15 +255,15 @@
             <x>10</x>
             <y>30</y>
             <width>97</width>
             <height>21</height>
           </rect>
         </property>
         <property name="toolTip">
-          <string>delay based on layer observation time</string>
+          <string>delay based on dataset observation time</string>
         </property>
         <property name="text">
           <string>Time Lapse</string>
         </property>
         <property name="checked">
           <bool>true</bool>
         </property>
```

### Comparing `uwsift-1.2.3/uwsift/ui/export_image_dialog_ui.py` & `uwsift-2.0.0b0/uwsift/ui/export_image_dialog_ui.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 # -*- coding: utf-8 -*-
 
 # Form implementation generated from reading ui file 'export_image_dialog.ui'
 #
-# Created by: PyQt5 UI code generator 5.9.2
+# Created by: PyQt5 UI code generator 5.15.7
 #
-# WARNING! All changes made in this file will be lost!
+# WARNING: Any manual changes made to this file will be lost when pyuic5 is
+# run again.  Do not edit this file unless you know what you are doing.
+
 
 from PyQt5 import QtCore, QtGui, QtWidgets
 
 
 class Ui_ExportImageDialog(object):
     def setupUi(self, ExportImageDialog):
         ExportImageDialog.setObjectName("ExportImageDialog")
@@ -24,15 +26,15 @@
         self.buttonBox.setGeometry(QtCore.QRect(110, 450, 161, 32))
         self.buttonBox.setOrientation(QtCore.Qt.Horizontal)
         self.buttonBox.setStandardButtons(QtWidgets.QDialogButtonBox.Cancel | QtWidgets.QDialogButtonBox.Save)
         self.buttonBox.setCenterButtons(False)
         self.buttonBox.setObjectName("buttonBox")
         self.frameRangeGroupBox = QtWidgets.QGroupBox(ExportImageDialog)
         self.frameRangeGroupBox.setGeometry(QtCore.QRect(10, 30, 251, 111))
-        self.frameRangeGroupBox.setAlignment(int(QtCore.Qt.AlignLeading | QtCore.Qt.AlignLeft | QtCore.Qt.AlignTop))
+        self.frameRangeGroupBox.setAlignment(QtCore.Qt.AlignLeading | QtCore.Qt.AlignLeft | QtCore.Qt.AlignTop)
         self.frameRangeGroupBox.setFlat(False)
         self.frameRangeGroupBox.setCheckable(False)
         self.frameRangeGroupBox.setObjectName("frameRangeGroupBox")
         self.frameCurrentRadio = QtWidgets.QRadioButton(self.frameRangeGroupBox)
         self.frameCurrentRadio.setGeometry(QtCore.QRect(10, 20, 121, 21))
         self.frameCurrentRadio.setChecked(True)
         self.frameCurrentRadio.setObjectName("frameCurrentRadio")
@@ -148,16 +150,16 @@
         self.footerFontSizeSpinBox.setMaximum(72)
         self.footerFontSizeSpinBox.setProperty("value", 11)
         self.footerFontSizeSpinBox.setObjectName("footerFontSizeSpinBox")
         self.horizontalLayout.addWidget(self.footerFontSizeSpinBox)
         self.horizontalLayout_2.addLayout(self.horizontalLayout)
 
         self.retranslateUi(ExportImageDialog)
-        self.buttonBox.accepted.connect(ExportImageDialog.accept)
-        self.buttonBox.rejected.connect(ExportImageDialog.reject)
+        self.buttonBox.accepted.connect(ExportImageDialog.accept)  # type: ignore
+        self.buttonBox.rejected.connect(ExportImageDialog.reject)  # type: ignore
         QtCore.QMetaObject.connectSlotsByName(ExportImageDialog)
 
     def retranslateUi(self, ExportImageDialog):
         _translate = QtCore.QCoreApplication.translate
         ExportImageDialog.setWindowTitle(_translate("ExportImageDialog", "Export Image"))
         self.frameRangeGroupBox.setTitle(_translate("ExportImageDialog", "Frame Range"))
         self.frameCurrentRadio.setText(_translate("ExportImageDialog", "Current"))
@@ -166,15 +168,15 @@
         self.label.setText(_translate("ExportImageDialog", "from:"))
         self.label_2.setText(_translate("ExportImageDialog", "to:"))
         self.saveAsButton.setText(_translate("ExportImageDialog", "..."))
         self.animationGroupBox.setTitle(_translate("ExportImageDialog", "Animation Type"))
         self.loopRadio.setText(_translate("ExportImageDialog", "Loop"))
         self.rockRadio.setText(_translate("ExportImageDialog", "Rock"))
         self.frameDelayGroup.setTitle(_translate("ExportImageDialog", "Frame Delay"))
-        self.timeLapseRadio.setToolTip(_translate("ExportImageDialog", "delay based on layer observation time"))
+        self.timeLapseRadio.setToolTip(_translate("ExportImageDialog", "delay based on dataset observation time"))
         self.timeLapseRadio.setText(_translate("ExportImageDialog", "Time Lapse"))
         self.constantDelayRadio.setText(_translate("ExportImageDialog", "Constant:"))
         self.constantDelaySpin.setSuffix(_translate("ExportImageDialog", "ms"))
         self.fpsDelayRadio.setText(_translate("ExportImageDialog", "FPS:"))
         self.frameDelayGroup_2.setTitle(_translate("ExportImageDialog", "Export Colorbar"))
         self.colorbarNoneRadio.setText(_translate("ExportImageDialog", "None"))
         self.colorbarHorizontalRadio.setText(_translate("ExportImageDialog", "Horizontal"))
```

### Comparing `uwsift-1.2.3/uwsift/ui/open_cache_dialog.ui` & `uwsift-2.0.0b0/uwsift/ui/open_cache_dialog.ui`

 * *Files 7% similar despite different names*

#### Comparing `uwsift-1.2.3/uwsift/ui/open_cache_dialog.ui` & `uwsift-2.0.0b0/uwsift/ui/open_cache_dialog.ui`

```diff
@@ -7,23 +7,23 @@
         <x>0</x>
         <y>0</y>
         <width>593</width>
         <height>427</height>
       </rect>
     </property>
     <property name="windowTitle">
-      <string>Open Cached Layers</string>
+      <string>Open Cached Datasets</string>
     </property>
     <layout class="QVBoxLayout" name="verticalLayout">
       <item>
         <layout class="QHBoxLayout" name="horizontalLayout">
           <item>
             <widget class="QLabel" name="label">
               <property name="text">
-                <string>Pre-processed layers stored in cache will load quickly.</string>
+                <string>Pre-processed datasets stored in cache will load quickly.</string>
               </property>
             </widget>
           </item>
           <item>
             <spacer name="horizontalSpacer">
               <property name="orientation">
                 <enum>Qt::Horizontal</enum>
@@ -35,15 +35,15 @@
                 </size>
               </property>
             </spacer>
           </item>
           <item>
             <widget class="QPushButton" name="removeFromCacheButton">
               <property name="text">
-                <string>Remove Selected Cached Layers</string>
+                <string>Remove Selected Cached Datasets</string>
               </property>
             </widget>
           </item>
         </layout>
       </item>
       <item>
         <widget class="QListWidget" name="cacheListWidget">
```

### Comparing `uwsift-1.2.3/uwsift/ui/open_cache_dialog_ui.py` & `uwsift-2.0.0b0/uwsift/ui/open_cache_dialog_ui.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,26 +1,18 @@
 # -*- coding: utf-8 -*-
 
 # Form implementation generated from reading ui file 'open_cache_dialog.ui'
 #
-# Created by: PyQt4 UI code generator 4.11.4
+# Created by: PyQt5 UI code generator 5.15.7
 #
-# WARNING! All changes made in this file will be lost!
-
-from PyQt5 import QtCore, QtGui, QtWidgets
-
-try:
-    _encoding = QtWidgets.QApplication.UnicodeUTF8
+# WARNING: Any manual changes made to this file will be lost when pyuic5 is
+# run again.  Do not edit this file unless you know what you are doing.
 
 
-    def _translate(context, text, disambig):
-        return QtCore.QCoreApplication.translate(context, text, disambig, _encoding)
-except AttributeError:
-    def _translate(context, text, disambig):
-        return QtCore.QCoreApplication.translate(context, text, disambig)
+from PyQt5 import QtCore, QtGui, QtWidgets
 
 
 class Ui_openFromCacheDialog(object):
     def setupUi(self, openFromCacheDialog):
         openFromCacheDialog.setObjectName("openFromCacheDialog")
         openFromCacheDialog.resize(593, 427)
         self.verticalLayout = QtWidgets.QVBoxLayout(openFromCacheDialog)
@@ -43,16 +35,29 @@
         self.buttonBox = QtWidgets.QDialogButtonBox(openFromCacheDialog)
         self.buttonBox.setOrientation(QtCore.Qt.Horizontal)
         self.buttonBox.setStandardButtons(QtWidgets.QDialogButtonBox.Cancel | QtWidgets.QDialogButtonBox.Ok)
         self.buttonBox.setObjectName("buttonBox")
         self.verticalLayout.addWidget(self.buttonBox)
 
         self.retranslateUi(openFromCacheDialog)
-        self.buttonBox.accepted.connect(openFromCacheDialog.accept)
-        self.buttonBox.rejected.connect(openFromCacheDialog.reject)
+        self.buttonBox.accepted.connect(openFromCacheDialog.accept)  # type: ignore
+        self.buttonBox.rejected.connect(openFromCacheDialog.reject)  # type: ignore
         QtCore.QMetaObject.connectSlotsByName(openFromCacheDialog)
 
     def retranslateUi(self, openFromCacheDialog):
-        openFromCacheDialog.setWindowTitle(_translate("openFromCacheDialog", "Open Cached Layers", None))
+        _translate = QtCore.QCoreApplication.translate
+        openFromCacheDialog.setWindowTitle(_translate("openFromCacheDialog", "Open Cached Datasets"))
         self.label.setText(
-            _translate("openFromCacheDialog", "Pre-processed layers stored in cache will load quickly.", None))
-        self.removeFromCacheButton.setText(_translate("openFromCacheDialog", "Remove Selected Cached Layers", None))
+            _translate("openFromCacheDialog", "Pre-processed datasets stored in cache will load quickly.")
+        )
+        self.removeFromCacheButton.setText(_translate("openFromCacheDialog", "Remove Selected Cached Datasets"))
+
+
+if __name__ == "__main__":
+    import sys
+
+    app = QtWidgets.QApplication(sys.argv)
+    openFromCacheDialog = QtWidgets.QDialog()
+    ui = Ui_openFromCacheDialog()
+    ui.setupUi(openFromCacheDialog)
+    openFromCacheDialog.show()
+    sys.exit(app.exec_())
```

### Comparing `uwsift-1.2.3/uwsift/util/__init__.py` & `uwsift-2.0.0b0/uwsift/util/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -2,50 +2,58 @@
 # -*- coding: utf-8 -*-
 
 import logging
 import os
 import sys
 
 from uwsift.util.default_paths import (  # noqa
-    WORKSPACE_DB_DIR, DOCUMENT_SETTINGS_DIR, USER_DESKTOP_DIRECTORY,
-    USER_CACHE_DIR)
+    DOCUMENT_SETTINGS_DIR,
+    USER_CACHE_DIR,
+    USER_DESKTOP_DIRECTORY,
+    WORKSPACE_DB_DIR,
+)
+from uwsift.util.heap_profiler import HeapProfiler  # noqa
 
 LOG = logging.getLogger(__name__)
-IS_FROZEN = getattr(sys, 'frozen', False)
+IS_FROZEN = getattr(sys, "frozen", False)
 SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))
 
 
 def check_imageio_deps():
-    if IS_FROZEN and not os.getenv('IMAGEIO_FFMPEG_EXE'):
-        ffmpeg_exe = os.path.realpath(os.path.join(SCRIPT_DIR, '..', '..', 'ffmpeg'))
+    if IS_FROZEN and not os.getenv("IMAGEIO_FFMPEG_EXE"):
+        ffmpeg_exe = os.path.realpath(os.path.join(SCRIPT_DIR, "..", "..", "ffmpeg"))
         LOG.debug("Setting ffmpeg location to %s", ffmpeg_exe)
-        os.environ['IMAGEIO_FFMPEG_EXE'] = ffmpeg_exe
+        os.environ["IMAGEIO_FFMPEG_EXE"] = ffmpeg_exe
 
 
 def check_grib_definition_dir():
     # patch GRIB API C library when frozen
-    var_name = 'ECCODES_DEFINITION_PATH'
+    # second variable introduced after following commit raises a warning
+    # https://github.com/ecmwf/ecmwflibs/commit/f97bc933e4f952670d1db60ffbe6fb92b5f93234
+    var_names = ["ECCODES_DEFINITION_PATH", "ECMWFLIBS_ECCODES_DEFINITION_PATH"]
     grib_paths = []
-    if os.getenv(var_name):
-        grib_paths.append(os.getenv(var_name))
+    for var_name in var_names:
+        if os.getenv(var_name):
+            grib_paths.append(os.getenv(var_name))
 
     # Add NCEP specific definition locations
-    grib_paths.append(os.path.realpath(os.path.join(get_package_data_dir(), 'grib_definitions')))
+    grib_paths.append(os.path.realpath(os.path.join(get_package_data_dir(), "grib_definitions")))
 
     if IS_FROZEN:
         # add the ECCodes definitions because otherwise they point to the
         # wrong location
-        grib_paths.append(os.path.realpath(os.path.join(SCRIPT_DIR, '..', '..', 'share', 'eccodes', 'definitions')))
+        grib_paths.append(os.path.realpath(os.path.join(SCRIPT_DIR, "..", "..", "share", "eccodes", "definitions")))
     else:
-        grib_paths.append(os.path.join(prefix_share_dir(), 'eccodes', 'definitions'))
+        grib_paths.append(os.path.join(prefix_share_dir(), "eccodes", "definitions"))
 
     if grib_paths:
-        grib_var_value = ':'.join(grib_paths)
-        LOG.info("Setting GRIB definition path to %s", grib_var_value)
-        os.environ[var_name] = grib_var_value
+        grib_var_value = ":".join(grib_paths)
+        for var_name in var_names:
+            LOG.info(f"Setting {var_name} definition path to {grib_var_value}")
+            os.environ[var_name] = grib_var_value
 
 
 def get_package_data_dir():
     """Return location of the package 'data' directory.
 
     When frozen the data directory is placed in 'sift_data' of the root
     package directory.
@@ -53,8 +61,15 @@
     if IS_FROZEN:
         return os.path.realpath(os.path.join(SCRIPT_DIR, "..", "..", "sift_data"))
     else:
         return os.path.realpath(os.path.join(SCRIPT_DIR, "..", "data"))
 
 
 def prefix_share_dir():
-    return os.path.realpath(os.path.join(sys.prefix, 'share'))
+    return os.path.realpath(os.path.join(sys.prefix, "share"))
+
+
+def get_base_dir():
+    """Return the uwsift installation/package base directory."""
+    if IS_FROZEN:
+        return os.path.realpath(os.path.join(SCRIPT_DIR, "..", ".."))
+    return os.path.realpath(os.path.join(SCRIPT_DIR, ".."))
```

### Comparing `uwsift-1.2.3/uwsift/view/colormap_editor.py` & `uwsift-2.0.0b0/uwsift/view/colormap_editor.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 import json
 import logging
 import math
 import os
-from PyQt5 import QtCore, QtGui, QtWidgets
 
 import pyqtgraph as pg
+from PyQt5 import QtCore, QtGui, QtWidgets
 
 LOG = logging.getLogger(__name__)
 
 
 class ColormapEditor(QtWidgets.QDialog):
     def __init__(self, doc, parent=None, **kwargs):
         super(ColormapEditor, self).__init__(parent)
@@ -17,55 +17,55 @@
         layout.setSpacing(0)
         self.setLayout(layout)
         self.doc = doc
         self.user_colormap_states = {}
         self.builtin_colormap_states = {}
 
         # Setup Color Bar & clear its data
-        self.ColorBar = pg.GradientWidget(orientation='bottom')
+        self.ColorBar = pg.GradientWidget(orientation="bottom")
         tickList = self.ColorBar.listTicks()
         for tick in tickList:
             self.ColorBar.removeTick(tick[0])
         self.ColorBar.setEnabled(False)
 
         self.CloneButton = QtWidgets.QPushButton("Clone Colormap")
-        self.CloneButton.clicked.connect(self.clone_colormap)
+        self.CloneButton.clicked.connect(self._clone_colormap)
         self.CloneButton.setEnabled(False)
 
         # Create Import button
         self.ImportButton = QtWidgets.QPushButton("Import Colormap")
-        self.ImportButton.clicked.connect(self.importButtonClick)
+        self.ImportButton.clicked.connect(self._importButtonClick)
 
         # Create Colormap List and Related Functions
         self.cmap_list = QtWidgets.QListWidget()
         self.cmap_list.setSelectionMode(QtWidgets.QAbstractItemView.ExtendedSelection)
-        self.cmap_list.itemSelectionChanged.connect(self.update_color_bar)
+        self.cmap_list.itemSelectionChanged.connect(self._update_color_bar)
 
         # Create SQRT Button and Related Functions
         self.sqrt = QtWidgets.QCheckBox("SQRT")
-        self.sqrt.clicked.connect(self.sqrt_action)
+        self.sqrt.clicked.connect(self._sqrt_action)
         self.sqrt.setEnabled(False)
 
         # Create Close button
         self.CloseButton = QtWidgets.QPushButton("Close")
         self.CloseButton.clicked.connect(self.close)
 
         # Create Delete Button and Related Functions
         self.DeleteButton = QtWidgets.QPushButton("Delete Colormap")
-        self.DeleteButton.clicked.connect(self.handle_delete_click)
+        self.DeleteButton.clicked.connect(self._handle_delete_click)
         self.DeleteButton.setEnabled(False)
 
         # Create Export Button and Related Functions
         self.ExportButton = QtWidgets.QPushButton("Export Colormap")
-        self.ExportButton.clicked.connect(self.exportButtonClick)
+        self.ExportButton.clicked.connect(self._exportButtonClick)
         self.ExportButton.setEnabled(False)
 
         # Create Save button
         self.SaveButton = QtWidgets.QPushButton("Save Colormap")
-        self.SaveButton.clicked.connect(self.save_button_click)
+        self.SaveButton.clicked.connect(self._save_button_click)
         self.SaveButton.setEnabled(False)
 
         # Add widgets to their respective spots in the UI grid
         layout.addWidget(self.ImportButton, 0, 0)
         layout.addWidget(self.SaveButton, 0, 2)
         layout.addWidget(self.sqrt, 1, 2)
         layout.addWidget(self.ColorBar, 4, 1)
@@ -77,75 +77,79 @@
 
         # Import custom colormaps
         cmap_manager = self.doc.colormaps
         for cmap in cmap_manager.iter_colormaps():
             editable = cmap_manager.is_writeable_colormap(cmap)
             cmap_obj = cmap_manager[cmap]
             if cmap_obj.colors and hasattr(cmap_obj, "_controls"):
-                is_sqrt = getattr(cmap_obj, 'sqrt', False)
+                is_sqrt = getattr(cmap_obj, "sqrt", False)
                 self.import_colormaps(cmap, cmap_obj._controls, cmap_obj.colors._rgba, sqrt=is_sqrt, editable=editable)
 
-    def save_button_click(self):
+    def _save_button_click(self):
         # save custom colormap
         name = self.cmap_list.item(self.cmap_list.currentRow()).text()
         self.user_colormap_states[name] = self.ColorBar.saveState()
         self.user_colormap_states[name]["sqrt"] = self.sqrt.isChecked()
-        self.save_new_map(self.user_colormap_states[name], name)
+        self._save_new_map(self.user_colormap_states[name], name)
 
-    def clone_colormap(self):
+    def _clone_colormap(self):
         # Clone existing colormap
-        text, ok = QtWidgets.QInputDialog.getText(self, 'Clone Colormap', 'Enter colormap name:')
-        protected_names = ['mode', 'ticks', 'step']
+        text, ok = QtWidgets.QInputDialog.getText(self, "Clone Colormap", "Enter colormap name:")
+        protected_names = ["mode", "ticks", "step"]
 
         if ok:
             save_name = str(text)
             if save_name in self.user_colormap_states or save_name in protected_names:
                 overwrite_msg = "There is already a save with this name. Would you like to Overwrite?"
                 reply = QtWidgets.QMessageBox.question(
-                    self, 'Message', overwrite_msg, QtWidgets.QMessageBox.Yes, QtWidgets.QMessageBox.No)
+                    self, "Message", overwrite_msg, QtWidgets.QMessageBox.Yes, QtWidgets.QMessageBox.No
+                )
 
                 if reply == QtWidgets.QMessageBox.Yes:
                     if save_name in self.builtin_colormap_states or save_name in protected_names:
                         QtWidgets.QMessageBox.information(
-                            self, "Error", "You cannot save a colormap with "
-                                           "the same name as one of the "
-                                           "internal colormaps or one of the "
-                                           "protected names ('mode', "
-                                           "'ticks', 'step').")
+                            self,
+                            "Error",
+                            "You cannot save a colormap with "
+                            "the same name as one of the "
+                            "internal colormaps or one of the "
+                            "protected names ('mode', "
+                            "'ticks', 'step').",
+                        )
                         reply.close()
                         return
 
                     self.user_colormap_states[save_name] = self.ColorBar.saveState()
             else:
                 if save_name in self.builtin_colormap_states:
                     QtWidgets.QMessageBox.information(
-                        self, "Error",
-                        "You cannot save a colormap with the same name as one of the internal colormaps.")
+                        self, "Error", "You cannot save a colormap with the same name as one of the internal colormaps."
+                    )
                     return
 
                 self.user_colormap_states[save_name] = self.ColorBar.saveState()
-            self.updateListWidget(save_name)
-            self.save_new_map(self.user_colormap_states[save_name], save_name)
+            self._updateListWidget(save_name)
+            self._save_new_map(self.user_colormap_states[save_name], save_name)
 
-    def toRemoveDelete(self):
+    def _toRemoveDelete(self):
         # Determine if an internal colormap is selected, returns boolean
         toReturn = False
 
         ListCount = self.cmap_list.count()
 
         index = 0
         while index < ListCount:
             if self.cmap_list.item(index).isSelected():
                 if self.cmap_list.item(index).text() in self.builtin_colormap_states:
                     toReturn = True
             index = index + 1
 
         return toReturn
 
-    def save_new_map(self, new_cmap, name):
+    def _save_new_map(self, new_cmap, name):
         # Call document function with new colormap
         self.doc.update_user_colormap(new_cmap, name)
 
     def import_colormaps(self, name, controls, colors, sqrt=False, editable=False):
         # Import a colormap into either the internal or custom colormap lists
         try:
             # FIXME: GradientWidget can accept 'allowAdd' flag for whether or
@@ -153,58 +157,59 @@
             newWidget = pg.GradientWidget()
             newWidget.hide()
 
             for tick in newWidget.listTicks():
                 newWidget.removeTick(tick[0])
 
             if not isinstance(controls, (tuple, list)):
-                # convert numpy arrays to a list so we can JSON serialize
+                # convert numpy arrays to a list, so we can JSON serialize
                 # numpy data types
                 controls = controls.tolist()
             for control, color in zip(controls, colors):
-                newWidget.addTick(control, QtGui.QColor(*(color * 255.)), movable=editable)
+                color_int = (color * 255.0).astype(int)
+                newWidget.addTick(control, QtGui.QColor(*color_int), movable=editable)
 
             if editable:
                 self.user_colormap_states[name] = newWidget.saveState()
                 self.user_colormap_states[name]["sqrt"] = sqrt
             else:
                 self.builtin_colormap_states[name] = newWidget.saveState()
-            self.updateListWidget()
+            self._updateListWidget()
         except AssertionError as e:
             LOG.error(e)
 
-    def updateListWidget(self, to_show=None):
+    def _updateListWidget(self, to_show=None):
         # Update list widget with new colormap list
         self.cmap_list.clear()
 
         total_count = 0
         corVal = 0
         for key in self.user_colormap_states:
             self.cmap_list.addItem(key)
             total_count += 1
             if to_show is not None and key == to_show:
                 corVal = total_count
 
-        self.cmap_list.addItem("----------------------------- "
-                               "Below Are Builtin ColorMaps"
-                               " -----------------------------")
+        self.cmap_list.addItem(
+            "----------------------------- " "Below Are Builtin ColorMaps" " -----------------------------"
+        )
         barrier_item = self.cmap_list.item(total_count)
         barrier_item.setFlags(QtCore.Qt.NoItemFlags)
         total_count += 1
 
         for key2 in self.builtin_colormap_states:
             self.cmap_list.addItem(key2)
             total_count += 1
             if to_show is not None and key2 == to_show:
                 corVal = total_count
 
         if to_show is not None:
             self.cmap_list.setCurrentRow(corVal, QtCore.QItemSelectionModel.Select)
 
-    def update_color_bar(self):
+    def _update_color_bar(self):
         # Update the colorbar with the newly selected colormap
         # FIXME: isn't this redundant?
         self.sqrt.setCheckState(False)
 
         cmap_name = self.cmap_list.item(self.cmap_list.currentRow()).text()
         if cmap_name in self.user_colormap_states:
             NewBar = self.user_colormap_states[self.cmap_list.item(self.cmap_list.currentRow()).text()]
@@ -237,115 +242,118 @@
             self.sqrt.setEnabled(showDel)
             self.CloneButton.setEnabled(True)
             self.SaveButton.setEnabled(showDel)
             self.ExportButton.setEnabled(True)
 
         self.DeleteButton.setEnabled(showDel)
 
-    def sqrt_action(self):
+    def _sqrt_action(self):
         # If square root button is checked/unchecked, modify the ticks as such
         if self.sqrt.isChecked():
             tickList = self.ColorBar.listTicks()
             for tick in tickList:
                 self.ColorBar.setTickValue(tick[0], math.sqrt(self.ColorBar.tickValue(tick[0])))
         else:
             tickList = self.ColorBar.listTicks()
             for tick in tickList:
                 self.ColorBar.setTickValue(tick[0], self.ColorBar.tickValue(tick[0]) * self.ColorBar.tickValue(tick[0]))
 
-    def handle_delete_click(self):
+    def _handle_delete_click(self):
         # Delete colormap(s)
-        block = self.toRemoveDelete()
+        block = self._toRemoveDelete()
         if block is True:
             # This shouldn't happen
             QtWidgets.QMessageBox.information(self, "Error: Can not delete internal colormaps.")
             return
 
         selected_colormaps = self.cmap_list.selectedItems()
         to_print = ",".join([x.text() for x in selected_colormaps])
 
         delete_msg = "Please confirm you want to delete the colormap(s): " + to_print
         reply = QtWidgets.QMessageBox.question(
-            self, 'Message', delete_msg, QtWidgets.QMessageBox.Yes, QtWidgets.QMessageBox.No)
+            self, "Message", delete_msg, QtWidgets.QMessageBox.Yes, QtWidgets.QMessageBox.No
+        )
         if reply == QtWidgets.QMessageBox.Yes:
             for index in selected_colormaps:
                 del self.user_colormap_states[index.text()]
                 self.doc.remove_user_colormap(index.text())
-            self.updateListWidget()
+            self._updateListWidget()
 
-    def importButtonClick(self):
+    def _importButtonClick(self):
         # Import colormap
         fname = QtWidgets.QFileDialog.getOpenFileName(
-            self, 'Get Colormap File', os.path.expanduser('~'), "Colormaps (*.json)")[0]
+            self, "Get Colormap File", os.path.expanduser("~"), "Colormaps (*.json)"
+        )[0]
         self._import_single_file(fname)
 
     def _import_single_file(self, filename):
         try:
-            cmap_content = json.loads(open(filename, 'r').read())
+            cmap_content = json.loads(open(filename, "r").read())
             # FUTURE: Handle all types of colormaps, make sure they are copied to the settings directory
-            if isinstance(cmap_content, dict) and 'ticks' in cmap_content:
+            if isinstance(cmap_content, dict) and "ticks" in cmap_content:
                 # single colormap file
                 cmap_name = os.path.splitext(os.path.basename(filename))[0]
                 cmap_content = {cmap_name: cmap_content}
             elif isinstance(cmap_content, list) and isinstance(cmap_content[0], dict):
                 # list of individual colormap objects (not currently used
-                cmap_content = {cmap['name']: cmap for cmap in cmap_content}
+                cmap_content = {cmap["name"]: cmap for cmap in cmap_content}
             elif not isinstance(cmap_content, dict):
                 raise ValueError("Unknown colormap file format: {}".format(filename))
 
             for cmap_name in cmap_content:
                 if cmap_name in self.builtin_colormap_states:
                     QtWidgets.QMessageBox.information(
-                        self, "Error", "You cannot import a colormap with "
-                                       "the same name as one of the internal "
-                                       "colormaps: {}".format(cmap_name))
+                        self,
+                        "Error",
+                        "You cannot import a colormap with "
+                        "the same name as one of the internal "
+                        "colormaps: {}".format(cmap_name),
+                    )
                     return
 
             for cmap_name, cmap_info in cmap_content.items():
                 if cmap_name in self.user_colormap_states:
                     LOG.info("Overwriting colormap '{}'".format(cmap_name))
                 else:
                     LOG.info("Importing new colormap '{}'".format(cmap_name))
-                self.save_new_map(cmap_info, cmap_name)
+                self._save_new_map(cmap_info, cmap_name)
             self.user_colormap_states.update(cmap_content)
-            self.updateListWidget(cmap_name)
+            self._updateListWidget(cmap_name)
         except IOError:
-            LOG.error("Error importing colormap from file "
-                      "{}".format(filename), exc_info=True)
+            LOG.error("Error importing colormap from file " "{}".format(filename), exc_info=True)
 
-    def exportButtonClick(self):
+    def _exportButtonClick(self):
         # Export colormap(s)
         selected_colormaps = self.cmap_list.selectedItems()
-        fname = QtWidgets.QFileDialog.getSaveFileName(None, 'Save As', 'Export.json')[0]
+        fname = QtWidgets.QFileDialog.getSaveFileName(None, "Save As", "Export.json")[0]
         toExport = set()
         for index in selected_colormaps:
             toExport.add(index.text())
         done = {}
 
         for k in self.user_colormap_states:
             if k in toExport:
                 done[k] = self.user_colormap_states[k]
 
         for k in self.builtin_colormap_states:
             if k in toExport:
                 done[k] = self.builtin_colormap_states[k]
         try:
-            file = open(fname, 'w')
+            file = open(fname, "w")
             file.write(json.dumps(done, indent=2, sort_keys=True))
             file.close()
         except IOError:
-            LOG.error("Error exporting colormaps: {}".format(fname),
-                      exc_info=True)
+            LOG.error("Error exporting colormaps: {}".format(fname), exc_info=True)
 
 
 def main():
     app = QtWidgets.QApplication([])
     w = ColormapEditor()
     w.show()
     app.exec_()
     return 0
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     import sys
 
     sys.exit(main())
```

### Comparing `uwsift-1.2.3/uwsift/view/export_image.py` & `uwsift-2.0.0b0/uwsift/view/export_image.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,47 +1,48 @@
+import io
 import logging
 import os
-import io
-from PyQt5 import QtCore, QtGui, QtWidgets
 
 import imageio
-import numpy
 import matplotlib as mpl
+import numpy
 from matplotlib import pyplot as plt
 from PIL import Image, ImageDraw, ImageFont
+from PyQt5 import QtCore, QtGui, QtWidgets
 
 from uwsift.common import Info
+from uwsift.model.layer_model import LayerModel
 from uwsift.ui import export_image_dialog_ui
+from uwsift.util import USER_DESKTOP_DIRECTORY, get_package_data_dir
 from uwsift.view.colormap import COLORMAP_MANAGER
-from uwsift.util import get_package_data_dir, USER_DESKTOP_DIRECTORY
 
 LOG = logging.getLogger(__name__)
 DATA_DIR = get_package_data_dir()
 
 NUM_TICKS = 8
 TICK_SIZE = 14
-FONT = 'arial'
+FONT = "arial"
 
 
 def is_gif_filename(fn):
-    return os.path.splitext(fn)[-1] in ['.gif']
+    return os.path.splitext(fn)[-1] in [".gif"]
 
 
 def is_video_filename(fn):
-    return os.path.splitext(fn)[-1] in ['.mp4', '.m4v', '.gif']
+    return os.path.splitext(fn)[-1] in [".mp4", ".m4v", ".gif"]
 
 
 def get_imageio_format(fn):
     """Ask imageio if it knows what to do with this filename."""
-    request = imageio.core.Request(fn, 'w?')
+    request = imageio.core.Request(fn, "w?")
     return imageio.formats.search_write_format(request)
 
 
 class ExportImageDialog(QtWidgets.QDialog):
-    default_filename = 'sift_screenshot.png'
+    default_filename = "sift_screenshot.png"
 
     def __init__(self, parent):
         super(ExportImageDialog, self).__init__(parent)
 
         self.ui = export_image_dialog_ui.Ui_ExportImageDialog()
         self.ui.setupUi(self)
 
@@ -61,28 +62,27 @@
         self._last_dir = USER_DESKTOP_DIRECTORY
         self.ui.saveAsLineEdit.setText(os.path.join(self._last_dir, self.default_filename))
         self._validate_filename()
 
         self.ui.includeFooterCheckbox.clicked.connect(self._footer_changed)
         self._footer_changed()
 
-        self.ui.frameAllRadio.clicked.connect(self.change_frame_range)
-        self.ui.frameCurrentRadio.clicked.connect(self.change_frame_range)
-        self.ui.frameRangeRadio.clicked.connect(self.change_frame_range)
-        self.change_frame_range()  # set default
+        self.ui.frameAllRadio.clicked.connect(self._change_frame_range)
+        self.ui.frameCurrentRadio.clicked.connect(self._change_frame_range)
+        self.ui.frameRangeRadio.clicked.connect(self._change_frame_range)
+        self._change_frame_range()  # set default
 
     def set_total_frames(self, n):
         self.ui.frameRangeFrom.validator().setBottom(1)
         self.ui.frameRangeTo.validator().setBottom(2)
         self.ui.frameRangeFrom.validator().setTop(n - 1)
         self.ui.frameRangeTo.validator().setTop(n)
-        if (self.ui.frameRangeFrom.text() == '' or
-                int(self.ui.frameRangeFrom.text()) > n - 1):
-            self.ui.frameRangeFrom.setText('1')
-        if self.ui.frameRangeTo.text() in ['', '1']:
+        if self.ui.frameRangeFrom.text() == "" or int(self.ui.frameRangeFrom.text()) > n - 1:
+            self.ui.frameRangeFrom.setText("1")
+        if self.ui.frameRangeTo.text() in ["", "1"]:
             self.ui.frameRangeTo.setText(str(n))
 
     def _delay_clicked(self):
         if self.ui.constantDelayRadio.isChecked():
             self.ui.constantDelaySpin.setDisabled(False)
         else:
             self.ui.constantDelaySpin.setDisabled(True)
@@ -91,28 +91,30 @@
         if self.ui.includeFooterCheckbox.isChecked():
             self.ui.footerFontSizeSpinBox.setDisabled(False)
         else:
             self.ui.footerFontSizeSpinBox.setDisabled(True)
 
     def _show_file_dialog(self):
         fn = QtWidgets.QFileDialog.getSaveFileName(
-            self, caption=self.tr('Screenshot Filename'),
+            self,
+            caption=self.tr("Screenshot Filename"),
             directory=os.path.join(self._last_dir, self.default_filename),
-            filter=self.tr('Image Files (*.png *.jpg *.gif *.mp4 *.m4v)'),
-            options=QtWidgets.QFileDialog.DontConfirmOverwrite)[0]
+            filter=self.tr("Image Files (*.png *.jpg *.gif *.mp4 *.m4v)"),
+            options=QtWidgets.QFileDialog.DontConfirmOverwrite,
+        )[0]
         if fn:
             self.ui.saveAsLineEdit.setText(fn)
         # bring this dialog back in focus
         self.raise_()
         self.activateWindow()
 
     def _validate_filename(self):
         t = self.ui.saveAsLineEdit.text()
         bt = self.ui.buttonBox.button(QtWidgets.QDialogButtonBox.Save)
-        if not t or os.path.splitext(t)[-1] not in ['.png', '.jpg', '.gif', '.mp4', '.m4v']:
+        if not t or os.path.splitext(t)[-1] not in [".png", ".jpg", ".gif", ".mp4", ".m4v"]:
             bt.setDisabled(True)
         else:
             self._last_dir = os.path.dirname(t)
             bt.setDisabled(False)
         self._check_animation_controls()
 
     def _is_gif_filename(self):
@@ -137,34 +139,31 @@
         is_frame_current = self.ui.frameCurrentRadio.isChecked()
         is_valid_frame_to = self.ui.frameRangeTo.validator().top() == 1
         disable = is_frame_current or is_valid_frame_to
 
         self.ui.animationGroupBox.setDisabled(disable or not is_gif)
         self.ui.frameDelayGroup.setDisabled(disable or not (is_gif or is_video))
 
-    def change_frame_range(self):
+    def _change_frame_range(self):
         if self.ui.frameRangeRadio.isChecked():
             self.ui.frameRangeFrom.setDisabled(False)
             self.ui.frameRangeTo.setDisabled(False)
         else:
             self.ui.frameRangeFrom.setDisabled(True)
             self.ui.frameRangeTo.setDisabled(True)
 
         self._check_animation_controls()
 
-    def get_frame_range(self):
+    def _get_frame_range(self):
         if self.ui.frameCurrentRadio.isChecked():
             frame = None
         elif self.ui.frameAllRadio.isChecked():
             frame = [None, None]
         elif self.ui.frameRangeRadio.isChecked():
-            frame = [
-                int(self.ui.frameRangeFrom.text()),
-                int(self.ui.frameRangeTo.text())
-            ]
+            frame = [int(self.ui.frameRangeFrom.text()), int(self.ui.frameRangeTo.text())]
         else:
             LOG.error("Unknown frame range selection")
             return
         return frame
 
     def get_info(self):
         if self.ui.timeLapseRadio.isChecked():
@@ -173,51 +172,53 @@
             delay = self.ui.constantDelaySpin.value()
             fps = 1000 / delay
         elif self.ui.fpsDelayRadio.isChecked():
             fps = self.ui.fpsDelaySpin.value()
 
         # loop is actually an integer of number of times to loop (0 infinite)
         info = {
-            'frame_range': self.get_frame_range(),
-            'include_footer': self.ui.includeFooterCheckbox.isChecked(),
+            "frame_range": self._get_frame_range(),
+            "include_footer": self.ui.includeFooterCheckbox.isChecked(),
             # 'transparency': self.ui.transparentCheckbox.isChecked(),
-            'loop': self.ui.loopRadio.isChecked(),
-            'filename': self.ui.saveAsLineEdit.text(),
-            'fps': fps,
-            'font_size': self.ui.footerFontSizeSpinBox.value(),
-            'colorbar': self._get_append_direction(),
+            "loop": self.ui.loopRadio.isChecked(),
+            "filename": self.ui.saveAsLineEdit.text(),
+            "fps": fps,
+            "font_size": self.ui.footerFontSizeSpinBox.value(),
+            "colorbar": self._get_append_direction(),
         }
         return info
 
     def show(self):
         self._check_animation_controls()
         return super(ExportImageDialog, self).show()
 
 
 class ExportImageHelper(QtCore.QObject):
     """Handle all the logic for creating screenshot images"""
-    default_font = os.path.join(DATA_DIR, 'fonts', 'Andale Mono.ttf')
 
-    def __init__(self, parent, doc, sgm):
+    default_font = os.path.join(DATA_DIR, "fonts", "Andale Mono.ttf")
+
+    def __init__(self, parent, sgm, model: LayerModel):
         """Initialize helper with defaults and other object handles.
 
         Args:
             doc: Main ``Document`` object for frame metadata
             sgm: ``SceneGraphManager`` object to get image data
         """
         super(ExportImageHelper, self).__init__(parent)
-        self.doc = doc
         self.sgm = sgm
+        self.model = model
         self._screenshot_dialog = None
 
     def take_screenshot(self):
         if not self._screenshot_dialog:
             self._screenshot_dialog = ExportImageDialog(self.parent())
             self._screenshot_dialog.accepted.connect(self._save_screenshot)
-        self._screenshot_dialog.set_total_frames(max(self.sgm.layer_set.max_frame, 1))
+        frame_count = self.sgm.animation_controller.get_frame_count()
+        self._screenshot_dialog.set_total_frames(max(frame_count, 1))
         self._screenshot_dialog.show()
 
     def _add_screenshot_footer(self, im, banner_text, font_size=11):
         orig_w, orig_h = im.size
         font = ImageFont.truetype(self.default_font, font_size)
         banner_h = font_size
         new_im = Image.new(im.mode, (orig_w, orig_h + banner_h), "black")
@@ -228,60 +229,72 @@
         new_draw.text([1, orig_h], banner_text, fill="#ffffff", font=font)
         txt_w, txt_h = new_draw.textsize("SIFT", font)
         new_draw.text([orig_w - txt_w, orig_h], "SIFT", fill="#ffffff", font=font)
         new_im.paste(im, (0, 0, orig_w, orig_h))
         return new_im
 
     def _create_colorbar(self, mode, u, size):
-        mpl.rcParams['font.sans-serif'] = FONT
-        mpl.rcParams.update({'font.size': TICK_SIZE})
+        mpl.rcParams["font.sans-serif"] = FONT
+        mpl.rcParams.update({"font.size": TICK_SIZE})
+
+        colormap = self.model.get_dataset_presentation_by_uuid(u).colormap
+        colors = COLORMAP_MANAGER[colormap]
 
-        colors = COLORMAP_MANAGER[self.doc.colormap_for_uuid(u)]
-        if self.doc.prez_for_uuid(u).colormap == 'Square Root (Vis Default)':
+        if colormap == "Square Root (Vis Default)":
             colors = colors.map(numpy.linspace((0, 0, 0, 1), (1, 1, 1, 1), 256))
         else:
             colors = colors.colors.rgba
 
         dpi = self.sgm.main_canvas.dpi
-        if mode == 'vertical':
-            fig = plt.figure(figsize=(size[0] / dpi * .1, size[1] / dpi * 1.2), dpi=dpi)
+        if mode == "vertical":
+            fig = plt.figure(figsize=(size[0] / dpi * 0.1, size[1] / dpi * 1.2), dpi=dpi)
             ax = fig.add_axes([0.3, 0.05, 0.2, 0.9])
         else:
-            fig = plt.figure(figsize=(size[0] / dpi * 1.2, size[1] / dpi * .1), dpi=dpi)
+            fig = plt.figure(figsize=(size[0] / dpi * 1.2, size[1] / dpi * 0.1), dpi=dpi)
             ax = fig.add_axes([0.05, 0.4, 0.9, 0.2])
 
         cmap = mpl.colors.ListedColormap(colors)
-        vmin, vmax = self.doc.prez_for_uuid(u).climits
+        vmin, vmax = self.model.get_dataset_presentation_by_uuid(u).climits
         norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)
         cbar = mpl.colorbar.ColorbarBase(ax, cmap=cmap, norm=norm, orientation=mode)
 
-        ticks = [str(self.doc[u][Info.UNIT_CONVERSION][2](self.doc[u][Info.UNIT_CONVERSION][1](t)))
-                 for t in numpy.linspace(vmin, vmax, NUM_TICKS)]
+        ticks = [
+            str(
+                self.model.get_dataset_by_uuid(u).info.get(Info.UNIT_CONVERSION)[2](
+                    self.model.get_dataset_by_uuid(u).info.get(Info.UNIT_CONVERSION)[1](t)
+                )
+            )
+            for t in numpy.linspace(vmin, vmax, NUM_TICKS)
+        ]
         cbar.set_ticks(numpy.linspace(vmin, vmax, NUM_TICKS))
         cbar.set_ticklabels(ticks)
 
         return fig
 
     def _append_colorbar(self, mode, im, u):
-        if mode is None or COLORMAP_MANAGER.get(self.doc.colormap_for_uuid(u)) is None:
+        if (
+            mode is None
+            or not self.model.get_dataset_presentation_by_uuid(u)
+            or COLORMAP_MANAGER.get(self.model.get_dataset_presentation_by_uuid(u).colormap) is None
+        ):
             return im
 
         fig = self._create_colorbar(mode, u, im.size)
 
         buf = io.BytesIO()
-        fig.savefig(buf, format='png', bbox_inches='tight', dpi=self.sgm.main_canvas.dpi)
+        fig.savefig(buf, format="png", bbox_inches="tight", dpi=self.sgm.main_canvas.dpi)
         buf.seek(0)
         fig_im = Image.open(buf)
 
         fig_im.thumbnail(im.size)
         orig_w, orig_h = im.size
         fig_w, fig_h = fig_im.size
 
         offset = 0
-        if mode == 'vertical':
+        if mode == "vertical":
             new_im = Image.new(im.mode, (orig_w + fig_w, orig_h))
             for i in [im, fig_im]:
                 new_im.paste(i, (offset, 0))
                 offset += i.size[0]
             return new_im
         else:
             new_im = Image.new(im.mode, (orig_w, orig_h + fig_h))
@@ -293,17 +306,17 @@
     def _create_filenames(self, uuids, base_filename):
         if not uuids or uuids[0] is None:
             return [None], [base_filename]
         filenames = []
         # only use the first uuid to fill in filename information
         file_uuids = uuids[:1] if is_video_filename(base_filename) else uuids
         for u in file_uuids:
-            layer_info = self.doc[u]
+            dataset_info = self.model.get_dataset_by_uuid(u).info
             fn = base_filename.format(
-                start_time=layer_info[Info.SCHED_TIME],
+                start_time=dataset_info[Info.SCHED_TIME],
                 scene=Info.SCENE,
                 instrument=Info.INSTRUMENT,
             )
             filenames.append(fn)
 
         # check for duplicate filenames
         if len(filenames) > 1 and all(filenames[0] == fn for fn in filenames):
@@ -326,100 +339,111 @@
             # XXX: This could technically reach a recursion limit
             self.take_screenshot()
             return False
         return True
 
     def _get_animation_parameters(self, info, images):
         params = {}
-        if info['fps'] is None:
-            t = [self.doc[u][Info.SCHED_TIME] for u, im in images]
+        if info["fps"] is None:
+            t = [self.model.get_dataset_by_uuid(u).info.get(Info.SCHED_TIME) for u, im in images]
             t_diff = [max(1, (t[i] - t[i - 1]).total_seconds()) for i in range(1, len(t))]
             min_diff = float(min(t_diff))
             # imageio seems to be using duration in seconds
             # so use 1/10th of a second
-            duration = [.1 * (this_diff / min_diff) for this_diff in t_diff]
+            duration = [0.1 * (this_diff / min_diff) for this_diff in t_diff]
             duration = [duration[0]] + duration
-            if not info['loop']:
+            if not info["loop"]:
                 duration = duration + duration[-2:0:-1]
-            params['duration'] = duration
+            params["duration"] = duration
         else:
-            params['fps'] = info['fps']
+            params["fps"] = info["fps"]
 
-        if is_gif_filename(info['filename']):
-            params['loop'] = 0  # infinite number of loops
-        elif 'duration' in params:
+        if is_gif_filename(info["filename"]):
+            params["loop"] = 0  # infinite number of loops
+        elif "duration" in params:
             # not gif but were given "Time Lapse", can only have one FPS
-            params['fps'] = int(1. / params.pop('duration')[0])
+            params["fps"] = int(1.0 / params.pop("duration")[0])
 
         return params
 
     def _convert_frame_range(self, frame_range):
         """Convert 1-based frame range to SGM's 0-based"""
         if frame_range is None:
             return None
         s, e = frame_range
         # user provided frames are 1-based, scene graph are 0-based
         if s is None:
             s = 1
         if e is None:
-            e = max(self.sgm.layer_set.max_frame, 1)
+            e = max(self.sgm.animation_controller.get_frame_count(), 1)
         return s - 1, e - 1
 
     def _save_screenshot(self):
         info = self._screenshot_dialog.get_info()
         LOG.info("Exporting image with options: {}".format(info))
-        info['frame_range'] = self._convert_frame_range(info['frame_range'])
-        if info['frame_range']:
-            s, e = info['frame_range']
-            uuids = self.sgm.layer_set.frame_order[s: e + 1]
+        info["frame_range"] = self._convert_frame_range(info["frame_range"])
+        if info["frame_range"]:
+            s, e = info["frame_range"]
         else:
-            uuids = [self.sgm.layer_set.top_layer_uuid()]
-        uuids, filenames = self._create_filenames(uuids, info['filename'])
+            s = e = self.sgm.animation_controller.get_current_frame_index()
+
+        uuids = self.sgm.animation_controller.get_frame_uuids()[s : e + 1]
+
+        uuids, filenames = self._create_filenames(uuids, info["filename"])
 
         # check for existing filenames
-        if (any(os.path.isfile(fn) for fn in filenames) and
-                not self._overwrite_dialog()):
+        if any(os.path.isfile(fn) for fn in filenames) and not self._overwrite_dialog():
             return
 
         # get canvas screenshot arrays (numpy arrays of canvas pixels)
-        img_arrays = self.sgm.get_screenshot_array(info['frame_range'])
+        img_arrays = self.sgm.get_screenshot_array(info["frame_range"])
         if not img_arrays or len(uuids) != len(img_arrays):
-            LOG.error("Number of frames does not equal number of UUIDs")
+            LOG.error(
+                f"Number of frames: {0 if not img_arrays else len(img_arrays)}"
+                f" does not equal "
+                f"number of UUIDs: {len(uuids)}"
+            )
             return
 
         images = [(u, Image.fromarray(x)) for u, x in img_arrays]
 
-        if info['colorbar'] is not None:
-            images = [(u, self._append_colorbar(info['colorbar'], im, u)) for (u, im) in images]
+        if info["colorbar"] is not None:
+            images = [(u, self._append_colorbar(info["colorbar"], im, u)) for (u, im) in images]
 
-        if info['include_footer']:
-            banner_text = [self.doc[u][Info.DISPLAY_NAME] if u else "" for u, im in images]
-            images = [(u, self._add_screenshot_footer(im, bt, font_size=info['font_size'])) for (u, im), bt in
-                      zip(images, banner_text)]
+        if info["include_footer"]:
+            banner_text = [
+                self.model.get_dataset_by_uuid(u).info.get(Info.DISPLAY_NAME) if u else "" for u, im in images
+            ]
+            images = [
+                (u, self._add_screenshot_footer(im, bt, font_size=info["font_size"]))
+                for (u, im), bt in zip(images, banner_text)
+            ]
 
         imageio_format = get_imageio_format(filenames[0])
         if imageio_format:
             format_name = imageio_format.name
-        elif filenames[0].upper().endswith('.M4V'):
-            format_name = 'MP4'
+        elif filenames[0].upper().endswith(".M4V"):
+            format_name = "MP4"
         else:
             raise ValueError("Not sure how to handle file with format: {}".format(filenames[0]))
 
         if is_video_filename(filenames[0]) and len(images) > 1:
             params = self._get_animation_parameters(info, images)
-            if not info['loop'] and is_gif_filename(filenames[0]):
+            if not info["loop"] and is_gif_filename(filenames[0]):
                 # rocking animation
                 # we want frames 0, 1, 2, 3, 2, 1
                 # NOTE: this must be done *after* we get animation properties
                 images = images + images[-2:0:-1]
 
             filenames = [(filenames[0], images)]
         else:
             params = {}
             filenames = list(zip(filenames, [[x] for x in images]))
 
+        self._write_images(filenames, format_name, params)
+
+    def _write_images(self, filenames, format_name, params):
         for filename, file_images in filenames:
             writer = imageio.get_writer(filename, format_name, **params)
             for _, x in file_images:
                 writer.append_data(numpy.array(x))
-
         writer.close()
```

### Comparing `uwsift-1.2.3/uwsift/view/probes.py` & `uwsift-2.0.0b0/uwsift/view/probes.py`

 * *Files 25% similar despite different names*

```diff
@@ -5,203 +5,221 @@
 ~~~
 This module holds the code which deals with the controller and view for the area probe graphs.
 
 :author: Eva Schiffer <evas@ssec.wisc.edu>
 :copyright: 2015 by University of Wisconsin Regents, see AUTHORS for more details
 :license: GPLv3, see LICENSE for more details
 """
-__author__ = 'evas'
-__docformat__ = 'reStructuredText'
+__author__ = "evas"
+__docformat__ = "reStructuredText"
 
 import logging
-from PyQt5 import QtWidgets
 
 import numpy as np
-from PyQt5.QtCore import QObject, pyqtSignal
+
 # http://stackoverflow.com/questions/12459811/how-to-embed-matplotib-in-pyqt-for-dummies
 # see also: http://matplotlib.org/users/navigation_toolbar.html
 from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
 from matplotlib.backends.backend_qt5agg import NavigationToolbar2QT as NavigationToolbar
 from matplotlib.colors import LogNorm
 from matplotlib.figure import Figure
+from PyQt5 import QtWidgets
+from PyQt5.QtCore import QObject, Qt, pyqtSignal
 
 # a useful constant
-from uwsift.common import Info, Kind
-from uwsift.queue import TASK_PROGRESS, TASK_DOING
+from uwsift.common import Info
+from uwsift.model.layer_model import LayerModel
+from uwsift.queue import TASK_DOING, TASK_PROGRESS
 
 # Stuff for custom toolbars
 try:
-    import six
     import matplotlib.backends.qt_editor.figureoptions as figureoptions
 except ImportError:
     figureoptions = None
 
 LOG = logging.getLogger(__name__)
 DEFAULT_POINT_PROBE = "default_probe_name"
 
 
-class NavigationToolbar(NavigationToolbar):
+class CustomNavigationToolbar(NavigationToolbar):
     """Custom matplotlib toolbar."""
 
     def __init__(self, *args, **kwargs):
-        self.__include_colorbar = kwargs.get('include_colorbar', False)
-        super(NavigationToolbar, self).__init__(*args, **kwargs)
+        self.__include_colorbar = kwargs.get("include_colorbar", False)
+        super(CustomNavigationToolbar, self).__init__(*args, **kwargs)
 
     def edit_parameters(self):
         allaxes = self.canvas.figure.get_axes()
         if not allaxes:
-            QtWidgets.QMessageBox.warning(
-                self.parent, "Error", "There are no axes to edit.")
+            QtWidgets.QMessageBox.warning(self.parent, "Error", "There are no axes to edit.")
             return
         elif len(allaxes) == 1:
-            axes, = allaxes
+            (axes,) = allaxes
         else:
             titles = []
             not_colorbar_idx = -1
             for idx, axes in enumerate(allaxes):
                 if any(x.colorbar for x in axes.images):
                     not_colorbar_idx = idx
-                name = (axes.get_title() or
-                        " - ".join(filter(None, [axes.get_xlabel(),
-                                                 axes.get_ylabel()])) or
-                        "<anonymous {} (id: {:#x})>".format(
-                            type(axes).__name__, id(axes)))
+                name = (
+                    axes.get_title()
+                    or " - ".join(filter(None, [axes.get_xlabel(), axes.get_ylabel()]))
+                    or "<anonymous {} (id: {:#x})>".format(type(axes).__name__, id(axes))
+                )
                 titles.append(name)
 
             if len(titles) == 2 and not_colorbar_idx != -1 and not self.__include_colorbar:
                 axes = allaxes[not_colorbar_idx]
             else:
-                item, ok = QtWidgets.QInputDialog.getItem(
-                    self.parent, 'Customize', 'Select axes:', titles, 0, False)
+                item, ok = QtWidgets.QInputDialog.getItem(self.parent, "Customize", "Select axes:", titles, 0, False)
                 if ok:
-                    axes = allaxes[titles.index(six.text_type(item))]
+                    axes = allaxes[titles.index(str(item))]
                 else:
                     return
 
         figureoptions.figure_edit(axes, self)
 
 
 class ProbeGraphManager(QObject):
     """The ProbeGraphManager manages the many tabs of the Area Probe Graphs."""
 
     # signals
-    didChangeTab = pyqtSignal(tuple, )  # list of probe areas to show
+    didChangeTab = pyqtSignal(
+        tuple,
+    )  # list of probe areas to show
     didClonePolygon = pyqtSignal(str, str)
-    drawChildGraph = pyqtSignal(str, )
+    drawChildGraph = pyqtSignal(
+        str,
+    )
     pointProbeChanged = pyqtSignal(str, bool, tuple)
 
-    graphs = None
-    selected_graph_index = -1
-    workspace = None
-    queue = None
-    document = None
-    tab_widget_object = None
-    max_tab_letter = None
-
-    def __init__(self, tab_widget, workspace, document, queue):
-        """Setup our tab widget with an appropriate graph object in the first tab.
-
-        FUTURE, once we are saving our graph configurations, load those instead of setting up this default.
+    def __init__(self, tab_widget, auto_update_checkbox, update_button, workspace, layer_model: LayerModel, queue):
+        """Set up our tab widget with an appropriate graph object in the first
+        tab.
+
+        FUTURE, once we are saving our graph configurations, load those instead
+        of setting up this default.
+
+        :param auto_update_checkbox: the QCheckBox defined in the pov_main.ui
+               file. It's logic - to switch on/off automatic update of graphs
+               is managed here
+
+        :param update_button: the QButton defined in the pov_main.ui
+               file to trigger manual graph updates in case auto_update_checkbox
+               is off.
         """
 
         super(ProbeGraphManager, self).__init__(tab_widget)
 
-        # hang on to the workspace and document
+        # hang on to the workspace
         self.workspace = workspace
-        self.document = document
+        self.layer_model = layer_model
         self.queue = queue
 
         # hang on to the tab widget
         self.tab_widget_object = tab_widget
-        if self.tab_widget_object.count() != 1:
-            LOG.info("Unexpected number of tabs in the QTabWidget used for the Area Probe Graphs.")
+        self.new_tab_button = QtWidgets.QToolButton()
+        self.new_tab_button.setText("+")
+        self.tab_widget_object.setCornerWidget(self.new_tab_button, corner=Qt.TopLeftCorner)
+        self.tab_widget_object.clear()  # Delete all tabs that may have been created in the Designer
+        self.auto_update_checkbox = auto_update_checkbox
+        self.update_button = update_button
         # hold on to point probe locations (point probes are shared across tabs)
-        self.point_probes = {}
+        self.point_probes: dict = {}
 
         # set up the first tab
-        self.graphs = []
-        self.selected_graph_index = 0
-        self.max_tab_letter = 'A'
-        self.set_up_tab(self.selected_graph_index, do_increment_tab_letter=False)
+        self.graphs: list = []
+        self.selected_graph_index = -1
+        self.max_tab_letter = "A"
 
         # hook things up so we know when the selected tab changes
-        self.tab_widget_object.currentChanged[int].connect(self.handle_tab_change)
-        self.drawChildGraph.connect(self.draw_child)
+        self.tab_widget_object.currentChanged[int].connect(self._handle_tab_change)
+        self.drawChildGraph.connect(self._draw_child)
+
+        # hook up signals relating to changes in the number of tabs
+        self.new_tab_button.clicked.connect(self._add_tab)
 
-        # hook up the various document signals that would mean we need to reload things
-        self.document.didReorderLayers.connect(self.handleLayersChanged)
-        self.document.didChangeLayerName.connect(self.handleLayersChanged)
-        self.document.didAddBasicLayer.connect(self.handleLayersChanged)
-        self.document.didAddCompositeLayer.connect(self.handleLayersChanged)
-        self.document.willPurgeLayer.connect(self.handleLayersChanged)
-        self.document.didSwitchLayerSet.connect(self.handleLayersChanged)
+        # hook up auto update vs manual update changes
+        self.update_button.clicked.connect(self.handleActiveProductDatasetsChanged)
+        self.update_button.clicked.connect(self._update_default_point_probe_graph)
+        self.auto_update_checkbox.stateChanged.connect(self._on_auto_update_checkbox_state_changed)
+        self.auto_update_checkbox.setCheckState(Qt.Unchecked)
 
-    def draw_child(self, child_name):
+    def _draw_child(self, child_name):
         for child in self.graphs:
             if child.myName == child_name:
-                child.draw()
+                child._draw()
                 break
 
     def set_up_tab(self, tab_index, do_increment_tab_letter=True):
-        """Create a new tab at tab_index and add it to the list of graphs
-        """
+        """Create a new tab at tab_index and add it to the list of graphs"""
 
         # increment our tab label letter if desired
         if do_increment_tab_letter:
             self.max_tab_letter = chr(ord(self.max_tab_letter) + 1)  # this will get strange after Z!
 
         # create our tab
         temp_widget = QtWidgets.QWidget()
         self.tab_widget_object.insertTab(tab_index, temp_widget, self.max_tab_letter)
 
         # create the associated graph display object
-        graph = ProbeGraphDisplay(self, temp_widget, self.workspace, self.queue, self.document, self.max_tab_letter)
+        graph = ProbeGraphDisplay(self, temp_widget, self.workspace, self.layer_model, self.queue, self.max_tab_letter)
         self.graphs.append(graph)
 
         # load up the layers for this new tab
-        uuid_list = self.document.current_layer_uuid_order
-        graph.set_possible_layers(uuid_list)
+        graph.set_possible_layers()
 
         # clone the previous tab
         if self.selected_graph_index != tab_index:
             # if we aren't setting up the initial tab, clone the current tab
             current_graph = self.graphs[self.selected_graph_index]
-            graph.set_default_layer_selections(current_graph.xSelectedUUID, current_graph.ySelectedUUID)
-            # give it a copy of the current polygon
-            graph.setPolygon(current_graph.polygon[:] if current_graph.polygon is not None else None)
+            graph.set_default_layer_selections([current_graph.xSelectedUUID, current_graph.ySelectedUUID])
+            # give it a copy of the current full_data_selection or polygon
+            if current_graph.full_data_selection:
+                graph.setRegion(select_full_data=graph.full_data_selection)
+            else:
+                graph.setRegion(polygon_points=current_graph.polygon[:] if current_graph.polygon is not None else None)
             graph.checked = current_graph.checked
             point_status, point_xy = self.point_probes.get(DEFAULT_POINT_PROBE, (None, None))
             point_xy = point_xy if point_status else None
             graph.setPoint(point_xy, rebuild=False)
 
         # Create the initial plot
         graph.rebuildPlot()
 
         # go to the tab we just created
         self.tab_widget_object.setCurrentIndex(tab_index)
 
-    def handleLayersChanged(self):
-        """Used when the document signals that something about the layers has changed
+    def handleActiveProductDatasetsChanged(self):
+        """Used when the layer model signals that something about the layers
+        has changed
         """
 
         # reload the layer list for the existing graphs
-        uuid_list = self.document.current_layer_uuid_order
         for graphObj in self.graphs:
             doRebuild = graphObj is self.graphs[self.selected_graph_index]
-            graphObj.set_possible_layers(uuid_list, do_rebuild_plot=doRebuild)  # FIXME
+            graphObj.set_possible_layers(do_rebuild_plot=doRebuild)  # FIXME
 
-    def currentPolygonChanged(self, polygonPoints):
-        """Update the current polygon in the selected graph and rebuild it's plot
+    def current_graph_set_region(self, polygon_points=None, select_full_data=False):
+        """Update the current region in the selected graph and rebuild its plot
 
+        :return: Name of the current probe graph ('A', 'B', ...)
+
+        Probably outdated comment (TODO):
         FUTURE, once the polygon is a layer, this signal will be unnecessary
         """
 
-        return self.graphs[self.selected_graph_index].setPolygon(polygonPoints)
+        return self.graphs[self.selected_graph_index].setRegion(
+            polygon_points=polygon_points, select_full_data=select_full_data
+        )
+
+    def current_graph_has_polygon(self) -> bool:
+        return self.graphs[self.selected_graph_index].polygon is not None
 
-    def update_point_probe(self, probe_name, xy_pos=None, state=None):
+    def update_point_probe(self, probe_name=DEFAULT_POINT_PROBE, xy_pos=None, state=None):
         if xy_pos is None and state is None:
             if probe_name not in self.point_probes:
                 # nothing to do
                 return
             # they didn't ask to change anything
             # but they may want to refresh stuff
             state, xy_pos = self.point_probes[probe_name]
@@ -228,15 +246,20 @@
                 LOG.info("Changing point probe '{}' state to '{}'".format(probe_name, "on" if state else "off"))
             if old_xy_pos != xy_pos:
                 LOG.info("Changing point probe '{}' position to '{}'".format(probe_name, xy_pos))
 
         self.point_probes[probe_name] = [state, xy_pos]
         self.pointProbeChanged.emit(probe_name, state, xy_pos)
 
-    def update_point_probe_graph(self, probe_name, state, xy_pos):
+    def _update_default_point_probe_graph(self):
+        probe_name = DEFAULT_POINT_PROBE
+        point_probe = self.point_probes.get(probe_name, [None, None])
+        self._update_point_probe_graph(probe_name, *point_probe)
+
+    def _update_point_probe_graph(self, probe_name, state, xy_pos):
         # need to set the point for all graphs because the point probe
         # is used across all plots
         for idx, graph in enumerate(self.graphs):
             rebuild = idx == self.selected_graph_index
             if state:
                 graph.setPoint(xy_pos, rebuild=rebuild)
             elif state is not None:
@@ -253,108 +276,122 @@
             LOG.info("No point probe to toggle")
             return
 
         old_state = self.point_probes[probe_name][0]
         state = state if state is not None else not old_state
         self.update_point_probe(probe_name, state=state)
 
-    def set_default_layer_selections(self, *uuids):
-        """Set the UUIDs for the current graph if it doesn't have a polygon
-        """
-        return self.graphs[self.selected_graph_index].set_default_layer_selections(*uuids)
+    def set_default_layer_selections(self, layer_uuids):
+        """Set the UUIDs for the current graph if it doesn't have a polygon"""
+        return self.graphs[self.selected_graph_index].set_default_layer_selections(layer_uuids)
 
-    def handle_tab_change(self):
-        """deal with the fact that the tab changed in the tab widget
-        """
+    def on_region_probe_tool_selected(self):
+        if len(self.graphs) > 0:
+            return
+        # There is no graph tab yet, we must create one
+        self.set_up_tab(self.tab_widget_object.count(), do_increment_tab_letter=False)
 
-        newTabIndex = self.tab_widget_object.currentIndex()
+        current_name = self.graphs[self.selected_graph_index].getName()
+        self.didChangeTab.emit((current_name,))
 
-        # if this is the last tab, make a new tab and switch to that
-        if newTabIndex == (self.tab_widget_object.count() - 1):
-            LOG.info("Creating new area probe graph tab.")
+    def _add_tab(self):
+        LOG.info("Creating new area probe graph tab.")
 
-            old_name = self.graphs[self.selected_graph_index].getName()
-            self.set_up_tab(newTabIndex)
+        old_name = self.graphs[self.selected_graph_index].getName()
+        self.set_up_tab(self.tab_widget_object.count())
 
-            # notify everyone that we cloned a polygon (if we did)
-            if self.graphs[self.selected_graph_index].polygon is not None:
-                new_name = self.graphs[-1].getName()
-                self.didClonePolygon.emit(old_name, new_name)
+        # notify everyone that we cloned a polygon (if we did)
+        if self.graphs[self.selected_graph_index].polygon is not None:
+            new_name = self.graphs[-1].getName()
+            self.didClonePolygon.emit(old_name, new_name)
 
-        # otherwise, just update our current index and make sure the graph is fresh
-        else:
-            self.selected_graph_index = newTabIndex
-            self.graphs[self.selected_graph_index].rebuildPlot()
+        current_name = self.graphs[self.selected_graph_index].getName()
+        self.didChangeTab.emit((current_name,))
 
-        currentName = self.graphs[self.selected_graph_index].getName()
-        self.didChangeTab.emit((currentName,))
+    def _handle_tab_change(self):
+        """Deal with the fact that the tab changed in the tab widget"""
+
+        new_tab_index = self.tab_widget_object.currentIndex()
+
+        self.selected_graph_index = new_tab_index
+        self.graphs[self.selected_graph_index].rebuildPlot()
+
+        current_name = self.graphs[self.selected_graph_index].getName()
+        self.didChangeTab.emit((current_name,))
+
+    def _on_auto_update_checkbox_state_changed(self, state):
+        if self.auto_update_checkbox.isChecked():
+            self.update_button.setEnabled(False)
+            self.layer_model.didFinishActivateProductDatasets.connect(self.handleActiveProductDatasetsChanged)
+            self.pointProbeChanged.connect(self._update_point_probe_graph)
+        else:
+            self.layer_model.didFinishActivateProductDatasets.disconnect(self.handleActiveProductDatasetsChanged)
+            self.pointProbeChanged.disconnect(self._update_point_probe_graph)
+            self.update_button.setEnabled(True)
 
 
 class ProbeGraphDisplay(object):
     """The ProbeGraphDisplay controls one tab of the Area Probe Graphs.
     The ProbeGraphDisplay handles generating a displaying a single graph.
     """
 
     # the most data we are willing to plot in a scatter plot
     # this limit was determined experimentally on Eva's laptop for glance, may need to revisit this
     MAX_SCATTER_PLOT_DATA = 1e7
 
     # the default number of bins for the histogram and density scatter plot
     DEFAULT_NUM_BINS = 100
 
-    # the display name of the probe, should be unique across all probes
-    myName = None
-
-    # plotting related controls
-    figure = None
-    canvas = None
-    toolbar = None
-    yCheckBox = None
-    xDropDown = None
-    yDropDown = None
-
-    # internal objects to reference for info and data
-    polygon = None
-    point = None
-    manager = None
-    workspace = None
-    queue = None
-    document = None
-
-    # internal values that control the behavior of plotting and controls
-    xSelectedUUID = None
-    ySelectedUUID = None
-    uuidMap = None  # this is needed because the drop downs can't properly handle objects as ids
-    _stale = True  # whether or not the plot needs to be redrawn
-
-    def __init__(self, manager, qt_parent, workspace, queue, document, name_str):
+    def __init__(self, manager, qt_parent, workspace, layer_model: LayerModel, queue, name_str):
         """build the graph tab controls
+        :param layer_model:
         :return:
         """
 
         # hang on to our name
         self.myName = name_str
 
+        # plotting related controls
+        self.toolbar = None
+        self.yCheckBox = None
+        self.xDropDown = None
+        self.yDropDown = None
+
+        # internal objects to reference for info and data
+        self.polygon = None
+        self.point = None
+        self.full_data_selection = False
+
         # save the workspace and queue for use later
         self.manager = manager
         self.workspace = workspace
+        self.layer_model = layer_model
         self.queue = queue
-        self.document = document
+
+        # internal values that control the behavior of plotting and controls
+        self.xSelectedUUID = None
+        self.ySelectedUUID = None
+
+        self.xCurrentDatasetUUID = None
+        self.yCurrentDatasetUUID = None
+
+        self.uuidMap = None  # this is needed because the drop downs can't properly handle objects as ids
+        self._stale = True  # whether or not the plot needs to be redrawn
 
         # a figure instance to plot on
         self.figure = Figure(figsize=(3, 3), dpi=72)
         # this is the Canvas Widget that displays the `figure`
         # it takes the `figure` instance as a parameter to __init__
         self.canvas = FigureCanvas(self.figure)
         self.canvas.setMinimumSize(100, 100)
         # make sure our figure is clear
         self.clearPlot()
 
         # make a matplotlib toolbar to attach to the graph
-        self.toolbar = NavigationToolbar(self.canvas, qt_parent)
+        self.toolbar = CustomNavigationToolbar(self.canvas, qt_parent)
 
         # create our selection controls
 
         # the label for the x selection
         xLabel = QtWidgets.QLabel("X layer:")
 
         # the check box that turns on and off comparing to a y layer
@@ -380,73 +417,95 @@
         layout.addWidget(self.canvas, 2, 1, 1, 3)
         layout.addWidget(xLabel, 3, 1)
         layout.addWidget(self.xDropDown, 3, 2, 1, 2)
         layout.addWidget(self.yCheckBox, 4, 1)
         layout.addWidget(self.yDropDown, 4, 2, 1, 2)
         qt_parent.setLayout(layout)
 
-    def set_possible_layers(self, uuid_list, do_rebuild_plot=False):
-        """Given a list of layer UUIDs, set the names and UUIDs in the drop downs
-        """
+    def set_possible_layers(self, do_rebuild_plot=False):
+        """Given a list of layer UUIDs, set the names and UUIDs in the drop downs"""
 
         # make a uuid map because the mapping in a combo box doesn't work with objects
         self.uuidMap = {}
 
         # clear out the current lists
         self.xDropDown.clear()
         self.yDropDown.clear()
 
         # fill up our lists of layers
-        for uuid in uuid_list:
-            layer = self.document[uuid]
-            layer_name = layer.get(Info.DISPLAY_NAME, "??unknown layer??")
-            if layer.get(Info.KIND, None) == Kind.RGB:  # skip RGB layers
-                continue
-            uuid_string = str(uuid)
-            self.xDropDown.addItem(layer_name, uuid_string)
-            self.yDropDown.addItem(layer_name, uuid_string)
+        for layer in self.layer_model.get_probeable_layers():
+            uuid_string = str(layer.uuid)
+            self.xDropDown.addItem(layer.descriptor, uuid_string)
+            self.yDropDown.addItem(layer.descriptor, uuid_string)
 
-            self.uuidMap[uuid_string] = uuid
+            self.uuidMap[uuid_string] = layer.uuid
 
         # if possible, set the selections back to the way they were
         need_rebuild = False
-        xIndex = self.xDropDown.findData(str(self.xSelectedUUID))
-        if xIndex >= 0:
+        x_index = self.xDropDown.findData(str(self.xSelectedUUID))
+        if x_index >= 0:
             # Selection didn't change
-            self.xDropDown.setCurrentIndex(xIndex)
+            self.xDropDown.setCurrentIndex(x_index)
         elif self.xDropDown.count() > 0:
             # Setting to a new layer
             need_rebuild = True
             self.xSelectedUUID = self.uuidMap[self.xDropDown.itemData(0)]
             self.xDropDown.setCurrentIndex(0)
         else:
             # we had something selected but now there is nothing new to select
             need_rebuild = need_rebuild or self.xSelectedUUID is not None
             self.xSelectedUUID = None
-        yIndex = self.yDropDown.findData(str(self.ySelectedUUID))
-        if yIndex >= 0:
+        y_index = self.yDropDown.findData(str(self.ySelectedUUID))
+        if y_index >= 0:
             # Selection didn't change
-            self.yDropDown.setCurrentIndex(yIndex)
+            self.yDropDown.setCurrentIndex(y_index)
         elif self.yDropDown.count() > 0:
             # Setting to a new layer
             need_rebuild = need_rebuild or self.yCheckBox.isChecked()
             self.ySelectedUUID = self.uuidMap[self.yDropDown.itemData(0)]
             self.yDropDown.setCurrentIndex(0)
         else:
             # we had something selected but now there is nothing new to select
             need_rebuild = need_rebuild or self.ySelectedUUID is not None
             self.ySelectedUUID = None
 
+        need_rebuild |= self._check_active_datasets_changed()
+
         # refresh the plot
         self._stale = need_rebuild
         if do_rebuild_plot:
-            # Rebuild the plot (stale is used to determine if actual rebuild is needed)
+            # Rebuild the plot (stale is used to determine if actual rebuild
+            # is needed)
             self.rebuildPlot()
 
-    def set_default_layer_selections(self, *layer_uuids):
+    def _check_active_datasets_changed(self):
+        # check whether active datasets have changed. If so, update stored
+        # dataset uuids and indicate that the graph needs to be rebuilt.
+        need_rebuild = False
+        x_layer = self.layer_model.get_layer_by_uuid(self.xSelectedUUID)
+        x_active_product_dataset = None if not x_layer else x_layer.get_first_active_product_dataset()
+        if not x_active_product_dataset:
+            if self.xCurrentDatasetUUID is not None:
+                need_rebuild = True
+                self.xCurrentDatasetUUID = None
+        elif x_active_product_dataset.uuid != self.xCurrentDatasetUUID:
+            need_rebuild = True
+            self.xCurrentDatasetUUID = x_active_product_dataset.uuid
+        y_layer = self.layer_model.get_layer_by_uuid(self.ySelectedUUID)
+        y_active_product_dataset = None if not y_layer else y_layer.get_first_active_product_dataset()
+        if not y_active_product_dataset:
+            if self.yCurrentDatasetUUID is not None:
+                need_rebuild |= self.yCheckBox.isChecked()
+                self.yCurrentDatasetUUID = None
+        elif y_active_product_dataset.uuid != self.yCurrentDatasetUUID:
+            need_rebuild |= self.yCheckBox.isChecked()
+            self.yCurrentDatasetUUID = y_active_product_dataset.uuid
+        return need_rebuild
+
+    def set_default_layer_selections(self, layer_uuids):
         # only set the defaults if we don't have a polygon yet
         if self.polygon is not None:
             return
 
         if len(layer_uuids) >= 1:
             xIndex = self.xDropDown.findData(str(layer_uuids[0]))
             if xIndex >= 0:
@@ -468,16 +527,15 @@
         return self.yCheckBox.isChecked()
 
     @checked.setter
     def checked(self, is_checked):
         return self.yCheckBox.setChecked(is_checked)
 
     def xSelected(self):
-        """The user selected something in the X layer list.
-        """
+        """The user selected something in the X layer list."""
         oldXStr = str(self.xSelectedUUID)
         newXStr = self.xDropDown.itemData(self.xDropDown.currentIndex())
         self.xSelectedUUID = self.uuidMap[newXStr]
 
         # regenerate the plot
         if oldXStr != newXStr:
             self._stale = True
@@ -500,18 +558,28 @@
         doPlotVS = self.yCheckBox.isChecked()
         self.yDropDown.setDisabled(not doPlotVS)
 
         # regenerate the plot
         self._stale = True
         self.rebuildPlot()
 
-    def setPolygon(self, polygonPoints):
-        """Set the polygon selection for this graph."""
+    def setRegion(self, polygon_points=None, select_full_data=False):
+        """Set the region for this graph as polygon selection or full data."""
 
-        self.polygon = polygonPoints
+        assert polygon_points is None or not select_full_data, (  # nosec B101
+            "Must not give both 'polygon_points' and True for 'select_full_data':"
+            " Defining region by polygon and as full data are mutually exclusive."
+        )
+
+        # Even with assertions switched off we will get a valid state here: a
+        # polygonal region wins over the full data selection: the first one will
+        # have a visual echo, the second one not, so this is more likely to give
+        # a consistent state.
+        self.polygon = polygon_points
+        self.full_data_selection = False if self.polygon is not None else select_full_data
 
         # regenerate the plot
         self._stale = True
         self.rebuildPlot()
 
         # return our name to be used for the polygon name
         return self.myName
@@ -532,143 +600,184 @@
 
         Note: This should be called only when the selections change in some way.
         """
         if not self._stale:
             LOG.debug("Plot doesn't need to be rebuilt")
             return
 
-        # should be be plotting vs Y?
+        # should be plotting vs Y?
         doPlotVS = self.yCheckBox.isChecked()
         task_name = "%s_%s_region_plotting" % (self.xSelectedUUID, self.ySelectedUUID)
-        self.queue.add(task_name,
-                       self._rebuild_plot_task(self.xSelectedUUID, self.ySelectedUUID, self.polygon, self.point,
-                                               plot_versus=doPlotVS), "Creating plot for region probe data",
-                       interactive=True)
+        task_description = (
+            "Creating plot for full data" if self.full_data_selection else "Creating plot for region probe data"
+        )
+        self.queue.add(
+            task_name,
+            self._rebuild_plot_task(
+                self.xSelectedUUID,
+                self.ySelectedUUID,
+                self.polygon,
+                self.point,
+                plot_versus=doPlotVS,
+                plot_full_data=self.full_data_selection,
+            ),
+            task_description,
+            interactive=True,
+        )
         # Assume that the task gets resolved otherwise we might try to draw multiple times
         self._stale = False
 
-    def _rebuild_plot_task(self, x_uuid, y_uuid, polygon, point_xy, plot_versus=False):
+    def _rebuild_plot_task(  # noqa: C901
+        self, x_layer_uuid, y_layer_uuid, polygon, point_xy, plot_versus=False, plot_full_data=True
+    ):
+        data_source_description = "full data" if plot_full_data else "polygon data"
+
+        x_layer = self.layer_model.get_layer_by_uuid(x_layer_uuid)
+        x_active_product_dataset = None if not x_layer else x_layer.get_first_active_product_dataset()
+        x_uuid = None if not x_active_product_dataset else x_active_product_dataset.uuid
+
+        y_layer = self.layer_model.get_layer_by_uuid(y_layer_uuid)
+        y_active_product_dataset = None if not y_layer else y_layer.get_first_active_product_dataset()
+        y_uuid = None if not y_active_product_dataset else y_active_product_dataset.uuid
 
         # if we are plotting only x and we have a selected x and a polygon
-        if not plot_versus and x_uuid is not None and polygon is not None:
-            yield {TASK_DOING: 'Probe Plot: Collecting polygon data...', TASK_PROGRESS: 0.0}
+        have_x_layer = x_layer_uuid is not None
+        have_y_layer = y_layer_uuid is not None
+        should_plot = polygon is not None or plot_full_data
+        if not plot_versus and have_x_layer and should_plot:
+            yield {TASK_DOING: f"Probe Plot: Collecting {data_source_description}...", TASK_PROGRESS: 0.0}
 
             # get the data and info we need for this plot
-            data_polygon = self.workspace.get_content_polygon(x_uuid, polygon)
-            unit_info = self.document[x_uuid][Info.UNIT_CONVERSION]
-            data_polygon = unit_info[1](data_polygon)
-            title = self.document[x_uuid][Info.DISPLAY_NAME]
+            if x_active_product_dataset:
+                if plot_full_data:
+                    data_polygon = self.workspace.get_content(x_active_product_dataset.uuid)
+                else:
+                    data_polygon = self.workspace.get_content_polygon(x_active_product_dataset.uuid, polygon)
+            else:
+                data_polygon = np.array([])
+
+            x_conv_func = x_layer.info[Info.UNIT_CONVERSION][1]
+            data_polygon = x_conv_func(data_polygon)
+            time = x_active_product_dataset.info[Info.DISPLAY_TIME]
+            title = f"{time}"
+            x_axis_label = x_layer.descriptor
+            y_axis_label = "Count of data points"
 
             # get point probe value
-            if point_xy:
-                x_point = self.workspace.get_content_point(x_uuid, point_xy)
-                x_point = unit_info[1](x_point)
+            if x_active_product_dataset and point_xy:
+                x_point = self.workspace.get_content_point(x_active_product_dataset.uuid, point_xy)
+                x_point = x_conv_func(x_point)
             else:
                 x_point = None
 
             # plot a histogram
-            yield {TASK_DOING: 'Probe Plot: Creating histogram plot', TASK_PROGRESS: 0.25}
-            self.plotHistogram(data_polygon, title, x_point)
+            yield {TASK_DOING: "Probe Plot: Creating histogram plot", TASK_PROGRESS: 0.25}
+            self.plotHistogram(data_polygon, title, x_point, x_axis_label, y_axis_label)
 
         # if we are plotting x vs y and have x, y, and a polygon
-        elif plot_versus and x_uuid is not None and y_uuid is not None and polygon is not None:
-            yield {TASK_DOING: 'Probe Plot: Collecting polygon data (layer 1)...', TASK_PROGRESS: 0.0}
+        elif plot_versus and have_x_layer and have_y_layer and should_plot:
+            yield {TASK_DOING: f"Probe Plot: Collecting {data_source_description} (layer 1)...", TASK_PROGRESS: 0.0}
 
-            # get the data and info we need for this plot
-            x_info = self.document[x_uuid]
-            y_info = self.document[y_uuid]
-            name1 = x_info[Info.DISPLAY_NAME]
-            name2 = y_info[Info.DISPLAY_NAME]
-            hires_uuid = self.workspace.lowest_resolution_uuid(x_uuid, y_uuid)
-            # hires_coord_mask are the lat/lon coordinates of each of the
-            # pixels in hires_data. The coordinates are (lat, lon) to resemble
-            # the (Y, X) indexing of numpy arrays
-            hires_coord_mask, hires_data = self.workspace.get_coordinate_mask_polygon(hires_uuid, polygon)
-            hires_conv_func = self.document[hires_uuid][Info.UNIT_CONVERSION][1]
-            x_conv_func = x_info[Info.UNIT_CONVERSION][1]
-            y_conv_func = y_info[Info.UNIT_CONVERSION][1]
-            hires_data = hires_conv_func(hires_data)
-            yield {TASK_DOING: 'Probe Plot: Collecting polygon data (layer 2)...', TASK_PROGRESS: 0.15}
-            if hires_uuid == x_uuid:
-                # the hires data was from the X UUID
-                data1 = hires_data
-                data2 = self.workspace.get_content_coordinate_mask(y_uuid, hires_coord_mask)
-                data2 = y_conv_func(data2)
-            else:
-                # the hires data was from the Y UUID
-                data2 = hires_data
-                data1 = self.workspace.get_content_coordinate_mask(x_uuid, hires_coord_mask)
-                data1 = x_conv_func(data1)
-            yield {TASK_DOING: 'Probe Plot: Creating scatter plot...', TASK_PROGRESS: 0.25}
-
-            if point_xy:
-                x_point = self.workspace.get_content_point(x_uuid, point_xy)
-                x_point = x_conv_func(x_point)
-                y_point = self.workspace.get_content_point(y_uuid, point_xy)
-                y_point = y_conv_func(y_point)
-            else:
+            name1 = x_layer.descriptor
+            name2 = y_layer.descriptor
+            if not x_active_product_dataset or not y_active_product_dataset:
                 x_point = None
                 y_point = None
+                time1 = None
+                time2 = None
+                data1 = np.array([0])
+                data2 = np.array([0])
+            else:
+                # get the data and info we need for this plot
+                x_info = x_active_product_dataset.info
+                y_info = y_active_product_dataset.info
+                time1 = x_info[Info.DISPLAY_TIME]
+                time2 = y_info[Info.DISPLAY_TIME]
+                hires_uuid = self.workspace.lowest_resolution_uuid(x_uuid, y_uuid)
+                # hires_coord_mask are the lat/lon coordinates of each of the
+                # pixels in hires_data. The coordinates are (lat, lon) to resemble
+                # the (Y, X) indexing of numpy arrays
+                if plot_full_data:
+                    hires_coord_mask = None
+                    hires_data = self.workspace.get_content(hires_uuid)
+                else:
+                    hires_coord_mask, hires_data = self.workspace.get_coordinate_mask_polygon(hires_uuid, polygon)
 
-            # plot a scatter plot
-            good_mask = ~(np.isnan(data1) | np.isnan(data2))
-            data1 = data1[good_mask]
-            data2 = data2[good_mask]
-            self.plotDensityScatterplot(data1, name1, data2, name2, x_point, y_point)
+                x_conv_func = x_layer.info[Info.UNIT_CONVERSION][1]
+                y_conv_func = y_layer.info[Info.UNIT_CONVERSION][1]
+                yield {
+                    TASK_DOING: f"Probe Plot: Collecting {data_source_description} (layer 2)...",
+                    TASK_PROGRESS: 0.15,
+                }
+                if hires_uuid == x_uuid:
+                    # the hires data was from the X UUID
+                    data1 = x_conv_func(hires_data)
+                    if plot_full_data:
+                        data2 = self.workspace.get_content(y_uuid)
+                    else:
+                        data2 = self.workspace.get_content_coordinate_mask(y_uuid, hires_coord_mask)
+                    data2 = y_conv_func(data2)
+                else:
+                    # the hires data was from the Y UUID
+                    data2 = y_conv_func(hires_data)
+                    if plot_full_data:
+                        data1 = self.workspace.get_content(x_uuid)
+                    else:
+                        data1 = self.workspace.get_content_coordinate_mask(x_uuid, hires_coord_mask)
+                    data1 = x_conv_func(data1)
+                yield {TASK_DOING: "Probe Plot: Creating scatter plot...", TASK_PROGRESS: 0.25}
+
+                if point_xy:
+                    x_point = self.workspace.get_content_point(x_uuid, point_xy)
+                    x_point = x_conv_func(x_point)
+                    y_point = self.workspace.get_content_point(y_uuid, point_xy)
+                    y_point = y_conv_func(y_point)
+                else:
+                    x_point = None
+                    y_point = None
+
+                # plot a scatter plot
+                good_mask = ~(np.isnan(data1) | np.isnan(data2))
+                data1 = data1[good_mask]
+                data2 = data2[good_mask]
+
+            self.plotDensityScatterplot(data1, name1, time1, data2, name2, time2, x_point, y_point)
 
         # if we have some combination of selections we don't understand, clear the figure
         else:
-            yield {TASK_DOING: 'Probe Plot: Clearing plot figure...', TASK_PROGRESS: 0.0}
+            yield {TASK_DOING: "Probe Plot: Clearing plot figure...", TASK_PROGRESS: 0.0}
             self.clearPlot()
 
-        yield {TASK_DOING: 'Probe Plot: Drawing plot...', TASK_PROGRESS: 0.95}
+        yield {TASK_DOING: "Probe Plot: Drawing plot...", TASK_PROGRESS: 0.95}
         self.manager.drawChildGraph.emit(self.myName)
-        yield {TASK_DOING: 'Probe Plot: Done', TASK_PROGRESS: 1.0}
+        yield {TASK_DOING: "Probe Plot: Done", TASK_PROGRESS: 1.0}
 
-    def draw(self):
+    def _draw(self):
         self.canvas.draw()
 
-    def plotHistogram(self, data, title, x_point, numBins=100):
-        """Make a histogram using the given data and label it with the given title
-        """
+    def plotHistogram(self, data, title, x_point, x_label, y_label, numBins=100):
+        """Make a histogram using the given data and label it with the given title"""
         self.figure.clf()
         axes = self.figure.add_subplot(111)
         bars = axes.hist(data[~np.isnan(data)], bins=self.DEFAULT_NUM_BINS)
         if x_point is not None:
             # go through each rectangle object and make the one that contains x_point 'red'
             # default color is blue so red should stand out
             for bar in bars[2][::-1]:
                 if bar.xy[0] <= x_point:
-                    bar.set_color('red')
+                    bar.set_color("red")
                     break
         axes.set_title(title)
+        axes.set_xlabel(x_label)
+        axes.set_ylabel(y_label)
 
-    def plotScatterplot(self, dataX, nameX, dataY, nameY):
-        """Make a scatter plot of the x and y data
-        """
-
-        # we should have the same size data here
-        assert (dataX.size == dataY.size)
-
-        if dataX.size > self.MAX_SCATTER_PLOT_DATA:
-            LOG.info("Too much data in selected region to generate scatter plot.")
-            self.clearPlot()
-            # self.plotDensityScatterplot(dataX, nameX, dataY, nameY)
-
-        else:
-            self.figure.clf()
-            axes = self.figure.add_subplot(111)
-            axes.scatter(dataX, dataY, color='b', s=1, alpha=0.5)
-            axes.set_xlabel(nameX)
-            axes.set_ylabel(nameY)
-            axes.set_title(nameX + " vs " + nameY)
-            self._draw_xy_line(axes)
-
-    def plotDensityScatterplot(self, dataX, nameX, dataY, nameY, pointX, pointY):
+    def plotDensityScatterplot(self, dataX, nameX, timeX, dataY, nameY, timeY, pointX, pointY):
         """Make a density scatter plot for the given data
+        :param timeX:
+        :param timeY:
         """
 
         # clear the figure and make a new subplot
         self.figure.clf()
         axes = self.figure.add_subplot(111)
 
         # figure out the range of the data
@@ -682,46 +791,55 @@
 
         # make the binned density map for this data set
         density_map, _, _ = np.histogram2d(dataX, dataY, bins=self.DEFAULT_NUM_BINS, range=bounds)
         # mask out zero counts; flip because y goes the opposite direction in an imshow graph
         density_map = np.flipud(np.transpose(np.ma.masked_array(density_map, mask=density_map == 0)))
 
         # display the density map data
-        img = axes.imshow(density_map, extent=[xmin_value, xmax_value, ymin_value, ymax_value], aspect='auto',
-                          interpolation='nearest', norm=LogNorm())
+        img = axes.imshow(
+            density_map,
+            extent=[xmin_value, xmax_value, ymin_value, ymax_value],
+            aspect="auto",
+            interpolation="nearest",
+            norm=LogNorm(),
+        )
         if pointX is not None:
             axes.set_autoscale_on(False)
-            axes.plot(pointX, pointY, marker='o',
-                      markerfacecolor='white', markeredgecolor='black',
-                      markersize=10, markeredgewidth=1.)
+            axes.plot(
+                pointX,
+                pointY,
+                marker="o",
+                markerfacecolor="white",
+                markeredgecolor="black",
+                markersize=10,
+                markeredgewidth=1.0,
+            )
             axes.set_autoscale_on(True)
 
         colorbar = self.figure.colorbar(img)
-        colorbar.set_label('log(count of data points)')
+        colorbar.set_label("log(count of data points)")
 
         # set the various text labels
-        axes.set_xlabel(nameX)
-        axes.set_ylabel(nameY)
-        axes.set_title(nameX + " vs " + nameY)
+        axes.set_xlabel(f"{nameX}")
+        axes.set_ylabel(f"{nameY}")
+        axes.set_title(timeX)
 
         # draw the x vs y line
         self._draw_xy_line(axes)
 
     def clearPlot(self):
-        """Clear our plot
-        """
-
+        """Clear our plot"""
+        self.full_data_selection = False
         self.figure.clf()
 
     def _draw_xy_line(self, axes):
-
         # get the bounds for our calculations and so we can reset the viewing window later
         x_bounds = axes.get_xbound()
         y_bounds = axes.get_ybound()
 
         # draw the x=y line
         perfect = [max(x_bounds[0], y_bounds[0]), min(x_bounds[1], y_bounds[1])]
-        axes.plot(perfect, perfect, '--', color='k', label='X = Y')
+        axes.plot(perfect, perfect, "--", color="k", label="X = Y")
 
         # reset the bounds
         axes.set_xbound(x_bounds)
         axes.set_ybound(y_bounds)
```

### Comparing `uwsift-1.2.3/uwsift/view/rgb_config.py` & `uwsift-2.0.0b0/uwsift/view/rgb_config.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,48 +1,53 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
 """UI objects for configuring RGB layers."""
-
 import logging
+import uuid
 from functools import partial
-from typing import List, Tuple, Optional, Mapping
+from typing import Optional, Tuple
 
 from PyQt5.QtCore import QObject, pyqtSignal
 from PyQt5.QtGui import QDoubleValidator
 from PyQt5.QtWidgets import QComboBox, QLineEdit
 
-from uwsift.common import Info, Kind
+from uwsift.common import DEFAULT_GAMMA_VALUE, Info, Kind
+from uwsift.model.composite_recipes import RGBA2IDX, CompositeRecipe
+from uwsift.model.layer_item import LayerItem
+from uwsift.model.layer_model import LayerModel
 
 LOG = logging.getLogger(__name__)
-RGBA2IDX: Mapping[str, int] = dict(r=0, g=1, b=2, a=3)
 
 
 class RGBLayerConfigPane(QObject):
     """Configures RGB channel selection and ranges on behalf of document.
     Document in turn generates update signals which cause the SceneGraph to refresh.
     """
-    # recipe being changed, character from 'rgba', layer being assigned
-    didChangeRGBComponentSelection = pyqtSignal(tuple, str, object)
-    # recipe being changed, ((min, max), (min, max), (min, max))
-    didChangeRGBComponentLimits = pyqtSignal(tuple, tuple)
-    # recipe being changed, (new-gamma, new-gamma, new-gamma)
-    didChangeRGBComponentGamma = pyqtSignal(tuple, tuple)
+
+    # Recipe, Channel (RGB), layer uuid, color limits, gamma
+    didChangeRGBInputLayers = pyqtSignal(CompositeRecipe, str, object, tuple, float)
+    # Recipe, Channel (RGB), color limits
+    didChangeRGBColorLimits = pyqtSignal(CompositeRecipe, str, tuple)
+    # Recipe, Channel (RGB), gamma
+    didChangeRGBGamma = pyqtSignal(CompositeRecipe, str, float)
+
+    didChangeRecipeName = pyqtSignal(CompositeRecipe, str)
 
     _rgb = None  # combo boxes in r,g,b order; cache
     _sliders = None  # sliders in r,g,b order; cache
     _edits = None
-    _valid_ranges: List[Tuple[float, float]] = None  # tuples of each component's c-limits
     _gamma_boxes = None  # tuple of each component's gamma spin boxes
 
-    def __init__(self, ui, parent):
+    def __init__(self, ui, parent, model: LayerModel):
         super(RGBLayerConfigPane, self).__init__(parent)
         self.ui = ui
-        self._valid_ranges = [(None, None), (None, None), (None, None)]
-        self._families = {}
-        self.recipe = None
+        self._valid_ranges = [(None, None), (None, None), (None, None)]  # tuples of each component's c-limits
+        self._layer_uuids: list = []
+        self.recipe: Optional[CompositeRecipe] = None
+        self.model = model
 
         self._slider_steps = 100
         self.ui.slideMinRed.setRange(0, self._slider_steps)
         self.ui.slideMaxRed.setRange(0, self._slider_steps)
         self.ui.slideMinGreen.setRange(0, self._slider_steps)
         self.ui.slideMaxGreen.setRange(0, self._slider_steps)
         self.ui.slideMinBlue.setRange(0, self._slider_steps)
@@ -58,30 +63,47 @@
         self.ui.editMaxGreen.setValidator(qdoba)
         self.ui.editMaxGreen.setText("0.0")
         self.ui.editMinBlue.setValidator(qdoba)
         self.ui.editMinBlue.setText("0.0")
         self.ui.editMaxBlue.setValidator(qdoba)
         self.ui.editMaxBlue.setText("0.0")
 
-        [x.currentIndexChanged.connect(partial(self._combo_changed, combo=x, color=rgb))
-         for rgb, x in zip(('b', 'g', 'r'), (self.ui.comboBlue, self.ui.comboGreen, self.ui.comboRed))]
-        [x.valueChanged.connect(partial(self._slider_changed, slider=x, color=rgb, is_max=False))
-         for rgb, x in zip(('b', 'g', 'r'), (self.ui.slideMinBlue, self.ui.slideMinGreen, self.ui.slideMinRed))]
-        [x.valueChanged.connect(partial(self._slider_changed, slider=x, color=rgb, is_max=True))
-         for rgb, x in zip(('b', 'g', 'r'), (self.ui.slideMaxBlue, self.ui.slideMaxGreen, self.ui.slideMaxRed))]
-        [x.editingFinished.connect(partial(self._edit_changed, line_edit=x, color=rgb, is_max=False))
-         for rgb, x in zip(('b', 'g', 'r'), (self.ui.editMinBlue, self.ui.editMinGreen, self.ui.editMinRed))]
-        [x.editingFinished.connect(partial(self._edit_changed, line_edit=x, color=rgb, is_max=True))
-         for rgb, x in zip(('b', 'g', 'r'), (self.ui.editMaxBlue, self.ui.editMaxGreen, self.ui.editMaxRed))]
-        [x.valueChanged.connect(self._gamma_changed)
-         for rgb, x in
-         zip(('b', 'g', 'r'), (self.ui.redGammaSpinBox, self.ui.greenGammaSpinBox, self.ui.blueGammaSpinBox))]
+        [
+            x.currentIndexChanged.connect(partial(self._combo_changed, combo=x, color=rgb))
+            for rgb, x in zip(("b", "g", "r"), (self.ui.comboBlue, self.ui.comboGreen, self.ui.comboRed))
+        ]
+        [
+            x.valueChanged.connect(partial(self._slider_changed, slider=x, color=rgb, is_max=False))
+            for rgb, x in zip(("b", "g", "r"), (self.ui.slideMinBlue, self.ui.slideMinGreen, self.ui.slideMinRed))
+        ]
+        [
+            x.valueChanged.connect(partial(self._slider_changed, slider=x, color=rgb, is_max=True))
+            for rgb, x in zip(("b", "g", "r"), (self.ui.slideMaxBlue, self.ui.slideMaxGreen, self.ui.slideMaxRed))
+        ]
+        [
+            x.editingFinished.connect(partial(self._edit_changed, line_edit=x, color=rgb, is_max=False))
+            for rgb, x in zip(("b", "g", "r"), (self.ui.editMinBlue, self.ui.editMinGreen, self.ui.editMinRed))
+        ]
+        [
+            x.editingFinished.connect(partial(self._edit_changed, line_edit=x, color=rgb, is_max=True))
+            for rgb, x in zip(("b", "g", "r"), (self.ui.editMaxBlue, self.ui.editMaxGreen, self.ui.editMaxRed))
+        ]
+        [
+            x.valueChanged.connect(self._gamma_changed)
+            for rgb, x in zip(
+                ("b", "g", "r"), (self.ui.blueGammaSpinBox, self.ui.greenGammaSpinBox, self.ui.redGammaSpinBox)
+            )
+        ]
+
+        self.ui.nameEdit.textEdited.connect(self._rgb_name_edit_changed)
+
+        self._rgb_name_edit = self.ui.nameEdit
 
         # initialize the combo boxes
-        self._set_combos_to_family_names()
+        self.set_combos_to_layer_names()
         # disable all UI elements to start
         self._show_settings_for_layer()
 
     @property
     def rgb(self):
         if self._rgb is None:
             self._rgb = [self.ui.comboRed, self.ui.comboGreen, self.ui.comboBlue]
@@ -106,91 +128,119 @@
                 (self.ui.editMinRed, self.ui.editMaxRed),
                 (self.ui.editMinGreen, self.ui.editMaxGreen),
                 (self.ui.editMinBlue, self.ui.editMaxBlue),
             ]
         return self._edits
 
     @property
+    def rgb_name_edit(self):
+        return self._rgb_name_edit
+
+    @property
     def gamma_boxes(self):
         if self._gamma_boxes is None:
             self._gamma_boxes = (
                 self.ui.redGammaSpinBox,
                 self.ui.greenGammaSpinBox,
                 self.ui.blueGammaSpinBox,
             )
         return self._gamma_boxes
 
-    def family_added(self, family, family_info):
-        if family_info[Info.KIND] in [Kind.RGB, Kind.CONTOUR]:
-            # can't choose RGBs as components of RGBs
-            return
-
-        self._families[family] = family_info
-        self._set_combos_to_family_names()
-
-    def family_removed(self, family):
-        if family in self._families:
-            del self._families[family]
-            self._set_combos_to_family_names()
+    def layer_added(self, layer: LayerItem):
+        if layer.kind in [Kind.IMAGE, Kind.COMPOSITE]:
+            self._layer_uuids.append(layer.uuid)
+            self.set_combos_to_layer_names()
+
+    def layer_removed(self, layer_uuid):
+        if layer_uuid in self._layer_uuids:
+            idx = self._layer_uuids.index(layer_uuid)
+            del self._layer_uuids[idx]
+            self.set_combos_to_layer_names()
             self._show_settings_for_layer(self.recipe)
 
     def _gamma_changed(self, value):
         gamma = tuple(x.value() for x in self.gamma_boxes)
-        self.didChangeRGBComponentGamma.emit(self.recipe, gamma)
+        recipe_gamma = self.recipe.gammas
+        channels = ["r", "g", "b"]
 
-    def _combo_changed(self, index, combo: QComboBox = None, color=None):
-        family = combo.itemData(index)
-        if not family:
+        changed_channel = ""
+
+        for idx in range(len(channels)):
+            if gamma[idx] != recipe_gamma[idx]:
+                changed_channel = channels[idx]
+
+        self.didChangeRGBGamma.emit(self.recipe, changed_channel, value)
+
+    def _combo_changed(self, index, combo: QComboBox, color):
+        layer_uuid = combo.itemData(index)
+        if not layer_uuid:
             # we use None as no-selection value, not empty string
-            family = None
+            layer_uuid = None
 
-        LOG.debug("RGB: user selected %s for %s" % (repr(family), color))
+        LOG.debug("RGB: user selected %s for %s" % (repr(layer_uuid), color))
         # reset slider position to min and max for layer
-        self._set_minmax_slider(color, family)
-        self.didChangeRGBComponentSelection.emit(self.recipe, color, family)
+        self._set_minmax_slider(color, layer_uuid)
+
+        layer = self.model.get_layer_by_uuid(layer_uuid)
+
+        if not layer:
+            clim = (None, None)
+        else:
+            valid_range = layer.valid_range
+            # TODO: is the actual range really used correctly here?
+            actual_range = layer.get_actual_range_from_layer()
+            assert actual_range != (None, None)  # nosec B101
+            clim = valid_range if valid_range else actual_range
+
+        self.didChangeRGBInputLayers.emit(self.recipe, color, layer_uuid, clim, DEFAULT_GAMMA_VALUE)
+
+        self._show_settings_for_layer(self.recipe)
+
+    def _rgb_name_edit_changed(self, text):
+        self.didChangeRecipeName.emit(self.recipe, text)
 
     def _display_to_data(self, color: str, values):
         """Convert display value to data value."""
         if self.recipe is None:
             # No recipe has been set yet
             return values
-        family = self.recipe.input_ids[RGBA2IDX[color]]
-        if family is None:
+        layer_uuid = self.recipe.input_layer_ids[RGBA2IDX[color]]
+        if layer_uuid is None:
             return values
-        family_info = self._families[family]
-        return family_info[Info.UNIT_CONVERSION][1](values, inverse=True)
+        layer = self.model.get_layer_by_uuid(layer_uuid)
+        assert layer is not None  # nosec B101 # suppress mypy [union-attr]
+        layer_info = layer.info
+
+        return (
+            layer_info[Info.UNIT_CONVERSION][1](values, inverse=True)
+            if layer_info.get(Info.UNIT_CONVERSION)
+            else values
+        )
 
     def _data_to_display(self, color: str, values):
         "convert data value to display value"
-        family = self.recipe.input_ids[RGBA2IDX[color]]
-        if family is None:
+        if self.recipe is None:
+            # No recipe has been set yet
+            return values
+        layer_uuid = self.recipe.input_layer_ids[RGBA2IDX[color]]
+        if layer_uuid is None:
             return values
-        family_info = self._families[family]
-        return family_info[Info.UNIT_CONVERSION][1](values)
+        layer = self.model.get_layer_by_uuid(layer_uuid)
+        assert layer is not None  # nosec B101 # suppress mypy [union-attr]
+        layer_info = layer.info
+
+        return layer_info[Info.UNIT_CONVERSION][1](values) if layer_info.get(Info.UNIT_CONVERSION) else values
 
     def _get_slider_value(self, valid_min, valid_max, slider_val):
         return (slider_val / self._slider_steps) * (valid_max - valid_min) + valid_min
 
     def _create_slider_value(self, valid_min, valid_max, channel_val):
         return int((channel_val - valid_min) / (valid_max - valid_min)) * self._slider_steps
 
-    def _min_max_for_color(self, rgba: str):
-        """
-        return min value, max value as represented in sliders
-        :param rgba: char in 'rgba'
-        :return: (min-value, max-value) where min can be > max
-        """
-        idx = RGBA2IDX[rgba]
-        slider = self.sliders[idx]
-        valid_min, valid_max = self._valid_ranges[idx]
-        n = self._get_slider_value(valid_min, valid_max, slider[0].value())
-        x = self._get_slider_value(valid_min, valid_max, slider[1].value())
-        return n, x
-
-    def _update_line_edits(self, color: str, n: float = None, x: float = None):
+    def _update_line_edits(self, color: str, n: Optional[float] = None, x: Optional[float] = None):
         """
         update edit controls to match non-None values provided
         if called with just color, returns current min and max
         implicitly convert data values to and from display values
         :param color: in 'rgba'
         :param n: minimum data value or None
         :param x: max data value or None
@@ -198,48 +248,46 @@
         """
         idx = RGBA2IDX[color]
         edn, edx = self.line_edits[idx]
         edn.blockSignals(True)
         edx.blockSignals(True)
         if n is not None:
             ndis = self._data_to_display(color, n)
-            edn.setText('%f' % ndis)
+            edn.setText("%f" % ndis)
         else:
             ndis = float(edn.text())
             n = self._display_to_data(color, ndis)
         if x is not None:
             xdis = self._data_to_display(color, x)
-            edx.setText('%f' % xdis)
+            edx.setText("%f" % xdis)
         else:
             xdis = float(edx.text())
             x = self._display_to_data(color, xdis)
         edn.blockSignals(False)
         edx.blockSignals(False)
         return n, x
 
     def _signal_color_changing_range(self, color: str, n: float, x: float):
-        idx = RGBA2IDX[color]
-        new_limits = list(self.recipe.color_limits)
-        new_limits[idx] = (n, x)
-        self.didChangeRGBComponentLimits.emit(self.recipe, tuple(new_limits))
+        new_limits = (n, x)
+        self.didChangeRGBColorLimits.emit(self.recipe, color, new_limits)
 
-    def _slider_changed(self, value=None, slider=None, color: str = None, is_max: bool = False):
+    def _slider_changed(self, value, slider, color: str, is_max: bool):
         """
         handle slider update event from user
         :param slider: control
         :param color: char in 'rgba'
         :param is_max: whether slider's value represents the max or the min
         :return:
         """
         idx = RGBA2IDX[color]
         valid_min, valid_max = self._valid_ranges[idx]
         if value is None:
             value = slider.value()
         value = self._get_slider_value(valid_min, valid_max, value)
-        LOG.debug('slider %s %s => %f' % (color, 'max' if is_max else 'min', value))
+        LOG.debug("slider %s %s => %f" % (color, "max" if is_max else "min", value))
         n, x = self._update_line_edits(color, value if not is_max else None, value if is_max else None)
         self._signal_color_changing_range(color, n, x)
 
     def _edit_changed(self, line_edit: QLineEdit, color: str, is_max: bool):
         """
         update relevant slider value, propagate to the document
         :param line_edit: field that got a new value
@@ -247,178 +295,172 @@
         :param is_max: whether the min or max edit field was changed
         :return:
         """
         idx = RGBA2IDX[color]
         vn, vx = self._valid_ranges[idx]
         vdis = float(line_edit.text())
         val = self._display_to_data(color, vdis)
-        LOG.debug('line edit %s %s => %f => %f' % (color, 'max' if is_max else 'min', vdis, val))
+        LOG.debug("line edit %s %s => %f => %f" % (color, "max" if is_max else "min", vdis, val))
         sv = self._create_slider_value(vn, vx, val)
         slider = self.sliders[idx][1 if is_max else 0]
         slider.blockSignals(True)
         slider.setValue(sv)
         slider.blockSignals(False)
         self._signal_color_changing_range(color, *self._update_line_edits(color))
 
-    def selection_did_change(self, recipe):
+    def selection_did_change(self, layers: Tuple[LayerItem]):
         """Change UI elements to reflect the provided recipe."""
-        self.recipe = recipe
-        self._show_settings_for_layer(recipe)
+        if layers is not None and len(layers) == 1:
+            layer = layers[0]
+            self.recipe = layer.recipe if isinstance(layer.recipe, CompositeRecipe) else None
+            self._show_settings_for_layer(self.recipe)
 
     def _show_settings_for_layer(self, recipe=None):
-        if recipe is None:
-            for slider in self.sliders:
-                slider[0].setDisabled(True)
-                slider[1].setDisabled(True)
-            for combo in self.rgb:
-                combo.setDisabled(True)
-            for edit in self.line_edits:
-                edit[0].setDisabled(True)
-                edit[1].setDisabled(True)
-            for sbox in self.gamma_boxes:
-                sbox.setDisabled(True)
+        if not isinstance(recipe, CompositeRecipe):
+            self.ui.rgbScrollAreaWidgetContents.setDisabled(True)
             return
         else:
-            # re-enable all the widgets
-            for slider in self.sliders:
-                slider[0].setDisabled(False)
-                slider[1].setDisabled(False)
-            for combo in self.rgb:
-                combo.setDisabled(False)
-            for edit in self.line_edits:
-                edit[0].setDisabled(False)
-                edit[1].setDisabled(False)
-            for sbox in self.gamma_boxes:
-                sbox.setDisabled(False)
+            self.ui.rgbScrollAreaWidgetContents.setDisabled(False)
 
         for widget in self.rgb:
             # block signals so an existing RGB layer doesn't get overwritten with new layer selections
             widget.blockSignals(True)
 
         # update the combo boxes
         self._select_components_for_recipe(recipe)
         self._set_minmax_sliders(recipe)
         self._set_gamma_boxes(recipe)
+        self._set_rgb_name_edit(recipe)
 
         for widget in self.rgb:
             # block signals so an existing RGB layer doesn't get overwritten with new layer selections
             widget.blockSignals(False)
 
-    def _set_minmax_slider(self, color: str, family: str, clims: Optional[Tuple[float, float]] = None):
+    def _set_rgb_name_edit(self, recipe):
+        if recipe is not None:
+            self.rgb_name_edit.setText(recipe.name)
+        else:
+            self.rgb_name_edit.setText("")
+
+    def _set_minmax_slider(self, color: str, layer_uuid: uuid.UUID, clims: Optional[Tuple[float, float]] = None):
         idx = RGBA2IDX[color]
         slider = self.sliders[idx]
         editn, editx = self.line_edits[idx]
-        if family not in self._families:
-            LOG.debug(
-                "Could not find {} in families {}".format(repr(family), repr(list(sorted(self._families.keys())))))
+        if layer_uuid not in self._layer_uuids:
+            LOG.debug("Could not find {} in layer_uuids {}".format(repr(layer_uuid), self._layer_uuids))
         # block signals so the changed sliders don't trigger updates
         slider[0].blockSignals(True)
         slider[1].blockSignals(True)
-        if clims is None or clims == (None, None) or \
-                family not in self._families:
+        if clims is None or clims == (None, None) or layer_uuid not in self._layer_uuids:
             self._valid_ranges[idx] = (None, None)
             slider[0].setSliderPosition(0)
             slider[1].setSliderPosition(0)
             editn.blockSignals(True)
             editx.blockSignals(True)
-            editn.setText('0.0')
-            editx.setText('0.0')
+            editn.setText("0.0")
+            editx.setText("0.0")
             editn.blockSignals(False)
             editx.blockSignals(False)
             slider[0].setDisabled(True)
             slider[1].setDisabled(True)
             editn.setDisabled(True)
             editx.setDisabled(True)
         else:
             slider[0].setDisabled(False)
             slider[1].setDisabled(False)
             editn.setDisabled(False)
             editx.setDisabled(False)
 
-            valid_range = self._families[family][Info.VALID_RANGE]
-            self._valid_ranges[idx] = valid_range
+            layer = self.model.get_layer_by_uuid(layer_uuid)
+            assert layer is not None  # nosec B101 # suppress mypy [union-attr]
+            valid_range = layer.valid_range
+            # TODO: is the actual range really used correctly here?
+            actual_range = layer.get_actual_range_from_layer()
+            assert actual_range != (None, None)  # nosec B101
+            layer_range = valid_range if valid_range else actual_range
+            self._valid_ranges[idx] = layer_range
 
-            slider_val = self._create_slider_value(valid_range[0], valid_range[1], clims[0])
+            slider_val = self._create_slider_value(layer_range[0], layer_range[1], clims[0])
             slider[0].setSliderPosition(max(slider_val, 0))
-            slider_val = self._create_slider_value(valid_range[0], valid_range[1], clims[1])
+            slider_val = self._create_slider_value(layer_range[0], layer_range[1], clims[1])
             slider[1].setSliderPosition(min(slider_val, self._slider_steps))
 
             self._update_line_edits(color, *clims)
         slider[0].blockSignals(False)
         slider[1].blockSignals(False)
 
     def _set_minmax_sliders(self, recipe):
         if recipe:
-            for idx, (color, clim) in enumerate(zip("rgb", recipe.color_limits)):
-                family = recipe.input_ids[idx]
-                self._set_minmax_slider(color, family, clim)
+            for idx, (color, clim) in enumerate(zip("rgb", self.recipe.color_limits)):
+                layer_uuid = self.recipe.input_layer_ids[idx]
+                self._set_minmax_slider(color, layer_uuid, clim)
         else:
             self._set_minmax_slider("r", None)
             self._set_minmax_slider("g", None)
             self._set_minmax_slider("b", None)
 
     def _select_components_for_recipe(self, recipe=None):
         if recipe is not None:
-            for family_name, widget in zip(recipe.input_ids, self.rgb):
-                if family_name is None:
+            for layer_uuid, widget in zip(recipe.input_layer_ids, self.rgb):
+                if not layer_uuid:
                     widget.setCurrentIndex(0)
                 else:
                     # Qt can't handle item data being tuples
-                    dex = widget.findData(family_name)
+                    dex = widget.findData(layer_uuid)
                     if dex <= 0:
                         widget.setCurrentIndex(0)
-                        LOG.error("Layer family '%s' not available to be selected" % (family_name,))
+                        LOG.error("Layer with uuid '%s' not available to" " be selected" % (layer_uuid,))
                     else:
                         widget.setCurrentIndex(dex)
         else:
             for widget in self.rgb:
                 widget.setCurrentIndex(0)
 
-    def _set_combos_to_family_names(self):
+    def set_combos_to_layer_names(self):
         """
         update combo boxes with the list of layer names and then select the right r,g,b,a layers if they're not None
         :return:
         """
         # Get the current selected families so we can reselect them when we
         # rebuild the lists.
-        current_families = [x.itemData(x.currentIndex()) for x in self.rgb]
+        current_layers = [x.itemData(x.currentIndex()) for x in self.rgb]
 
         for widget in self.rgb:
             # block signals so an existing RGB layer doesn't get overwritten with new layer selections
             widget.blockSignals(True)
 
         # clear out the current lists
         for widget in self.rgb:
             widget.clear()
-            widget.addItem('None', '')
+            widget.addItem("None", None)
 
         # fill up our lists of layers
-        for widget, selected_family in zip(self.rgb, current_families):
-            if not selected_family or selected_family not in self._families:
-                # if the selection is None or the current family was removed
-                # if the current family was removed by the document then the
+        for widget, selected_layer_uuid in zip(self.rgb, current_layers):
+            if not selected_layer_uuid or selected_layer_uuid not in self._layer_uuids:
+                # if the selection is None or the current layer was removed
+                # if the current layer was removed by the document then the
                 # document should have updated the recipe
                 widget.setCurrentIndex(0)
-            for idx, (family_name, family_info) in enumerate(
-                    sorted(self._families.items(), key=lambda x: x[1][Info.DISPLAY_FAMILY])):
-                # Qt can't handle tuples as
-                display_name = family_info[Info.DISPLAY_FAMILY]
-                LOG.debug('adding to widget family {} as "{}"'.format(family_name, display_name))
-                widget.addItem(display_name, family_name)
-                widget.findData(family_name)  # sanity check
-                if family_name == selected_family:
-                    # None is 0 so add 1 to index
+
+            for idx, layer_uuid in enumerate(self._layer_uuids):
+                layer: LayerItem = self.model.get_layer_by_uuid(layer_uuid)
+                display_name = layer.descriptor
+
+                widget.addItem(display_name, layer_uuid)
+                widget.findData(uuid)
+
+                if layer_uuid == selected_layer_uuid:
                     widget.setCurrentIndex(idx + 1)
 
         for widget in self.rgb:
             # block signals so an existing RGB layer doesn't get overwritten with new layer selections
             widget.blockSignals(False)
 
     def _set_gamma_boxes(self, recipe=None):
         if recipe is not None:
             for idx, sbox in enumerate(self.gamma_boxes):
-                sbox.setDisabled(recipe.input_ids[idx] is None)
+                sbox.setDisabled(recipe.input_layer_ids[idx] is None)
                 sbox.setValue(recipe.gammas[idx])
         else:
             for sbox in self.gamma_boxes:
                 sbox.setDisabled(True)
-                sbox.setValue(1.)
+                sbox.setValue(1.0)
```

### Comparing `uwsift-1.2.3/uwsift/view/scene_graph.py` & `uwsift-2.0.0b0/uwsift/view/scene_graph.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,742 +1,533 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
-"""
-LayerRep.py
-~~~~~~~~~~~
-
-PURPOSE
-Layer representation - the "physical" realization of content to draw on the map.
-A layer representation can have multiple levels of detail
+"""Provide a SceneGraphManager to handle display of visuals.
 
-A factory will convert URIs into LayerReps
-LayerReps are managed by document, and handed off to the MapWidget as part of a LayerDrawingPlan
-
-REFERENCES
+As per http://api.vispy.org/en/latest/scene.html (abridged)
 
+    - Vispy scene graph (SG) prerequisites:
+        1. create SceneCanvas -> this object's scene property is top level node in SG:
 
-REQUIRES
+           ```
+           vispy_canvas = scene.SceneCanvas
+           sg_root_node = vispy_canvas.scene
+           ```
+        2. create node instances (from vispy.scene.visuals)
+        3. add node instances to scene by making them children of canvas scene, or
+           of nodes already in the scene
 
+REFERENCES
+http://api.vispy.org/en/latest/scene.html
 
 :author: R.K.Garcia <rayg@ssec.wisc.edu>
 :copyright: 2014 by University of Wisconsin Regents, see AUTHORS for more details
 :license: GPLv3, see LICENSE for more details
 """
-__docformat__ = 'reStructuredText'
-__author__ = 'davidh'
 
 import logging
 import os
+from enum import Enum
+from numbers import Number
+from typing import Optional
 from uuid import UUID
 
 import numpy as np
-from PyQt5.QtCore import QObject, pyqtSignal, Qt
+from PyQt5.QtCore import QObject, Qt, pyqtSignal
 from PyQt5.QtGui import QCursor
-from vispy import app
-from vispy import scene
+from pyresample import AreaDefinition
+from vispy import app, scene
 from vispy.geometry import Rect
-from vispy.scene.visuals import Markers, Polygon, Compound, Line
-from vispy.util.keys import SHIFT
-from vispy.visuals import LineVisual
-from vispy.visuals.transforms import STTransform, MatrixTransform, ChainTransform
 from vispy.gloo.util import _screenshot
+from vispy.scene.visuals import Image, Line, Markers, Polygon
+from vispy.util.keys import SHIFT
+from vispy.visuals.transforms import MatrixTransform, STTransform
 
-from uwsift.common import DEFAULT_ANIMATION_DELAY, Info, Kind, Tool, Presentation
-from uwsift.model.document import DocLayerStack, DocBasicLayer
+from uwsift import IMAGE_DISPLAY_MODE, config
+from uwsift.common import (
+    BORDERS_DATASET_NAME,
+    DEFAULT_ANIMATION_DELAY,
+    DEFAULT_GRID_CELL_HEIGHT,
+    DEFAULT_GRID_CELL_WIDTH,
+    DEFAULT_TILE_HEIGHT,
+    DEFAULT_TILE_WIDTH,
+    LATLON_GRID_DATASET_NAME,
+    ImageDisplayMode,
+    Info,
+    Kind,
+    Presentation,
+    Tool,
+)
+from uwsift.model.area_definitions_manager import AreaDefinitionsManager
+from uwsift.model.document import Document
+from uwsift.model.layer_item import LayerItem
+from uwsift.model.layer_model import LayerModel
+from uwsift.model.product_dataset import ProductDataset
+from uwsift.model.time_manager import TimeManager
 from uwsift.queue import TASK_DOING, TASK_PROGRESS
 from uwsift.util import get_package_data_dir
 from uwsift.view.cameras import PanZoomProbeCamera
 from uwsift.view.probes import DEFAULT_POINT_PROBE
 from uwsift.view.transform import PROJ4Transform
-from uwsift.view.visuals import (NEShapefileLines, TiledGeolocatedImage, RGBCompositeLayer, PrecomputedIsocurve)
+from uwsift.view.visuals import (
+    Lines,
+    MultiChannelImage,
+    NEShapefileLines,
+    RGBCompositeImage,
+    TiledGeolocatedImage,
+)
+from uwsift.workspace.utils.metadata_utils import (
+    get_point_style_by_name,
+    map_point_style_to_marker_kwargs,
+)
 
 LOG = logging.getLogger(__name__)
 DATA_DIR = get_package_data_dir()
-DEFAULT_SHAPE_FILE = os.path.join(DATA_DIR, 'ne_50m_admin_0_countries', 'ne_50m_admin_0_countries.shp')
-DEFAULT_STATES_SHAPE_FILE = os.path.join(DATA_DIR, 'ne_50m_admin_1_states_provinces_lakes',
-                                         'ne_50m_admin_1_states_provinces_lakes.shp')
+DEFAULT_SHAPE_FILE = os.path.join(DATA_DIR, "ne_50m_admin_0_countries", "ne_50m_admin_0_countries.shp")
+DEFAULT_STATES_SHAPE_FILE = os.path.join(
+    DATA_DIR, "ne_50m_admin_1_states_provinces_lakes", "ne_50m_admin_1_states_provinces_lakes.shp"
+)
 DEFAULT_TEXTURE_SHAPE = (4, 16)
 
 
 class Markers2(Markers):
-    def _set_clipper(self, node, clipper):
-        return
+    pass
 
 
 Markers = Markers2
 
 
-class FakeMarker(Compound):
-    # FIXME: Temporary workaround because markers don't work on the target Windows laptops
-    def __init__(self, pos=None, parent=None, symbol=None, **kwargs):
-        self.line_one = None
-        self.line_two = None
-        self.symbol = symbol
-        point = pos[0]
-
-        kwargs["connect"] = "segments"
-        width = 5
-        pos1, pos2 = self._get_positions(point)
-        if self.line_one is None:
-            self.line_one = LineVisual(pos=pos1, width=width, **kwargs)
-            self.line_two = LineVisual(pos=pos2, width=width, **kwargs)
-
-        # For some reason we can't add the subvisuals later, so we'll live with redundant logic
-        super().__init__((self.line_one, self.line_two), parent=parent)
-
-        # self.set_point(point, **kwargs)
-
-    def _get_positions(self, point):
-        margin = 0.5
-        if self.symbol == 'x':
-            pos1 = np.array([[point[0] - margin, point[1] - margin * 2, point[2]],
-                             [point[0] + margin, point[1] + margin * 2, point[2]]])
-            pos2 = np.array([[point[0] - margin, point[1] + margin * 2, point[2]],
-                             [point[0] + margin, point[1] - margin * 2, point[2]]])
-        else:
-            pos1 = np.array([[point[0] - margin, point[1], point[2]], [point[0] + margin, point[1], point[2]]])
-            pos2 = np.array([[point[0], point[1] - margin * 2, point[2]], [point[0], point[1] + margin * 2, point[2]]])
-        return pos1, pos2
-
-    def set_point(self, point, **kwargs):
-        kwargs["connect"] = "segments"
-        pos1, pos2 = self._get_positions(point)
-        self.line_one.set_data(pos=pos1)
-        self.line_two.set_data(pos=pos2)
-
-
 class SIFTMainMapCanvas(scene.SceneCanvas):
     """High level map canvas node."""
+
     pass
 
 
 class MainMap(scene.Node):
     """Scene node for holding all of the information for the main map area."""
 
     def __init__(self, *args, **kwargs):
         super(MainMap, self).__init__(*args, **kwargs)
 
 
 class PendingPolygon(object):
-    """Temporary information holder for Probe Polygons.
-    """
+    """Temporary information holder for Probe Polygons."""
 
     def __init__(self, point_parent):
         self.parent = point_parent
         self.markers = []
         self.canvas_points = []
         self.points = []
         self.radius = 10.0
 
-    def is_complete(self, canvas_pos):
+    def _is_complete(self, canvas_pos):
         # XXX: Can't get "visuals_at" method of the SceneCanvas to work to find if the point is ready
         if len(self.points) < 3:
             return False
         p1 = self.canvas_points[0]
         r = self.radius
         if (p1[0] - r <= canvas_pos[0] <= p1[0] + r) and (p1[1] - r <= canvas_pos[1] <= p1[1] + r):
             return True
 
     def add_point(self, canvas_pos, xy_pos, z=100):
-        if self.is_complete(canvas_pos):
+        if self._is_complete(canvas_pos):
             # Are you finishing the polygon by adding this point (same point as the first point...or near it)
             return True
         self.canvas_points.append(canvas_pos)
         self.points.append(xy_pos)
         if len(xy_pos) == 2:
             xy_pos = [xy_pos[0], xy_pos[1], z]
-        point_visual = Markers(parent=self.parent,
-                               name='polygon_%02d' % (len(self.markers),),
-                               symbol="disc", pos=np.array([xy_pos]),
-                               face_color=np.array([0., 0.5, 0.5, 1.]),
-                               edge_color=np.array([.5, 1.0, 1.0, 1.]),
-                               size=18.,
-                               edge_width=3.,
-                               )
+        point_visual = Markers(
+            parent=self.parent,
+            name="polygon_%02d" % (len(self.markers),),
+            symbol="disc",
+            pos=np.array([xy_pos]),
+            face_color=np.array([0.0, 0.5, 0.5, 1.0]),
+            edge_color=np.array([0.5, 1.0, 1.0, 1.0]),
+            size=18.0,
+            edge_width=3.0,
+        )
         self.markers.append(point_visual)
         return False
 
     def reset(self):
         self.markers = []
         self.canvas_points = []
         self.points = []
 
 
-class LayerSet(object):
-    """Basic bookkeeping object for each layer set (A, B, C, D) from the UI.
-
-    Each LayerSet has its own:
-     - Per layer visiblity
-     - Animation loop and frame order
-     - Layer Order
-    """
+class AnimationController(object):
+    """Basic bookkeeping object for each layer set (A, B, C, D) from the UI."""
 
-    def __init__(self, parent, layers=None, layer_order=None, frame_order=None, frame_change_cb=None):
-        if layers is None and (layer_order is not None or frame_order is not None):
-            raise ValueError("'layers' required when 'layer_order' or 'frame_order' is specified")
-
-        self.parent = parent
-        self._layers = {}
-        self._layer_order = []  # display (z) order, top to bottom
-        self._frame_order = []  # animation order, first to last
-        self._animating = False
-        self._frame_number = 0
-        self._frame_change_cb = frame_change_cb
+    def __init__(self):
         self._animation_speed = DEFAULT_ANIMATION_DELAY  # milliseconds
-        self._animation_timer = app.Timer(self._animation_speed / 1000.0, connect=self.next_frame)
-
-        if layers is not None:
-            self.set_layers(layers)
+        self._animating = False
 
-            if layer_order is None:
-                layer_order = [x.name for x in layers.keys()]
-            self.set_layer_order(layer_order)
-
-            if frame_order is None:
-                frame_order = [x.name for x in layers.keys()]
-            self.frame_order = frame_order
+        self.time_manager = TimeManager(self._animation_speed)
 
-    @property
-    def current_frame(self):
-        return self._frame_number
+        self._animation_timer = app.Timer(self._convert_ms_to_s(self._animation_speed))
+        self._animation_timer.connect(self.time_manager.tick)
 
-    @property
-    def max_frame(self):
-        return len(self._frame_order)
+    @staticmethod
+    def _convert_ms_to_s(time_ms: float) -> float:
+        return time_ms / 1000.0
 
     @property
     def animation_speed(self):
-        """speed in milliseconds
-        """
+        """speed in milliseconds"""
         return self._animation_speed
 
     @animation_speed.setter
     def animation_speed(self, milliseconds):
         if milliseconds <= 0:
             return
         self._animation_timer.stop()
         self._animation_speed = milliseconds
-        self._animation_timer.interval = milliseconds / 1000.0
-        if self._frame_order:
-            self._animating = True
+        self._animation_timer.interval = self._convert_ms_to_s(milliseconds)
+        if self.animating:
             self._animation_timer.start()
-        if self._frame_change_cb is not None and self._frame_order:
-            uuid = self._frame_order[self._frame_number]
-            self._frame_change_cb((self._frame_number, len(self._frame_order), self._animating, uuid))
-
-    def set_layers(self, layers):
-        # FIXME clear the existing layers
-        for layer in layers:
-            self.add_layer(layer)
-
-    def add_layer(self, layer):
-        LOG.debug('add layer {}'.format(layer))
-        uuid = UUID(layer.name)  # we backitty-forth this because
-        self._layers[uuid] = layer
-        self._layer_order.insert(0, uuid)
-        self.update_layers_z()
-        # self._frame_order.append(uuid)
-
-    def set_layer_order(self, layer_order):
-        for o in layer_order:
-            # Layer names are UUIDs
-            if o not in self._layers and o is not None:
-                LOG.error('set_layer_order cannot deal with unknown layer {}'.format(o))
-                return
-        self._layer_order = list(layer_order)
-        self.update_layers_z()
-
-    @property
-    def frame_order(self):
-        return self._frame_order
-
-    @frame_order.setter
-    def frame_order(self, frame_order):
-        for o in frame_order:
-            if o not in self._layers:
-                LOG.error('set_frame_order cannot deal with unknown layer {}'.format(o))
-                return
-        self._frame_order = frame_order
-        # FIXME: ticket #92: this is not a good idea
-        self._frame_number = 0
-        # LOG.debug('accepted new frame order of length {}'.format(len(frame_order)))
-        # if self._frame_change_cb is not None and self._frame_order:
-        #     uuid = self._frame_order[self._frame_number]
-        #     self._frame_change_cb((self._frame_number, len(self._frame_order), self._animating, uuid))
-
-    def update_layers_z(self):
-        for z_level, uuid in enumerate(self._layer_order):
-            transform = self._layers[uuid].transform
-            if isinstance(transform, ChainTransform):
-                # assume ChainTransform where the last transform is STTransform for Z level
-                transform = transform.transforms[-1]
-            transform.translate = (0, 0, 0 - int(z_level))
-            self._layers[uuid].order = len(self._layer_order) - int(z_level)
-        # Need to tell the scene to recalculate the drawing order (HACK, but it works)
-        # FIXME: This should probably be accomplished by overriding the right method from the Node or Visual class
-        self.parent.main_canvas._update_scenegraph(None)
-
-    def top_layer_uuid(self):
-        for layer_uuid in self._layer_order:
-            if self._layers[layer_uuid].visible:
-                return layer_uuid
-        # None of the image layers are visible
-        return None
 
     @property
     def animating(self):
         return self._animating
 
     @animating.setter
     def animating(self, animate):
         if animate == self._animating:
             # Don't update anything if nothing about the animation has changed
             return
         elif self._animating and not animate:
-            # We are currently, but don't want to be
+            # Stop animation
             self._animating = False
             self._animation_timer.stop()
-        elif not self._animating and animate and self._frame_order:
-            # We are not currently, but want to be
+        elif not self._animating and animate:
+            # Start animation
             self._animating = True
             self._animation_timer.start()
-            # TODO: Add a proper AnimationEvent to self.events
-        if self._frame_change_cb is not None and self._frame_order:
-            uuid = self._frame_order[self._frame_number]
-            self._frame_change_cb((self._frame_number, len(self._frame_order), self._animating, uuid))
 
     def toggle_animation(self, *args):
         self.animating = not self._animating
         return self.animating
 
-    def _set_visible_node(self, node):
-        """Set all nodes to invisible except for the `event.added` node.
-        """
-        for child in self._layers.values():
-            with child.events.blocker():
-                if child is node.added:
-                    child.visible = True
-                else:
-                    child.visible = False
-
-    def _set_visible_child(self, frame_number):
-        for idx, uuid in enumerate(self._frame_order):
-            child = self._layers[uuid]
-            # not sure if this is actually doing anything
-            with child.events.blocker():
-                if idx == frame_number:
-                    child.visible = True
-                else:
-                    child.visible = False
-
-    def next_frame(self, event=None, frame_number=None):
-        """
-        skip to the frame (from 0) or increment one frame and update
-        typically this is run by self._animation_timer
-        :param frame_number: optional frame to go to, from 0
-        :return:
-        """
-        lfo = len(self._frame_order)
-        frame = self._frame_number
-        if frame_number is None:
-            frame = self._frame_number + 1
-        elif isinstance(frame_number, int):
-            if frame_number == -1:
-                frame = self._frame_number + (lfo - 1)
-            else:
-                frame = frame_number
-        if lfo > 0:
-            frame %= lfo
-        else:
-            frame = 0
-        self._set_visible_child(frame)
-        self._frame_number = frame
-        self.parent.update()
-        if self._frame_change_cb is not None and lfo:
-            uuid = self._frame_order[self._frame_number]
-            self._frame_change_cb((self._frame_number, lfo, self._animating, uuid))
+    def jump(self, index):
+        self.time_manager.jump(index)
 
+    def connect_to_model(self, model: LayerModel):
+        self.time_manager.connect_to_model(model)
 
-class ContourGroupNode(scene.Node):
-    """VisPy scene graph node managing multiple visuals.
+    def get_frame_count(self):
+        return self.time_manager.get_current_timebase_dataset_count()
 
-    This Node handles view changes and representing different "zoom" levels
-    in the data that is provided to its child widgets.
+    def get_current_frame_index(self):
+        return self.time_manager.get_current_timebase_timeline_index()
 
-    """
+    def get_current_frame_uuid(self):
+        return self.time_manager.get_current_timebase_current_dataset_uuid()
 
-    @staticmethod
-    def visible_first(children):
-        invisible_children = []
-        for c in children:
-            if c.visible:
-                yield c
-            else:
-                invisible_children.append(c)
-        for c in invisible_children:
-            yield c
-
-    def on_view_change(self):
-        zoom_level = None
-        for child in self.visible_first(self.children):
-            if isinstance(child, PrecomputedIsocurve):
-                if zoom_level is None:
-                    zoom_level = self._assess_contour(child)
-                # child handles an unchanged zoom_level
-                child.zoom_level = zoom_level
-            else:
-                raise NotImplementedError("Don't know how to assess "
-                                          "non-contour layer")
-
-    def _assess_contour(self, child):
-        """Calculate shown portion of image and image units per pixel
-
-        This method utilizes a precomputed "mesh" of relatively evenly
-        spaced points over the entire image space. This mesh is transformed
-        to the canvas space (-1 to 1 user-viewed space) to figure out which
-        portions of the image are currently being viewed and which portions
-        can actually be projected on the viewed projection.
-
-        While the result of the chosen method may not always be completely
-        accurate, it should work for all possible viewing cases.
+    def get_frame_uuids(self):
         """
-        # in contour coordinate space, the extents of the canvas
-        canvas_extents = child.transforms.get_transform().imap([
-            [-1., -1.],
-            [0., 0.],
-            [1., 1.],
-            [-1., 1.],
-            [1., -1.]
-        ])[:, :2]
-        canvas_size = self.canvas.size
-        # valid projection coordinates
-        canvas_extents = canvas_extents[(canvas_extents[:, 0] <= 1e30) & (canvas_extents[:, 1] <= 1e30), :]
-        if not canvas_extents.size:
-            LOG.warning("Can't determine current view box, using lowest contour resolution")
-            zoom_level = 0
-        else:
-            min_x = canvas_extents[:, 0].min()
-            max_x = canvas_extents[:, 0].max()
-            min_y = canvas_extents[:, 1].min()
-            max_y = canvas_extents[:, 1].max()
-            pixel_ratio = max((max_x - min_x) / canvas_size[0], (max_y - min_y) / canvas_size[1])
-
-            if pixel_ratio > 10000:
-                zoom_level = 0
-            elif pixel_ratio > 5000:
-                zoom_level = 1
-            elif pixel_ratio > 3000:
-                zoom_level = 2
-            elif pixel_ratio > 1000:
-                zoom_level = 3
-            else:
-                zoom_level = 4
+        Get a list of dataset uuids, one for each frame of the animation as the
+        current timeline manager would play. The uuids are those of the current
+        driving layer, therefore they are unique in the list.
 
-        return zoom_level
+        :return: list of dataset UUIDs
+        """
+        return self.time_manager.get_current_timebase_dataset_uuids()
 
 
 class SceneGraphManager(QObject):
     """
     SceneGraphManager represents a document as a vispy scenegraph.
     When document changes, it updates to correspond.
     Handles animation by cycling visibility.
     Provides means of highlighting areas.
     Decides what sampling to bring data in from the workspace,
     in order to feed the display optimally.
     """
 
-    document = None  # Document object we work with
-    workspace = None  # where we get data arrays from
+    # TODO(ar) REVIEW: distinction between class and member/instance
+    #  variables seems random (see below)
     queue = None  # background jobs go here
 
-    border_shapefile = None  # background political map
     texture_shape = None
-    polygon_probes = None
-    point_probes = None
 
-    image_elements = None  # {layer_uuid:element}
-    composite_element_dependencies = None  # {layer_uuid:set-of-dependent-uuids}
     datasets = None
     colormaps = None
-    layer_set = None
 
     _current_tool = None
     _color_choices = None
 
     # FIXME: many more undocumented member variables
 
     didRetilingCalcs = pyqtSignal(object, object, object, object, object, object)
-    didChangeFrame = pyqtSignal(tuple)
-    didChangeLayerVisibility = pyqtSignal(dict)  # similar to document didChangeLayerVisibility
     newPointProbe = pyqtSignal(str, tuple)
-    newProbePolygon = pyqtSignal(object, object)
-
-    def __init__(self, doc, workspace, queue,
-                 border_shapefile=None, states_shapefile=None,
-                 parent=None, texture_shape=(4, 16), center=None):
+    # REMARK: PyQT tends to fail if a signal with an argument of type 'list' is
+    # passed an empty list or the 'None' object. By declaring the signal as
+    # having an argument of type 'object' is avoided.
+    newProbePolygon = pyqtSignal(object)
+
+    def __init__(
+        self,
+        doc,
+        workspace,
+        queue,
+        borders_shapefiles: Optional[list] = None,
+        states_shapefile=None,
+        parent=None,
+        texture_shape=(4, 16),
+        center=None,
+    ):
         super(SceneGraphManager, self).__init__(parent)
         self.didRetilingCalcs.connect(self._set_retiled)
 
         # Parent should be the Qt widget that this GLCanvas belongs to
-        self.document = doc
-        self.workspace = workspace
+        self.document = doc  # Document object we work with
+        self.workspace = workspace  # where we get data arrays from
         self.queue = queue
-        self.border_shapefile = border_shapefile or DEFAULT_SHAPE_FILE
-        self.conus_states_shapefile = states_shapefile or DEFAULT_STATES_SHAPE_FILE
+        self.borders_shapefiles = borders_shapefiles or [DEFAULT_SHAPE_FILE, DEFAULT_STATES_SHAPE_FILE]
         self.texture_shape = texture_shape
-        self.polygon_probes = {}
-        self.point_probes = {}
+        self.polygon_probes: dict = {}
+        self.point_probes: dict = {}
+
+        self.layer_nodes: dict = {}  # {layer_uuid: layer_node}
+        self.dataset_nodes: dict = {}  # {dataset_uuid: dataset_node}
+        self.latlon_grid_node: Optional[Line] = None
+        self.borders_nodes: list = []
+
+        self.composite_element_dependencies: dict = {}  # {dataset_uuid:set-of-dependent-uuids}
+        self.animation_controller = AnimationController()
 
-        self.image_elements = {}
-        self.composite_element_dependencies = {}
-        self.layer_set = LayerSet(self, frame_change_cb=self.frame_changed)
         self._current_tool = None
 
         self._connect_doc_signals(self.document)
 
         # border and lat/lon grid color choices
         self._color_choices = [
-            np.array([1., 1., 1., 1.], dtype=np.float32),  # white
-            np.array([.5, .5, .5, 1.], dtype=np.float32),  # gray
-            np.array([0., 1., 1., 1.], dtype=np.float32),  # cyan
-            np.array([0., 0., 0., 1.], dtype=np.float32),  # black
-            np.array([0., 0., 0., 0.], dtype=np.float32),  # transparent
+            np.array([1.0, 1.0, 1.0, 1.0], dtype=np.float32),  # white
+            np.array([0.5, 0.5, 0.5, 1.0], dtype=np.float32),  # gray
+            np.array([0.0, 1.0, 1.0, 1.0], dtype=np.float32),  # cyan
+            np.array([0.0, 0.0, 0.0, 1.0], dtype=np.float32),  # black
+            np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32),  # transparent
         ]
+        self._latlon_grid_color_idx = 1
+        self._borders_color_idx = 0
 
-        self.setup_initial_canvas(center)
+        self._setup_initial_canvas(center)
         self.pending_polygon = PendingPolygon(self.main_map)
 
     def get_screenshot_array(self, frame_range=None):
         """Get numpy arrays representing the current canvas."""
+
         if frame_range is None:
             self.main_canvas.on_draw(None)
-            return [(self.layer_set.top_layer_uuid(), _screenshot())]
-        s, e = frame_range
+            return [("", _screenshot())]
+        else:
+            s, e = frame_range
+        # Store current index to reset the view once we are done
+        current_frame = self.animation_controller.get_current_frame_index()
 
-        # reset the view once we are done
-        c = self.layer_set.current_frame
         images = []
         for i in range(s, e + 1):
-            self.set_frame_number(i)
-            self.update()
+            self.animation_controller.jump(i)
+            self._update()
             self.main_canvas.on_draw(None)
-            u = self.layer_set.frame_order[i] if self.layer_set.frame_order else None
+            u = self.animation_controller.get_current_frame_uuid()
             images.append((u, _screenshot()))
-        self.set_frame_number(c)
-        self.update()
+
+        self.animation_controller.jump(current_frame)
+        self._update()
         self.main_canvas.on_draw(None)
         return images
 
-    def frame_changed(self, frame_info):
-        """Callback which emits information on current animation frame as a signal (see LayerSet.next_frame)
-
-        Args:
-            frame_info (tuple): to be relayed in the signal.
-                Typically (frame_index:int, total_frames:int, animating:bool, frame_id:UUID)
-
-        """
-        # LOG.debug('emitting didChangeFrame')
-        self.didChangeFrame.emit(frame_info)
-        is_animating = frame_info[2]
-        if not is_animating:
-            # emit a signal equivalent to document's didChangeLayerVisibility,
-            # except that visibility is being changed by animation interactions
-            # only do this when we're not animating, however
-            # watch out for signal loops!
-            uuids = self.layer_set.frame_order
-            # note that all the layers in the layer_order but the current one are now invisible
-            vis = dict((u, u == frame_info[3]) for u in uuids)
-            self.didChangeLayerVisibility.emit(vis)
-
-    def setup_initial_canvas(self, center=None):
+    def _setup_initial_canvas(self, center=None):
         self.main_canvas = SIFTMainMapCanvas(parent=self.parent())
-        self.main_view = self.main_canvas.central_widget.add_view()
+        self.main_view = self.main_canvas.central_widget.add_view(name="MainView")
 
         # Camera Setup
-        self.pz_camera = PanZoomProbeCamera(name=Tool.PAN_ZOOM.name, aspect=1, pan_limits=(-1., -1., 1., 1.),
-                                            zoom_limits=(0.0015, 0.0015))
+        self.pz_camera = PanZoomProbeCamera(
+            name=Tool.PAN_ZOOM.name, aspect=1, pan_limits=(-1.0, -1.0, 1.0, 1.0), zoom_limits=(0.0015, 0.0015)
+        )
         self.main_view.camera = self.pz_camera
         self.main_view.camera.flip = (False, False, False)
         self.main_view.events.mouse_press.connect(self.on_mouse_press_point)
         self.main_view.events.mouse_press.connect(self.on_mouse_press_region)
         self.change_tool(Tool.PAN_ZOOM)
 
         z_level_transform = MatrixTransform()
         # near/far is backwards it seems:
         camera_z_scale = 1e-6
-        z_level_transform.set_ortho(-1., 1., -1., 1., -100.0 * camera_z_scale, 100.0 * camera_z_scale)
+        z_level_transform.set_ortho(-1.0, 1.0, -1.0, 1.0, -100.0 * camera_z_scale, 100.0 * camera_z_scale)
 
         # Head node of all visualizations, needed mostly to scale Z level
         self.main_map_parent = scene.Node(name="HeadNode", parent=self.main_view.scene)
         self.main_map_parent.transform = z_level_transform
 
         # Head node of the map graph
-        proj_info = self.document.projection_info()
         self.main_map = MainMap(name="MainMap", parent=self.main_map_parent)
-        self.main_map.transform = PROJ4Transform(proj_info['proj4_str'])
-        self.proxy_nodes = {}
-
-        self._borders_color_idx = 0
-        self.borders = NEShapefileLines(self.border_shapefile, double=True,
-                                        color=self._color_choices[self._borders_color_idx], parent=self.main_map)
-        self.borders.transform = STTransform(translate=(0, 0, 40))
-        self.conus_states = NEShapefileLines(self.conus_states_shapefile, double=True,
-                                             color=self._color_choices[self._borders_color_idx], parent=self.main_map)
-        self.conus_states.transform = STTransform(translate=(0, 0, 45))
 
-        self._latlon_grid_color_idx = 1
-        self.latlon_grid = self._init_latlon_grid_layer(color=self._color_choices[self._latlon_grid_color_idx])
-        self.latlon_grid.transform = STTransform(translate=(0, 0, 45))
-
-        self.create_test_image()
+        self._create_test_image()
 
-        # Make the camera center on Guam
-        # center = (144.8, 13.5)
-        center = center or proj_info["default_center"]
-        width = proj_info["default_width"] / 2.
-        height = proj_info["default_height"] / 2.
-        ll_xy = self.borders.transforms.get_transform(map_to="scene").map(
-            [(center[0] - width, center[1] - height)])[0][:2]
-        ur_xy = self.borders.transforms.get_transform(map_to="scene").map(
-            [(center[0] + width, center[1] + height)])[0][:2]
-        self.main_view.camera.rect = Rect(ll_xy, (ur_xy[0] - ll_xy[0], ur_xy[1] - ll_xy[1]))
+        area_def = self.document.area_definition()
+        self._set_projection(area_def)
 
-    def create_test_image(self):
+    def _create_test_image(self):
         proj4_str = os.getenv("SIFT_DEBUG_IMAGE_PROJ", None)
         if proj4_str is None:
             return
         shape = (2000, 2000)
         fake_data = np.zeros(shape, np.float32) + 0.5
-        fake_data[:5, :] = 1.
-        fake_data[-5:, :] = 1.
-        fake_data[:, :5] = 1.
-        fake_data[:, -5:] = 1.
+        fake_data[:5, :] = 1.0
+        fake_data[-5:, :] = 1.0
+        fake_data[:, :5] = 1.0
+        fake_data[:, -5:] = 1.0
         cell_size = 1000
-        origin_x = -shape[1] / 2. * cell_size
-        origin_y = shape[0] / 2. * cell_size
+        origin_x = -shape[1] / 2.0 * cell_size
+        origin_y = shape[0] / 2.0 * cell_size
 
         image = TiledGeolocatedImage(
             fake_data,
             origin_x,
             origin_y,
             cell_size,
             cell_size,
             name="Test Image",
-            clim=(0., 1.),
-            gamma=1.,
-            interpolation='nearest',
-            method='subdivide',
-            cmap=self.document.find_colormap('grays'),
+            clim=(0.0, 1.0),
+            gamma=1.0,
+            interpolation="nearest",
+            method="subdivide",
+            cmap=self.document.find_colormap("grays"),
             double=False,
             texture_shape=DEFAULT_TEXTURE_SHAPE,
             wrap_lon=False,
             parent=self.main_map,
             projection=proj4_str,
         )
         image.transform = PROJ4Transform(proj4_str, inverse=True)
         image.transform *= STTransform(translate=(0, 0, -50.0))
         self._test_img = image
 
-    def set_projection(self, projection_name, proj_info, center=None):
-        self.main_map.transform = PROJ4Transform(proj_info['proj4_str'])
-        center = center or proj_info["default_center"]
-        width = proj_info["default_width"] / 2.
-        height = proj_info["default_height"] / 2.
-        ll_xy = self.borders.transforms.get_transform(map_to="scene").map(
-            [(center[0] - width, center[1] - height)])[0][:2]
-        ur_xy = self.borders.transforms.get_transform(map_to="scene").map(
-            [(center[0] + width, center[1] + height)])[0][:2]
-        self.main_view.camera.rect = Rect(ll_xy, (ur_xy[0] - ll_xy[0], ur_xy[1] - ll_xy[1]))
-        for img in self.image_elements.values():
-            if hasattr(img, 'determine_reference_points'):
-                img.determine_reference_points()
+    def set_projection(self, area_display_name: str, center=None):
+        area_def = AreaDefinitionsManager.area_def_by_name(area_display_name)
+        assert area_def is not None  # nosec B101
+        self._set_projection(area_def, center)
+
+        for dataset_node in self.dataset_nodes.values():
+            if hasattr(dataset_node, "determine_reference_points"):
+                dataset_node.determine_reference_points()
         self.on_view_change(None)
 
-    def _init_latlon_grid_layer(self, color=None, resolution=5.):
+    def _set_projection(self, area_def: AreaDefinition, center=None):
+        self.main_map.transform = PROJ4Transform(area_def.proj_str)
+
+        ll_xy = area_def.area_extent[:2]
+        ur_xy = area_def.area_extent[2:]
+
+        # FIXME: This method is called via setup_initial_canvas() before the
+        #  system layer(s) and their nodes (here: self.latlon_grid_node) have
+        #  been initialized.  When 'center' is not None in that case
+        #  calculating 'mapped_center' will crash. Therefore as long as there
+        #  is no solution for the next FIX-ME this must be prevented by
+        #  revising the application setup process.  For the moment, we assume
+        #  that no one wants to use 'center' already when the application is
+        #  started and therefore we ...
+        assert center is None or self.latlon_grid_node is not None  # nosec B101
+
+        if center:
+            # FIXME: We should be able to use the main_map object to do the
+            #  transform but it doesn't work (waiting on vispy developers)
+            # mapped_center = self.main_map.transforms\
+            #    .get_transform(map_to="scene").map([center])[0][:2]
+            assert self.latlon_grid_node is not None  # nosec B101 # suppress mypy [union-attr]
+            mapped_center = self.latlon_grid_node.transforms.get_transform(map_to="scene").map([center])[0][:2]
+            ll_xy += mapped_center
+            ur_xy += mapped_center
+
+        self.main_view.camera.rect = Rect(ll_xy, (ur_xy[0] - ll_xy[0], ur_xy[1] - ll_xy[1]))
+
+    @staticmethod
+    def _create_latlon_grid_points(resolution=5.0):
         """Create a series of line segments representing latitude and longitude lines.
 
         :param resolution: number of degrees between lines
         """
-        lons = np.arange(-180., 180. + resolution, resolution, dtype=np.float32)
-        lats = np.arange(-90., 90. + resolution, resolution, dtype=np.float32)
+        lons = np.arange(-180.0, 180.0 + resolution, resolution, dtype=np.float32)
+        lats = np.arange(-90.0, 90.0 + resolution, resolution, dtype=np.float32)
 
         # One long line of lawn mower pattern (lon lines, then lat lines)
         points = np.empty((lons.shape[0] * lats.shape[0] * 2, 2), np.float32)
         LOG.debug("Generating longitude lines...")
         for idx, lon_point in enumerate(lons):
-            points[idx * lats.shape[0]:(idx + 1) * lats.shape[0], 0] = lon_point
+            points[idx * lats.shape[0] : (idx + 1) * lats.shape[0], 0] = lon_point
             if idx % 2 == 0:
-                points[idx * lats.shape[0]:(idx + 1) * lats.shape[0], 1] = lats
+                points[idx * lats.shape[0] : (idx + 1) * lats.shape[0], 1] = lats
             else:
-                points[idx * lats.shape[0]:(idx + 1) * lats.shape[0], 1] = lats[::-1]
+                points[idx * lats.shape[0] : (idx + 1) * lats.shape[0], 1] = lats[::-1]
         start_idx = lons.shape[0] * lats.shape[0]
         LOG.debug("Generating latitude lines...")
         for idx, lat_point in enumerate(lats[::-1]):
-            points[start_idx + idx * lons.shape[0]:start_idx + (idx + 1) * lons.shape[0], 1] = lat_point
+            points[start_idx + idx * lons.shape[0] : start_idx + (idx + 1) * lons.shape[0], 1] = lat_point
             if idx % 2 == 0:
-                points[start_idx + idx * lons.shape[0]:start_idx + (idx + 1) * lons.shape[0], 0] = lons
+                points[start_idx + idx * lons.shape[0] : start_idx + (idx + 1) * lons.shape[0], 0] = lons
             else:
-                points[start_idx + idx * lons.shape[0]:start_idx + (idx + 1) * lons.shape[0], 0] = lons[::-1]
+                points[start_idx + idx * lons.shape[0] : start_idx + (idx + 1) * lons.shape[0], 0] = lons[::-1]
 
         # Repeat for "second" size of the earth (180 to 540)
         offset = 360  # box_x[lons.shape[0] - 1] - box_x[0]
         points2 = np.empty((points.shape[0] * 2, 2), dtype=np.float32)
-        points2[:points.shape[0], :] = points
-        points2[points.shape[0]:, :] = points
-        points2[points.shape[0]:, 0] += offset
-
-        # return Line(pos=points2, connect="segments", color=color, parent=self.main_map)
-        return Line(pos=points2, connect="strip", color=color, parent=self.main_map)
-        # return Line(pos=points, connect="strip", color=color, parent=self.main_map)
+        points2[: points.shape[0], :] = points
+        points2[points.shape[0] :, :] = points
+        points2[points.shape[0] :, 0] += offset
+
+        return points2
 
     def on_mouse_press_point(self, event):
-        """Handle mouse events that mean we are using the point probe.
-        """
+        """Handle mouse events that mean we are using the point probe."""
         if event.handled:
             return
         modifiers = event.mouse_event.modifiers
         if (event.button == 2 and not modifiers) or (self._current_tool == Tool.POINT_PROBE and event.button == 1):
             buffer_pos = event.sources[0].transforms.get_transform().map(event.pos)
             # FIXME: We should be able to use the main_map object to do the transform
             #  but it doesn't work (waiting on vispy developers)
             # map_pos = self.main_map.transforms.get_transform().imap(buffer_pos)
-            map_pos = self.borders.transforms.get_transform().imap(buffer_pos)
+            map_pos = self.latlon_grid_node.transforms.get_transform().imap(buffer_pos)
             if np.any(np.abs(map_pos[:2]) > 1e25):
                 LOG.error("Invalid point probe location")
                 return
             self.newPointProbe.emit(DEFAULT_POINT_PROBE, tuple(map_pos[:2]))
 
     def on_mouse_press_region(self, event):
-        """Handle mouse events that mean we are using the point probe.
-        """
+        """Handle mouse events that mean we are using the point probe."""
         if event.handled:
             return
         modifiers = event.mouse_event.modifiers
         if (event.button == 2 and modifiers == (SHIFT,)) or (
-                self._current_tool == Tool.REGION_PROBE and event.button == 1):
+            self._current_tool == Tool.REGION_PROBE and event.button == 1
+        ):
             buffer_pos = event.sources[0].transforms.get_transform().map(event.pos)
-            map_pos = self.borders.transforms.get_transform().imap(buffer_pos)
+            # FIXME: We should be able to use the main_map object to do the transform
+            #  but it doesn't work (waiting on vispy developers)
+            # map_pos = self.main_map.transforms.get_transform().imap(buffer_pos)
+            map_pos = self.latlon_grid_node.transforms.get_transform().imap(buffer_pos)
             if np.any(np.abs(map_pos[:2]) > 1e25):
                 LOG.error("Invalid region probe location")
                 return
             if self.pending_polygon.add_point(event.pos[:2], map_pos[:2], 60):
                 points = self.pending_polygon.points + [self.pending_polygon.points[0]]
                 self.clear_pending_polygon()
-                self.newProbePolygon.emit(self.layer_set.top_layer_uuid(), points)
+                self.newProbePolygon.emit(points)
 
     def clear_pending_polygon(self):
         for marker in self.pending_polygon.markers:
             # Remove the marker from the scene graph
             marker.parent = None
         # Reset the pending polygon object
         self.pending_polygon.reset()
 
     def remove_polygon(self, name=None):
-        """Remove a polygon from the SGM or clear the pending polygon if it exists.
-        """
+        """Remove a polygon from the SGM or clear the pending polygon if it exists."""
         if name is None:
             LOG.debug("No polygon name specified to remove")
             return
 
         if name not in self.polygon_probes:
             LOG.warning("Tried to remove a nonexistent polgyon: %s", name)
             return
@@ -745,44 +536,46 @@
         del self.polygon_probes[name]
 
     def has_pending_polygon(self):
         return len(self.pending_polygon.points) != 0
 
     def on_point_probe_set(self, probe_name, state, xy_pos, **kwargs):
         z = float(kwargs.get("z", 60))
-        edge_color = kwargs.get("edge_color", np.array([1.0, 0.5, 0.5, 1.]))
-        face_color = kwargs.get("face_color", np.array([0.5, 0., 0., 1.]))
+        edge_color = kwargs.get("edge_color", np.array([1.0, 0.5, 0.5, 1.0]))
+        face_color = kwargs.get("face_color", np.array([0.5, 0.0, 0.0, 1.0]))
         if len(xy_pos) == 2:
             xy_pos = [xy_pos[0], xy_pos[1], z]
 
         probe_kwargs = {
-            'symbol': 'disc',
-            'pos': np.array([xy_pos]),
-            'face_color': face_color,
-            'edge_color': edge_color,
-            'size': 18.,
-            'edge_width': 3.,
+            "symbol": "disc",
+            "pos": np.array([xy_pos]),
+            "face_color": face_color,
+            "edge_color": edge_color,
+            "size": 18.0,
+            "edge_width": 3.0,
         }
 
         if probe_name not in self.point_probes and xy_pos is None:
             raise ValueError("Probe '{}' does not exist".format(probe_name))
         elif probe_name not in self.point_probes:
-            # point_visual = FakeMarker(parent=self.main_map, symbol="x", pos=np.array([xy_pos]), color=color)
             point_visual = Markers(parent=self.main_map, name=probe_name, **probe_kwargs)
             self.point_probes[probe_name] = point_visual
         else:
             point_visual = self.point_probes[probe_name]
-            # point_visual.set_point(xy_pos)
             point_visual.set_data(**probe_kwargs)
 
         # set the Point visible or not
         point_visual.visible = state
 
     def on_new_polygon(self, probe_name, points, **kwargs):
-        kwargs.setdefault("color", (1.0, 0.0, 1.0, 0.5))
+        points = np.array(points, dtype=np.float32)  # convert list to NumPy array
+
+        kwargs.setdefault("color", None)
+        kwargs.setdefault("border_color", (1.0, 0.0, 1.0, 1.0))
+
         # marker default is 60, polygon default is 50 so markers can be put on top of polygons
         z = float(kwargs.get("z", 50))
         poly = Polygon(parent=self.main_map, pos=points, **kwargs)
         poly.order = 50  # set polygons to be drawn last (stops 'see through' polygons)
         poly.transform = STTransform(translate=(0, 0, z))
         if probe_name in self.polygon_probes:
             self.polygon_probes[probe_name].parent = None
@@ -792,48 +585,45 @@
         self.on_new_polygon(new_name, self.polygon_probes[old_name].pos)
 
     def show_only_polygons(self, polygon_names_to_show):
         temp_set = set(polygon_names_to_show)
         for polygon_name in self.polygon_probes.keys():
             self.polygon_probes[polygon_name].visible = polygon_name in temp_set
 
-    def update(self):
+    def _update(self):
         return self.main_canvas.update()
 
     def cycle_borders_color(self):
         self._borders_color_idx = (self._borders_color_idx + 1) % len(self._color_choices)
         if self._borders_color_idx + 1 == len(self._color_choices):
-            self.borders.visible = False
-            self.conus_states.visible = False
+            for borders_node in self.borders_nodes:
+                borders_node.visible = False
         else:
-            self.borders.set_data(color=self._color_choices[self._borders_color_idx])
-            self.borders.visible = True
-            self.conus_states.set_data(color=self._color_choices[self._borders_color_idx])
-            self.conus_states.visible = True
+            for borders_node in self.borders_nodes:
+                borders_node.set_data(color=self._color_choices[self._borders_color_idx])
+                borders_node.visible = True
 
-    def cycle_grid_color(self):
+    def cycle_latlon_grid_color(self):
         self._latlon_grid_color_idx = (self._latlon_grid_color_idx + 1) % len(self._color_choices)
         if self._latlon_grid_color_idx + 1 == len(self._color_choices):
-            self.latlon_grid.visible = False
+            self.latlon_grid_node.visible = False
         else:
-            self.latlon_grid.set_data(color=self._color_choices[self._latlon_grid_color_idx])
-            self.latlon_grid.visible = True
+            self.latlon_grid_node.set_data(color=self._color_choices[self._latlon_grid_color_idx])
+            self.latlon_grid_node.visible = True
 
-    def change_tool(self, name):
+    def change_tool(self, name: Tool):
         prev_tool = self._current_tool
         if name == prev_tool:
             # it's the same tool
             return
 
         self._current_tool = name
 
         # Set the cursor
         if name == Tool.PAN_ZOOM:
-            # self.main_canvas.native.setCursor(QCursor(QPixmap("py/uwsift/ui/cursors/noun_275_cc.png")))
-            # self.main_canvas.native.setCursor(QCursor(Qt.SizeAllCursor))
             self.main_canvas.native.setCursor(QCursor(Qt.OpenHandCursor))
         elif name == Tool.POINT_PROBE:
             self.main_canvas.native.setCursor(QCursor(Qt.PointingHandCursor))
         elif name == Tool.REGION_PROBE:
             self.main_canvas.native.setCursor(QCursor(Qt.CrossCursor))
 
         # disconnect the previous signals (if needed)
@@ -852,460 +642,769 @@
 
     def next_tool(self):
         tool_names = list(Tool)
         idx = tool_names.index(self._current_tool)
         idx = (idx + 1) % len(tool_names)
         self.change_tool(tool_names[idx])
 
-    def set_colormap(self, colormap, uuid=None):
+    def _set_colormap(self, colormap, uuid=None):
         colormap = self.document.find_colormap(colormap)
 
         uuids = uuid
         if uuid is None:
-            uuids = self.image_elements.keys()
+            uuids = self.dataset_nodes.keys()
         elif not isinstance(uuid, (list, tuple)):
             uuids = [uuid]
 
         for uuid in uuids:
-            layer = self.image_elements[uuid]
-            if isinstance(layer, TiledGeolocatedImage):
-                self.image_elements[uuid].cmap = colormap
+            dataset_node = self.dataset_nodes[uuid]
+            if isinstance(dataset_node, TiledGeolocatedImage) or isinstance(dataset_node, Image):
+                self.dataset_nodes[uuid].cmap = colormap
             else:
-                self.image_elements[uuid].color = colormap
+                self.dataset_nodes[uuid].color = colormap
 
-    def set_color_limits(self, clims, uuid=None):
-        """Update the color limits for the specified UUID
-        """
+    def _set_color_limits(self, clims, uuid=None):
+        """Update the color limits for the specified UUID"""
         uuids = uuid
         if uuid is None:
-            uuids = self.image_elements.keys()
+            uuids = self.dataset_nodes.keys()
         elif not isinstance(uuid, (list, tuple)):
             uuids = [uuid]
 
         for uuid in uuids:
-            element = self.image_elements.get(uuid, None)
-            if element is not None:
-                self.image_elements[uuid].clim = clims
+            dataset_node = self.dataset_nodes.get(uuid, None)
+            if dataset_node is not None:
+                self.dataset_nodes[uuid].clim = clims
 
-    def set_gamma(self, gamma, uuid):
+    def _set_gamma(self, gamma, uuid):
         uuids = uuid
         if uuid is None:
-            uuids = self.image_elements.keys()
+            uuids = self.dataset_nodes.keys()
         elif not isinstance(uuid, (list, tuple)):
             uuids = [uuid]
 
         for uuid in uuids:
-            element = self.image_elements.get(uuid, None)
-            if element is not None and hasattr(element, 'gamma'):
-                self.image_elements[uuid].gamma = gamma
+            dataset_node = self.dataset_nodes.get(uuid, None)
+            if dataset_node is not None and hasattr(dataset_node, "gamma"):
+                self.dataset_nodes[uuid].gamma = gamma
 
-    def change_layers_colormap(self, change_dict):
+    def change_dataset_nodes_colormap(self, change_dict):
         for uuid, cmapid in change_dict.items():
-            LOG.info('changing {} to colormap {}'.format(uuid, cmapid))
-            self.set_colormap(cmapid, uuid)
+            LOG.info("changing {} to colormap {}".format(uuid, cmapid))
+            self._set_colormap(cmapid, uuid)
 
-    def change_layers_color_limits(self, change_dict):
+    def change_dataset_nodes_color_limits(self, change_dict):
         for uuid, clims in change_dict.items():
-            LOG.debug('changing {} to color limits {}'.format(uuid, clims))
-            self.set_color_limits(clims, uuid)
+            LOG.debug("changing {} to color limits {}".format(uuid, clims))
+            self._set_color_limits(clims, uuid)
 
-    def change_layers_gamma(self, change_dict):
+    def change_dataset_nodes_gamma(self, change_dict):
         for uuid, gamma in change_dict.items():
-            LOG.debug('changing {} to gamma {}'.format(uuid, gamma))
-            self.set_gamma(gamma, uuid)
+            LOG.debug("changing {} to gamma {}".format(uuid, gamma))
+            self._set_gamma(gamma, uuid)
 
-    def change_layers_image_kind(self, change_dict):
-        for uuid, new_pz in change_dict.items():
-            LOG.info('changing {} to kind {}'.format(uuid, new_pz.kind.name))
-            self.add_basic_layer(None, uuid, new_pz)
-
-    def add_contour_layer(self, layer: DocBasicLayer, p: Presentation, overview_content: np.ndarray):
-        verts = overview_content[:, :2]
-        connects = overview_content[:, 2].astype(np.bool)
-        level_indexes = overview_content[:, 3]
-        level_indexes = level_indexes[~np.isnan(level_indexes)].astype(np.int)
-        levels = layer["contour_levels"]
-        cmap = self.document.find_colormap(p.colormap)
-
-        proj4_str = layer[Info.PROJ]
-        parent = self.proxy_nodes.get(proj4_str)
-        if parent is None:
-            parent = ContourGroupNode(parent=self.main_map)
-            parent.transform = PROJ4Transform(layer[Info.PROJ], inverse=True)
-            self.proxy_nodes[proj4_str] = parent
-
-        contour_visual = PrecomputedIsocurve(verts, connects, level_indexes,
-                                             levels=levels, color_lev=cmap,
-                                             clim=p.climits,
-                                             parent=parent,
-                                             name=str(layer[Info.UUID]))
-        contour_visual.transform *= STTransform(translate=(0, 0, -50.0))
-        self.image_elements[layer[Info.UUID]] = contour_visual
-        self.layer_set.add_layer(contour_visual)
-        self.on_view_change(None)
+    def change_layer_visible(self, layer_uuid: UUID, visible: bool):
+        self.layer_nodes[layer_uuid].visible = visible
 
-    def add_basic_layer(self, new_order: tuple, uuid: UUID, p: Presentation):
-        layer = self.document[uuid]
-        # create a new layer in the imagelist
-        if not layer.is_valid:
-            LOG.warning('unable to add an invalid layer, will try again later when layer changes')
-            return
-        if layer[Info.UUID] in self.image_elements:
-            image = self.image_elements[layer[Info.UUID]]
-            if p.kind == Kind.CONTOUR and isinstance(image, PrecomputedIsocurve):
-                LOG.warning("Contour layer already exists in scene")
-                return
-            if p.kind == Kind.IMAGE and isinstance(image, TiledGeolocatedImage):
-                LOG.warning("Image layer already exists in scene")
-                return
-            # we already have an image layer for it and it isn't what we want
-            # remove the existing image object and create the proper type now
-            image.parent = None
-            del self.image_elements[layer[Info.UUID]]
-
-        image_data = self.workspace.get_content(layer.uuid, kind=p.kind)
-        if p.kind == Kind.CONTOUR:
-            return self.add_contour_layer(layer, p, image_data)
+    def change_layer_opacity(self, layer_uuid: UUID, opacity: float):
+        # According to
+        #   https://vispy.org/api/vispy.scene.node.html#vispy.scene.node.Node.parent
+        # this should be sufficient, but it seems to be not:
+        #   self.layer_nodes[uuid].opacity = opacity
+        # Thus opacity must be set for all layer node children:
+        for child in self.layer_nodes[layer_uuid].children:
+            child.opacity = opacity
+        # TODO in case a dataset has its own Presentation simply overwriting
+        #  the opacity of the 'child' node representing it is wrong:
+        #  opacities have to be mixed then. This cannot be done here though
+        self._update()
 
-        image = TiledGeolocatedImage(
-            image_data,
-            layer[Info.ORIGIN_X],
-            layer[Info.ORIGIN_Y],
-            layer[Info.CELL_WIDTH],
-            layer[Info.CELL_HEIGHT],
-            name=str(uuid),
-            clim=p.climits,
-            gamma=p.gamma,
-            interpolation='nearest',
-            method='subdivide',
-            cmap=self.document.find_colormap(p.colormap),
-            double=False,
-            texture_shape=DEFAULT_TEXTURE_SHAPE,
-            wrap_lon=False,
-            parent=self.main_map,
-            projection=layer[Info.PROJ],
+    def change_dataset_visible(self, dataset_uuid: UUID, visible: bool):
+        self.dataset_nodes[dataset_uuid].visible = visible
+
+    @staticmethod
+    def _overwrite_with_test_pattern(data):
+        """
+        Fill given data with distinct test data.
+
+        Fill the given data array with zeros except for some selected cells
+        which are set to distinct values: 5 cells at each corner and 6 cells
+        around the center which form asymmetrical patterns to make them clearly
+        distinguishable.
+
+        When the data is visualized with the color table 'Rainbow (IR Default)'
+        these cells are colored as named in Enum RainbowValue.
+
+        This function must only be called during development for calibration/
+        validation purposes.
+        """
+        max_x, max_y = data.shape
+        data[:, :] = 0
+
+        class RainbowValue(Enum):
+            BROWN = 320.0
+            RED = 300.0
+            LIGHT_GREEN = 280.0
+            GREEN = 260.0
+            LIGHT_BLUE = 240.0
+            DARK_BLUE = 220.0
+            PINK = 200.0
+
+        center_x, center_y = max_x // 2, max_y // 2
+        pixels = [
+            {"x": center_x, "y": center_y, "color": RainbowValue.RED, "desc": "center"},
+            {"x": center_x - 1, "y": center_y - 1, "color": RainbowValue.GREEN, "desc": "upper left"},
+            {"x": center_x - 1, "y": center_y + 1, "color": RainbowValue.LIGHT_BLUE, "desc": "bottom left"},
+            {"x": center_x - 1, "y": center_y + 2, "color": RainbowValue.LIGHT_GREEN, "desc": "below bottom left"},
+            {"x": center_x + 1, "y": center_y - 1, "color": RainbowValue.DARK_BLUE, "desc": "upper right"},
+            {"x": center_x + 1, "y": center_y + 1, "color": RainbowValue.PINK, "desc": "bottom right"},
+            {"x": max_x - 1, "y": max_y - 1, "color": RainbowValue.RED, "desc": "bottom right corner"},
+            {"x": max_x - 2, "y": max_y - 1, "color": RainbowValue.GREEN, "desc": "bottom right corner"},
+            {"x": max_x - 3, "y": max_y - 1, "color": RainbowValue.LIGHT_BLUE, "desc": "bottom right corner"},
+            {"x": max_x - 1, "y": max_y - 2, "color": RainbowValue.PINK, "desc": "bottom right corner"},
+            {"x": max_x - 2, "y": max_y - 2, "color": RainbowValue.BROWN, "desc": "bottom right corner"},
+            {"x": 0, "y": max_y - 1, "color": RainbowValue.PINK, "desc": "bottom left corner"},
+            {"x": 1, "y": max_y - 1, "color": RainbowValue.LIGHT_BLUE, "desc": "bottom left corner"},
+            {"x": 0, "y": max_y - 2, "color": RainbowValue.BROWN, "desc": "bottom left corner"},
+            {"x": 0, "y": max_y - 3, "color": RainbowValue.RED, "desc": "bottom left corner"},
+            {"x": 1, "y": max_y - 2, "color": RainbowValue.GREEN, "desc": "bottom left corner"},
+            {"x": max_x - 1, "y": 0, "color": RainbowValue.LIGHT_BLUE, "desc": "upper right corner"},
+            {"x": max_x - 2, "y": 0, "color": RainbowValue.LIGHT_GREEN, "desc": "upper right corner"},
+            {"x": max_x - 3, "y": 0, "color": RainbowValue.RED, "desc": "upper right corner"},
+            {"x": max_x - 1, "y": 1, "color": RainbowValue.BROWN, "desc": "upper right corner"},
+            {"x": max_x - 2, "y": 1, "color": RainbowValue.PINK, "desc": "upper right corner"},
+            {"x": 0, "y": 0, "color": RainbowValue.BROWN, "desc": "upper left corner"},
+            {"x": 1, "y": 0, "color": RainbowValue.RED, "desc": "upper left corner"},
+            {"x": 0, "y": 1, "color": RainbowValue.PINK, "desc": "upper left corner"},
+            {"x": 0, "y": 2, "color": RainbowValue.DARK_BLUE, "desc": "upper left corner"},
+            {"x": 1, "y": 1, "color": RainbowValue.LIGHT_GREEN, "desc": "upper left corner"},
+        ]
+
+        for pixel in pixels:
+            data[pixel["y"], pixel["x"]] = pixel["color"].value
+
+        return data
+
+    def add_node_for_layer(self, layer: LayerItem):
+        if IMAGE_DISPLAY_MODE == ImageDisplayMode.PIXEL_MATRIX and layer.kind in [
+            Kind.IMAGE,
+            Kind.COMPOSITE,
+            Kind.RGB,
+            Kind.MC_IMAGE,
+        ]:
+            # Circumvent all reprojecting transformations
+            layer_node = scene.Node(parent=self.main_map_parent, name=str(layer.uuid))
+        else:
+            # Make child of the node with the reprojecting transform
+            layer_node = scene.Node(parent=self.main_map, name=str(layer.uuid))
+
+        z_transform = STTransform(translate=(0, 0, 0))
+        layer_node.transform = z_transform
+
+        self.layer_nodes[layer.uuid] = layer_node
+
+    def add_node_for_system_generated_data(self, layer: LayerItem):
+        layer_node = self.layer_nodes[layer.uuid]
+        if layer.name == LATLON_GRID_DATASET_NAME:
+            self._build_latlon_grid_node(layer_node)
+        elif layer.name == BORDERS_DATASET_NAME:
+            self._build_borders_nodes(layer_node)
+        else:
+            raise ValueError(f"Unsupported generated layer: {layer.name}")
+
+    def _build_latlon_grid_node(self, layer_node):
+        """Helper function for setting up the VisualNode for the system
+        layer for latitude/longitude grid.
+
+        :param layer_node: Scene graph node to be used as parent for the grid
+                           node.
+        """
+        latlon_grid_resolution = get_configured_latlon_grid_resolution()
+        latlon_grid_points = self._create_latlon_grid_points(resolution=latlon_grid_resolution)
+        self.latlon_grid_node = Line(
+            pos=latlon_grid_points,
+            connect="strip",
+            color=self._color_choices[self._latlon_grid_color_idx],
+            parent=layer_node,
         )
-        image.transform = PROJ4Transform(layer[Info.PROJ], inverse=True)
-        image.transform *= STTransform(translate=(0, 0, -50.0))
-        self.image_elements[uuid] = image
-        self.layer_set.add_layer(image)
-        image.determine_reference_points()
-        self.on_view_change(None)
+        self.latlon_grid_node.set_gl_state("translucent")
 
-    def add_composite_layer(self, new_order: tuple, uuid: UUID, p: Presentation):
-        layer = self.document[uuid]
-        LOG.debug("SceneGraphManager.add_composite_layer %s" % repr(layer))
-        if not layer.is_valid:
-            LOG.info('unable to add an invalid layer, will try again later when layer changes')
-            return
-        if p.kind == Kind.RGB:
-            dep_uuids = r, g, b = [c.uuid if c is not None else None for c in [layer.r, layer.g, layer.b]]
-            image_data = list(self.workspace.get_content(cuuid, kind=Kind.IMAGE) for cuuid in dep_uuids)
-            uuid = layer.uuid
-            LOG.debug("Adding composite layer to Scene Graph Manager with UUID: %s", uuid)
-            self.image_elements[uuid] = element = RGBCompositeLayer(
+    def _build_borders_nodes(self, layer_node):
+        """Helper function for setting up the VisualNodes for the system
+        layer for political borders.
+
+        One node is generated for each file stored in the (currently) internal
+        list of political borders shapefiles.
+
+        :param layer_node: Scene graph node to be used as parent for the
+                           borders node(s).
+        """
+        for shapefile in self.borders_shapefiles:
+            node = NEShapefileLines(
+                shapefile, double=True, color=self._color_choices[self._borders_color_idx], parent=layer_node
+            )
+            node.set_gl_state("translucent")
+            self.borders_nodes.append(node)
+
+    def apply_presentation_to_image_node(
+        self, image: Image, presentation: Presentation, visible: Optional[bool] = None
+    ):
+        """
+        Apply all relevant and set properties (not None) of the given
+        presentation to the given image.
+
+        Visibility can be explicitly overridden, because this is (at least for
+        now) the only property where a dataset may deviate from the layer
+        presentation; it depends on whether the dataset is active in the layer's
+        timeline.
+
+        :param image: the image node which should get the new presentation
+        :param presentation: to apply, usually the presentation of the owning
+               layer
+        :param visible:
+        """
+        if visible is not None:
+            image.visible = visible
+        elif presentation.visible:
+            image.visible = presentation.visible
+
+        if presentation.colormap:
+            image.cmap = self.document.find_colormap(presentation.colormap)
+        if presentation.climits:
+            image.clim = presentation.climits
+        if presentation.gamma:
+            image.gamma = presentation.gamma
+        if presentation.opacity:
+            image.opacity = presentation.opacity
+
+    @staticmethod
+    def _calc_subdivision_grid(dataset_info) -> tuple:
+        grid_cell_width = float(config.get("display.grid_cell_width", DEFAULT_GRID_CELL_WIDTH))
+        grid_cell_height = float(config.get("display.grid_cell_height", DEFAULT_GRID_CELL_HEIGHT))
+
+        if "longlat" in dataset_info[Info.PROJ]:
+            # The cell size unit is not metres but degrees, thus we do a rough unit conversion
+            EARTH_CIRCUMFERENCE: float = 40075017.0  # metres
+            pixel_width_metres = abs(dataset_info[Info.CELL_WIDTH]) * EARTH_CIRCUMFERENCE / 360.0
+            pixel_height_metres = abs(dataset_info[Info.CELL_HEIGHT]) * EARTH_CIRCUMFERENCE / 360.0
+
+        else:
+            pixel_width_metres = abs(dataset_info[Info.CELL_WIDTH])
+            pixel_height_metres = abs(dataset_info[Info.CELL_HEIGHT])
+
+        pixels_per_grid_cell_x = round(grid_cell_width / pixel_width_metres)
+        pixels_per_grid_cell_y = round(grid_cell_height / pixel_height_metres)
+
+        num_grid_cells_x = dataset_info[Info.SHAPE][0] // pixels_per_grid_cell_x
+        num_grid_cells_y = dataset_info[Info.SHAPE][1] // pixels_per_grid_cell_y
+
+        actual_grid_cell_width = dataset_info[Info.SHAPE][0] * abs(dataset_info[Info.CELL_WIDTH]) / num_grid_cells_x
+        actual_grid_cell_height = dataset_info[Info.SHAPE][1] * abs(dataset_info[Info.CELL_HEIGHT]) / num_grid_cells_y
+
+        LOG.debug(
+            f"Gridding to ({num_grid_cells_x} x {num_grid_cells_y}) cells"
+            f" with cell size ({actual_grid_cell_width} m, {actual_grid_cell_height} m) "
+        )
+
+        return num_grid_cells_x, num_grid_cells_y
+
+    def add_node_for_image_dataset(self, layer: LayerItem, product_dataset: ProductDataset):
+        assert self.layer_nodes[layer.uuid] is not None  # nosec B101
+        assert product_dataset.kind in [Kind.IMAGE, Kind.COMPOSITE]  # nosec B101
+
+        image_data = self.workspace.get_content(product_dataset.uuid, kind=product_dataset.kind)
+
+        if False:  # Set to True FOR TESTING ONLY DON'T REMOVE!
+            self._overwrite_with_test_pattern(image_data)
+
+        if IMAGE_DISPLAY_MODE == ImageDisplayMode.TILED_GEOLOCATED:
+            image = TiledGeolocatedImage(
                 image_data,
-                layer[Info.ORIGIN_X],
-                layer[Info.ORIGIN_Y],
-                layer[Info.CELL_WIDTH],
-                layer[Info.CELL_HEIGHT],
-                name=str(uuid),
-                clim=p.climits,
-                gamma=p.gamma,
-                interpolation='nearest',
-                method='subdivide',
-                cmap=None,
+                product_dataset.info[Info.ORIGIN_X],
+                product_dataset.info[Info.ORIGIN_Y],
+                product_dataset.info[Info.CELL_WIDTH],
+                product_dataset.info[Info.CELL_HEIGHT],
+                name=str(product_dataset.uuid),
+                interpolation="nearest",
+                method="subdivide",
                 double=False,
                 texture_shape=DEFAULT_TEXTURE_SHAPE,
                 wrap_lon=False,
-                parent=self.main_map,
-                projection=layer[Info.PROJ],
+                parent=self.layer_nodes[layer.uuid],
+                projection=product_dataset.info[Info.PROJ],
+            )
+            image.transform = PROJ4Transform(product_dataset.info[Info.PROJ], inverse=True)
+            image.determine_reference_points()
+        elif IMAGE_DISPLAY_MODE == ImageDisplayMode.SIMPLE_GEOLOCATED:
+            grid = self._calc_subdivision_grid(product_dataset.info)
+            image = Image(
+                image_data,
+                name=str(product_dataset.uuid),
+                interpolation="nearest",
+                method="subdivide",
+                grid=grid,
+                parent=self.layer_nodes[layer.uuid],
+            )
+            image.transform = PROJ4Transform(product_dataset.info[Info.PROJ], inverse=True) * STTransform(
+                scale=(product_dataset.info[Info.CELL_WIDTH], product_dataset.info[Info.CELL_HEIGHT], 1),
+                translate=(product_dataset.info[Info.ORIGIN_X], product_dataset.info[Info.ORIGIN_Y], 0),
             )
-            element.transform = PROJ4Transform(layer[Info.PROJ], inverse=True)
-            element.transform *= STTransform(translate=(0, 0, -50.0))
-            self.composite_element_dependencies[uuid] = dep_uuids
-            self.layer_set.add_layer(element)
-            if new_order:
-                self.layer_set.set_layer_order(new_order)
-            self.on_view_change(None)
-            element.determine_reference_points()
-            self.update()
-            return True
-        elif p.kind in [Kind.COMPOSITE, Kind.IMAGE]:
-            # algebraic layer
-            return self.add_basic_layer(new_order, uuid, p)
-
-    def change_composite_layers(self, new_order: tuple, uuid_list: list, presentations: list):
-        for uuid, presentation in zip(uuid_list, presentations):
-            self.change_composite_layer(None, uuid, presentation)
-        # set the order after we've updated and created all the new layers
-        if new_order:
-            self.layer_set.set_layer_order(new_order)
-
-    def change_composite_layer(self, new_order: tuple, uuid: UUID, presentation: Presentation):
-        layer = self.document[uuid]
-        if presentation.kind == Kind.RGB:
-            if layer.uuid in self.image_elements:
-                if layer.is_valid:
-                    # RGB selection has changed, rebuild the layer
-                    LOG.debug("Changing existing composite layer to Scene Graph Manager with UUID: %s", layer.uuid)
-                    dep_uuids = r, g, b = [c.uuid if c is not None else None for c in [layer.r, layer.g, layer.b]]
-                    image_arrays = list(self.workspace.get_content(cuuid) for cuuid in dep_uuids)
-                    self.composite_element_dependencies[layer.uuid] = dep_uuids
-                    elem = self.image_elements[layer.uuid]
-                    elem.set_channels(image_arrays,
-                                      cell_width=layer[Info.CELL_WIDTH],
-                                      cell_height=layer[Info.CELL_HEIGHT],
-                                      origin_x=layer[Info.ORIGIN_X],
-                                      origin_y=layer[Info.ORIGIN_Y])
-                    elem.clim = presentation.climits
-                    elem.gamma = presentation.gamma
-                    self.on_view_change(None)
-                    elem.determine_reference_points()
-                else:
-                    # layer is no longer valid and has to be removed
-                    LOG.debug("Purging composite ")
-                    self.purge_layer(layer.uuid)
-                self.update()
-            else:
-                if layer.is_valid:
-                    # Add this now valid layer
-                    self.add_composite_layer(new_order, layer.uuid, presentation)
-                else:
-                    LOG.info('unable to add an invalid layer, will try again later when layer changes')
-                    return
         else:
-            raise ValueError("Unknown or unimplemented composite type")
+            image = Image(
+                image_data,
+                name=str(product_dataset.uuid),
+                interpolation="nearest",
+                parent=self.layer_nodes[layer.uuid],
+            )
+            image.transform = STTransform(
+                scale=(product_dataset.info[Info.CELL_WIDTH], product_dataset.info[Info.CELL_HEIGHT], 1),
+                translate=(product_dataset.info[Info.ORIGIN_X], product_dataset.info[Info.ORIGIN_Y], 0),
+            )
+        self.dataset_nodes[product_dataset.uuid] = image
+        # Make sure *all* applicable properties of the owning layer's current
+        # presentation are applied to the new image node
+        self.apply_presentation_to_image_node(image, layer.presentation)
+        self.on_view_change(None)
+        LOG.debug("Scene Graph after IMAGE dataset insertion:")
+        LOG.debug(self.main_view.describe_tree(with_transform=True))
 
-    def remove_layer(self, new_order: tuple, uuids_removed: tuple, row: int, count: int):
-        """
-        remove (disable) a layer, though this may be temporary due to a move.
-        wait for purge to truly flush out this puppy
-        :param new_order:
-        :param uuid_removed:
-        :return:
-        """
-        for uuid_removed in uuids_removed:
-            self.set_layer_visible(uuid_removed, False)
-        # XXX: Used to rebuild_all instead of just update, is that actually needed?
-        # self.rebuild_all()
-
-    def _remove_layer(self, *args, **kwargs):
-        self.remove_layer(*args, **kwargs)
-        # when removing the layer is the only operation being performed then update when we are done
-        self.update()
+    def add_node_for_mc_image_dataset(self, layer: LayerItem, product_dataset: ProductDataset) -> None:
+        """Create and add a new node for a multichannel images to the SceneGraphManager.
 
-    def purge_layer(self, uuid_removed: UUID):
-        """
-        Layer has been purged from document (no longer used anywhere) - flush it all out
-        :param uuid_removed: UUID of the layer that is to be removed
-        :return:
-        """
-        self.set_layer_visible(uuid_removed, False)
-        if uuid_removed in self.image_elements:
-            image_layer = self.image_elements[uuid_removed]
-            image_layer.parent = None
-            del self.image_elements[uuid_removed]
-            LOG.info("layer {} purge from scenegraphmanager".format(uuid_removed))
+        Depending on the system configuration either a node with or without tiling is created from the product_dataset
+        and inserted as a child of the layer's node in the scene graph.
+
+        :param layer: LayerItem which owns the ProductDataset
+        :param product_dataset: ProductDataset to create the multichannel image for
+        """
+        assert self.layer_nodes[layer.uuid] is not None  # nosec B101
+        assert product_dataset.kind == Kind.MC_IMAGE  # nosec B101
+
+        img_data = self.workspace.get_content(product_dataset.uuid, kind=product_dataset.kind)
+
+        if IMAGE_DISPLAY_MODE == ImageDisplayMode.TILED_GEOLOCATED:
+            image = TiledGeolocatedImage(
+                img_data,
+                product_dataset.info[Info.ORIGIN_X],
+                product_dataset.info[Info.ORIGIN_Y],
+                product_dataset.info[Info.CELL_WIDTH],
+                product_dataset.info[Info.CELL_HEIGHT],
+                name=str(product_dataset.uuid),
+                interpolation="nearest",
+                method="subdivide",
+                double=False,
+                #  TODO: (Inform David about the strange behavior)
+                #   workaround (setting clims to (0.0, 1.0) because if no clim is set then default is auto and
+                #   this is not corrcetly resolved before build tiles. But the real clim value is need before
+                #   ImageVisual._build_color_transform() is executed.
+                clim=(0.0, 1.0),
+                texture_shape=DEFAULT_TEXTURE_SHAPE,
+                tile_shape=(DEFAULT_TILE_HEIGHT, DEFAULT_TILE_WIDTH, img_data.shape[2]),
+                wrap_lon=False,
+                parent=self.layer_nodes[layer.uuid],
+                projection=product_dataset.info[Info.PROJ],
+            )
+            image.transform = PROJ4Transform(product_dataset.info[Info.PROJ], inverse=True)
+            image.determine_reference_points()
+        elif IMAGE_DISPLAY_MODE == ImageDisplayMode.SIMPLE_GEOLOCATED:
+            grid = self._calc_subdivision_grid(product_dataset.info)
+            image = Image(
+                img_data,
+                name=str(product_dataset.uuid),
+                interpolation="nearest",
+                method="subdivide",
+                grid=grid,
+                parent=self.layer_nodes[layer.uuid],
+            )
+            image.transform = PROJ4Transform(product_dataset.info[Info.PROJ], inverse=True) * STTransform(
+                scale=(product_dataset.info[Info.CELL_WIDTH], product_dataset.info[Info.CELL_HEIGHT], 1),
+                translate=(product_dataset.info[Info.ORIGIN_X], product_dataset.info[Info.ORIGIN_Y], 0),
+            )
+        else:  # IMAGE_DISPLAY_MODE == ImageDisplayMode.PIXEL_MATRIX
+            image = Image(
+                img_data,
+                name=str(product_dataset.uuid),
+                interpolation="nearest",
+                parent=self.layer_nodes[layer.uuid],
+            )
+            image.transform = STTransform(
+                scale=(product_dataset.info[Info.CELL_WIDTH], product_dataset.info[Info.CELL_HEIGHT], 1),
+                translate=(product_dataset.info[Info.ORIGIN_X], product_dataset.info[Info.ORIGIN_Y], 0),
+            )
+        self.dataset_nodes[product_dataset.uuid] = image
+        self.on_view_change(None)
+        LOG.debug("Scene Graph after MC IMAGE dataset insertion:")
+        LOG.debug(self.main_view.describe_tree(with_transform=True))
+
+    def add_node_for_composite_dataset(self, layer: LayerItem, product_dataset: ProductDataset):
+        assert self.layer_nodes[layer.uuid] is not None  # nosec B101
+        assert product_dataset.kind == Kind.RGB  # nosec B101
+        assert product_dataset.input_datasets_uuids is not None  # nosec B101 # suppress mypy [union-attr]
+
+        images_data = list(
+            self.workspace.get_content(curr_input_uuid, Kind.IMAGE)
+            for curr_input_uuid in product_dataset.input_datasets_uuids
+        )
+
+        if IMAGE_DISPLAY_MODE == ImageDisplayMode.TILED_GEOLOCATED:
+            composite = RGBCompositeImage(
+                images_data,
+                product_dataset.info[Info.ORIGIN_X],
+                product_dataset.info[Info.ORIGIN_Y],
+                product_dataset.info[Info.CELL_WIDTH],
+                product_dataset.info[Info.CELL_HEIGHT],
+                name=str(product_dataset.uuid),
+                clim=layer.presentation.climits,
+                gamma=layer.presentation.gamma,
+                interpolation="nearest",
+                method="subdivide",
+                cmap=None,
+                double=False,
+                texture_shape=DEFAULT_TEXTURE_SHAPE,
+                wrap_lon=False,
+                parent=self.layer_nodes[layer.uuid],
+                projection=product_dataset.info[Info.PROJ],
+            )
+            composite.transform = PROJ4Transform(product_dataset.info[Info.PROJ], inverse=True)
+            composite.determine_reference_points()
+        elif IMAGE_DISPLAY_MODE == ImageDisplayMode.SIMPLE_GEOLOCATED:
+            grid = self._calc_subdivision_grid(product_dataset.info)
+            composite = MultiChannelImage(
+                images_data,
+                name=str(product_dataset.uuid),
+                clim=layer.presentation.climits,
+                gamma=layer.presentation.gamma,
+                interpolation="nearest",
+                method="subdivide",
+                grid=grid,
+                cmap=None,
+                parent=self.layer_nodes[layer.uuid],
+            )
+            composite.transform = PROJ4Transform(product_dataset.info[Info.PROJ], inverse=True) * STTransform(
+                scale=(product_dataset.info[Info.CELL_WIDTH], product_dataset.info[Info.CELL_HEIGHT], 1),
+                translate=(product_dataset.info[Info.ORIGIN_X], product_dataset.info[Info.ORIGIN_Y], 0),
+            )
         else:
-            LOG.debug("Layer {} already purged from Scene Graph".format(uuid_removed))
+            composite = MultiChannelImage(
+                images_data,
+                name=str(product_dataset.uuid),
+                clim=layer.presentation.climits,
+                gamma=layer.presentation.gamma,
+                interpolation="nearest",
+                cmap=None,
+                parent=self.layer_nodes[layer.uuid],
+            )
+            composite.transform = STTransform(
+                scale=(product_dataset.info[Info.CELL_WIDTH], product_dataset.info[Info.CELL_HEIGHT], 1),
+                translate=(product_dataset.info[Info.ORIGIN_X], product_dataset.info[Info.ORIGIN_Y], 0),
+            )
+        self.composite_element_dependencies[product_dataset.uuid] = product_dataset.input_datasets_uuids
+        self.dataset_nodes[product_dataset.uuid] = composite
+        self.on_view_change(None)
+        LOG.debug("Scene Graph after COMPOSITE dataset insertion:")
+        LOG.debug(self.main_view.describe_tree(with_transform=True))
 
-    def _purge_layer(self, *args, **kwargs):
-        res = self.purge_layer(*args, **kwargs)
-        # when purging the layer is the only operation being performed then update when we are done
-        self.update()
-        return res
-
-    def change_layers_visibility(self, layers_changed: dict):
-        for uuid, visible in layers_changed.items():
-            self.set_layer_visible(uuid, visible)
-
-    def rebuild_new_layer_set(self, new_set_number: int, new_prez_order: DocLayerStack, new_anim_order: list):
-        self.rebuild_all()
-        # raise NotImplementedError("layer set change not implemented in SceneGraphManager")
-
-    def _connect_doc_signals(self, document):
-        document.didReorderLayers.connect(self._rebuild_layer_order)  # current layer set changed z/anim order
-        document.didAddBasicLayer.connect(self.add_basic_layer)  # layer added to one or more layer sets
-        document.didAddCompositeLayer.connect(
-            self.add_composite_layer)  # layer derived from other layers (either basic or composite themselves)
-        document.didRemoveLayers.connect(self._remove_layer)  # layer removed from current layer set
-        document.willPurgeLayer.connect(self._purge_layer)  # layer removed from document
-        document.didSwitchLayerSet.connect(self.rebuild_new_layer_set)
-        document.didChangeColormap.connect(self.change_layers_colormap)
-        document.didChangeLayerVisibility.connect(self.change_layers_visibility)
-        document.didReorderAnimation.connect(self._rebuild_frame_order)
-        document.didChangeComposition.connect(self.change_composite_layer)
-        document.didChangeCompositions.connect(self.change_composite_layers)
-        document.didChangeColorLimits.connect(self.change_layers_color_limits)
-        document.didChangeGamma.connect(self.change_layers_gamma)
-        document.didChangeImageKind.connect(self.change_layers_image_kind)
-
-    def set_frame_number(self, frame_number=None):
-        self.layer_set.next_frame(None, frame_number)
-
-    def set_layer_visible(self, uuid, visible=None):
-        image = self.image_elements.get(uuid, None)
-        if image is None:
+    def add_node_for_lines_dataset(self, layer: LayerItem, product_dataset: ProductDataset) -> scene.VisualNode:
+        assert self.layer_nodes[layer.uuid] is not None  # nosec B101
+        assert product_dataset.kind == Kind.LINES  # nosec B101
+
+        content, _ = self.workspace.get_lines_arrays(product_dataset.uuid)
+        if content is None:
+            LOG.info(f"Dataset contains no lines: {product_dataset.uuid}")
             return
-        image.visible = not image.visible if visible is None else visible
 
-    def rebuild_layer_order(self, new_layer_index_order, *args, **kwargs):
-        """
-        layer order has changed; shift layers around.
-        an empty list is sent if the whole layer order has been changed
-        :param change:
-        :return:
-        """
-        # TODO this is the lazy implementation, eventually just change z order on affected layers
-        self.layer_set.set_layer_order(self.document.current_layer_uuid_order)
+        lines = Lines(content, parent=self.layer_nodes[layer.uuid])
+        lines.set_gl_state("translucent")
+        lines.name = str(product_dataset.uuid)
 
-    def _rebuild_layer_order(self, *args, **kwargs):
-        res = self.rebuild_layer_order(*args, **kwargs)
-        self.update()
-        return res
-
-    def rebuild_frame_order(self, uuid_list: list, *args, **kwargs):
-        LOG.debug('setting SGM new frame order to {0!r:s}'.format(uuid_list))
-        self.layer_set.frame_order = uuid_list
-
-    def _rebuild_frame_order(self, *args, **kwargs):
-        res = self.rebuild_frame_order(*args, **kwargs)
-        # when purging the layer is the only operation being performed then update when we are done
-        self.update()
-        return res
-
-    def rebuild_presentation(self, presentation_info: dict):
-        # refresh our presentation info
-        # presentation_info = self.document.current_layer_set
-        for uuid, layer_prez in presentation_info.items():
-            self.set_colormap(layer_prez.colormap, uuid=uuid)
-            self.set_color_limits(layer_prez.climits, uuid=uuid)
-            self.set_layer_visible(uuid, visible=layer_prez.visible)
-            # FUTURE, if additional information is added to the presentation tuple, you must also update it here
+        self.dataset_nodes[product_dataset.uuid] = lines
+        self.on_view_change(None)
+        LOG.debug("Scene Graph after LINES dataset insertion:")
+        LOG.debug(self.main_view.describe_tree(with_transform=True))
 
-    def rebuild_all(self, *args, **kwargs):
-        """
-        resynchronize the scenegraph to the document content
-        This includes creating elements for any newly-valid layers,
-        removing elements for no-longer-valid layers, and
-        making the display order, visibility, and animation order match the document
+    def add_node_for_points_dataset(self, layer: LayerItem, product_dataset: ProductDataset) -> scene.VisualNode:
+        assert self.layer_nodes[layer.uuid] is not None  # nosec B101
+        assert product_dataset.kind == Kind.POINTS  # nosec B101
+
+        pos, values = self.workspace.get_points_arrays(product_dataset.uuid)
+        if pos is None:
+            LOG.info(f"dataset contains no points: {product_dataset.uuid}")
+            return
+
+        kwargs = map_point_style_to_marker_kwargs(get_point_style_by_name(layer.presentation.style))
+
+        if values is not None:
+            assert len(pos) == len(values)  # nosec B101
+            # TODO use climits of the presentation instead of autoscaling?
+            colormap = self.document.find_colormap(layer.presentation.colormap)
+            kwargs["face_color"] = self.map_to_colors_autoscaled(colormap, values)
+
+        points = Markers(pos=pos, parent=self.layer_nodes[layer.uuid], **kwargs)
+        points.set_gl_state("translucent")  # makes no difference though
+        points.name = str(product_dataset.uuid)
+
+        self.dataset_nodes[product_dataset.uuid] = points
+        self.on_view_change(None)
+        LOG.debug("Scene Graph after POINTS dataset insertion:")
+        LOG.debug(self.main_view.describe_tree(with_transform=True))
+
+    def map_to_colors_autoscaled(self, colormap, values, m=2):
+        """Get a list of colors by mapping each entry in values by the given colormap.
+
+        The mapping range is adjusted automatically to m times the standard
+        deviation from the mean. This ignores outliers in the calculation of
+        the mapping range.
+
+        Caution: this is an expensive operation and must not be called in tight
+        loops.
+
+        :param colormap: the colormap to apply
+        :param values: the values to map to colors
+        :param m: factor to stretch the standard deviation around the mean to define the mapping range
+        :return: list of mapped colors in the same order as the input values
+        """
+        std_dev = np.std(values)
+        mean = np.mean(values)
+        min = mean - m * std_dev  # noqa: calm down PyCharm's spelling check, ...
+        max = mean + m * std_dev  # noqa: ... 'min' and 'max' are fine!
+        scaled_attr = np.interp(values, (min, max), (0, 1))
+        colors = colormap.map(scaled_attr)
+        return colors
+
+    def change_node_for_composite_dataset(self, layer: LayerItem, product_dataset: ProductDataset):
+        if layer.kind == Kind.RGB:
+            if product_dataset.uuid in self.dataset_nodes:
+                # RGB selection has changed, rebuild the dataset
+                LOG.debug(
+                    f"Changing existing composite dataset to"
+                    f" Scene Graph Manager with UUID:"
+                    f" {product_dataset.uuid}"
+                )
+
+                assert product_dataset.input_datasets_uuids is not None  # nosec B101 # suppress mypy [union-attr]
+                images_data = list(
+                    self.workspace.get_content(curr_input_id, Kind.IMAGE)
+                    for curr_input_id in product_dataset.input_datasets_uuids
+                )
+
+                self.composite_element_dependencies[product_dataset.uuid] = product_dataset.input_datasets_uuids
+
+                composite = self.dataset_nodes[product_dataset.uuid]
+                if isinstance(composite, RGBCompositeImage):
+                    composite.set_channels(
+                        images_data,
+                        cell_width=product_dataset.info[Info.CELL_WIDTH],
+                        cell_height=product_dataset.info[Info.CELL_HEIGHT],
+                        origin_x=product_dataset.info[Info.ORIGIN_X],
+                        origin_y=product_dataset.info[Info.ORIGIN_Y],
+                    )
+                elif isinstance(composite, MultiChannelImage):
+                    composite.set_data(images_data)
+                composite.clim = layer.presentation.climits
+                composite.gamma = layer.presentation.gamma
+                self.on_view_change(None)
+                if isinstance(composite, RGBCompositeImage):
+                    composite.determine_reference_points()
+                self._update()
+            else:
+                self.add_node_for_composite_dataset(layer, product_dataset)
+        else:
+            raise ValueError("Unknown or unimplemented composite type")
+
+    def update_basic_dataset(self, uuid: UUID, kind: Kind):
         """
-        # get the list of layers which are valid, and either visible or in the animation order
-        doc_layers = list(self.document.active_layer_order)
-        presentation_info = tuple(p for (p, l) in doc_layers)
-        active_layers = tuple(l for (p, l) in doc_layers)
-        active_uuids = set(x.uuid for x in active_layers)
-        active_lookup = dict((x.uuid, x) for x in active_layers)
-        prez_lookup = dict((x.uuid, x) for x in presentation_info)
-
-        uuids_w_elements = set(self.image_elements.keys())
-        # get set of valid layers not having elements and invalid layers having elements
-        inconsistent_uuids = uuids_w_elements ^ active_uuids
-
-        # current_uuid_order = self.document.current_layer_uuid_order
-        current_uuid_order = tuple(p.uuid for p in presentation_info)
-
-        remove_elements = []
-        for uuid in inconsistent_uuids:
-            if uuid in active_lookup and active_lookup[uuid].is_valid:
-                layer = active_lookup[uuid]
-                # create elements for layers which have transitioned to a valid state
-                LOG.debug('creating deferred element for layer %s' % layer.uuid)
-                if layer.kind in [Kind.COMPOSITE, Kind.RGB]:
-                    # create an invisible element with the RGB
-                    self.change_composite_layer(current_uuid_order, layer, prez_lookup[uuid])
-                else:
-                    # FIXME this was previously a NotImplementedError
-                    LOG.warning('unable to create deferred scenegraph element for %s' % repr(layer))
+        Push the data (content) of a basic dataset again to the associated scene
+        graph node.
+
+        This method shall be called whenever the data of a basic dataset changes.
+        :param uuid: identifier of the dataset
+        :param kind: kind of the dataset / data content.
+        """
+        try:
+            dataset_node = self.dataset_nodes[uuid]
+            dataset_content = self.workspace.get_content(uuid, kind=kind)
+            dataset_node.set_data(dataset_content)
+        except NotImplementedError:
+            if isinstance(dataset_node, TiledGeolocatedImage):
+                LOG.debug(
+                    f"Updating data for UUID {uuid} on its associated"
+                    f" scenegraph TiledGeolocatedImage node is not"
+                    f" possible, hopefully the data was modified in-place"
+                    f" (e.g. when merging new granules)."
+                )
+                # TODO: How to detect the case that the data was not changed in
+                #  place but a new reference was given? In this case, we must
+                #  re-raise the NotImplementedError exception (as in the 'else'
+                #  path)
+                # TODO: TiledGeolocatedImage does not provide a way to tell it
+                #  that it should drop all retiled data and start from scratch.
             else:
-                # remove elements for layers which are no longer valid
-                remove_elements.append(uuid)
+                # This is an unforeseen case: at the moment this method
+                # should only be called when merging data segments into existing
+                # image(!) data, looks like it was called for a node of another
+                # type not having set_data() too.
+                raise
+        except KeyError:
+            LOG.fatal(f"Node for dataset with the uuid '{uuid}' does not exist in the scene graph. This is a BUG!")
+            raise
 
-        # get info on the new order
-        self.layer_set.set_layer_order(current_uuid_order)
-        self.layer_set.frame_order = self.document.current_animation_order
-        self.rebuild_presentation(prez_lookup)
+        self.on_view_change(None)
+        self._update()
 
-        for elem in remove_elements:
-            self.purge_layer(elem)
+    def update_layers_z(self, uuids: list):
+        if self.layer_nodes:
+            # Rendering order must be set analogous to z order
+            # (higher z values -> further away), render back to front
+            # https://vispy.org/faq.html#how-to-achieve-transparency-with-2d-objects
+            z_counter = 0
+            for z_level, uuid in enumerate(uuids):
+                layer_node = self.layer_nodes[uuid]
+                layer_node.transform.translate = (0, 0, 0 - z_level)
+                layer_node.order = z_counter
+                z_counter -= 1
+            self._update()
 
-        self.update()
+    def purge_dataset(self, uuid_removed: UUID):
+        """
+        Dataset has been purged from document (no longer used anywhere) - flush it all out
+        :param uuid_removed: UUID of the dataset that is to be removed
+        :return:
+        """
+        if uuid_removed in self.dataset_nodes:
+            dataset = self.dataset_nodes[uuid_removed]
+            dataset.parent = None
+            del self.dataset_nodes[uuid_removed]
+            LOG.info(f"dataset {uuid_removed} purge from Scene Graph")
+        else:
+            LOG.debug(f"dataset {uuid_removed} already purged from Scene Graph")
+        LOG.debug("Scene Graph after dataset deletion:")
+        LOG.debug(self.main_view.describe_tree(with_transform=True))
+
+    def remove_layer_node(self, uuid_removed: UUID):
+        """
+        Layer will be removed, but before it can be removed correctly the scene graph node has to be removed.
+        :param uuid_removed: UUID of the layer that will be removed
+        """
+        if uuid_removed in self.layer_nodes:
+            layer = self.layer_nodes[uuid_removed]
+            layer.parent = None
+            del self.layer_nodes[uuid_removed]
+            LOG.info(f"layer {uuid_removed} removed from Scene Graph")
+        else:
+            LOG.debug(f"Layer {uuid_removed} already removed from Scene Graph")
+        LOG.debug("Scene Graph after layer deletion:")
+        LOG.debug(self.main_view.describe_tree(with_transform=True))
+
+    def _connect_doc_signals(self, document: Document):
+        document.didUpdateBasicDataset.connect(self.update_basic_dataset)  # new data integrated in existing layer
+
+    def _set_dataset_visible(self, uuid: UUID, visible: Optional[bool] = None):
+        dataset_node = self.dataset_nodes.get(uuid, None)
+        if dataset_node is None:
+            return
+        dataset_node.visible = not dataset_node.visible if visible is None else visible
 
     def on_view_change(self, scheduler):
-        """Simple event handler for when we need to reassess image layers.
-        """
+        """Simple event handler for when we need to reassess image datasets."""
         # Stop the timer so it doesn't continuously call this slot
         if scheduler:
             scheduler.stop()
 
         def _assess(uuid, child):
             need_retile, preferred_stride, tile_box = child.assess()
             if need_retile:
-                self.start_retiling_task(uuid, preferred_stride, tile_box)
+                self._start_retiling_task(uuid, preferred_stride, tile_box)
 
-        current_visible_layers = [p.uuid for (p, l) in self.document.active_layer_order if p.visible]
-        current_invisible_layers = set(self.image_elements.keys()) - set(current_visible_layers)
+        current_datasets_uuids = self.dataset_nodes.keys()
 
         def _assess_if_active(uuid):
-            element = self.image_elements.get(uuid, None)
-            if element is not None and hasattr(element, 'assess'):
-                _assess(uuid, element)
+            dataset_node = self.dataset_nodes.get(uuid, None)
+            if dataset_node is not None and hasattr(dataset_node, "assess"):
+                _assess(uuid, dataset_node)
 
-        for uuid in current_visible_layers:
-            _assess_if_active(uuid)
-        # update contours
-        for node in self.proxy_nodes.values():
-            node.on_view_change()
-        # update invisible layers
-        for uuid in current_invisible_layers:
+        # update all available datasets nodes
+        for uuid in current_datasets_uuids:
             _assess_if_active(uuid)
 
-    def start_retiling_task(self, uuid, preferred_stride, tile_box):
+    def _start_retiling_task(self, uuid, preferred_stride, tile_box):
         LOG.debug("Scheduling retile for child with UUID: %s", uuid)
-        self.queue.add(str(uuid) + "_retile", self._retile_child(uuid, preferred_stride, tile_box),
-                       'Retile calculations for image layer ' + str(uuid), interactive=True)
+        self.queue.add(
+            str(uuid) + "_retile",
+            self._retile_child(uuid, preferred_stride, tile_box),
+            "Retile calculations for image dataset" + str(uuid),
+            interactive=True,
+        )
 
     def _retile_child(self, uuid, preferred_stride, tile_box):
         LOG.debug("Retiling child with UUID: '%s'", uuid)
-        yield {TASK_DOING: 'Re-tiling', TASK_PROGRESS: 0.0}
+        yield {TASK_DOING: "Re-tiling", TASK_PROGRESS: 0.0}
         if uuid not in self.composite_element_dependencies:
-            child = self.image_elements[uuid]
-            data = self.workspace.get_content(uuid, lod=preferred_stride)
-            yield {TASK_DOING: 'Re-tiling', TASK_PROGRESS: 0.5}
+            kind = self.document[uuid].get(Info.KIND)
+            child = self.dataset_nodes[uuid]
+            data = self.workspace.get_content(uuid, lod=preferred_stride, kind=kind)
+            yield {TASK_DOING: "Re-tiling", TASK_PROGRESS: 0.5}
             # FIXME: Use LOD instead of stride and provide the lod to the workspace
-            data = data[::preferred_stride[0], ::preferred_stride[1]]
+            data = data[:: preferred_stride[0], :: preferred_stride[1]]
             tiles_info, vertices, tex_coords = child.retile(data, preferred_stride, tile_box)
-            yield {TASK_DOING: 'Re-tiling', TASK_PROGRESS: 1.0}
+            yield {TASK_DOING: "Re-tiling", TASK_PROGRESS: 1.0}
             self.didRetilingCalcs.emit(uuid, preferred_stride, tile_box, tiles_info, vertices, tex_coords)
         else:
-            child = self.image_elements[uuid]
-            data = [self.workspace.get_content(d_uuid, lod=preferred_stride) for d_uuid in
-                    self.composite_element_dependencies[uuid]]
-            yield {TASK_DOING: 'Re-tiling', TASK_PROGRESS: 0.5}
+            child = self.dataset_nodes[uuid]
+            data = [
+                self.workspace.get_content(d_uuid, lod=preferred_stride)
+                for d_uuid in self.composite_element_dependencies[uuid]
+            ]
+            yield {TASK_DOING: "Re-tiling", TASK_PROGRESS: 0.5}
             # FIXME: Use LOD instead of stride and provide the lod to the workspace
             data = [
-                d[::int(preferred_stride[0] / factor), ::int(preferred_stride[1] / factor)] if d is not None else None
-                for factor, d in zip(child._channel_factors, data)]
+                d[:: int(preferred_stride[0] / factor), :: int(preferred_stride[1] / factor)] if d is not None else None
+                for factor, d in zip(child._channel_factors, data)
+            ]
             tiles_info, vertices, tex_coords = child.retile(data, preferred_stride, tile_box)
-            yield {TASK_DOING: 'Re-tiling', TASK_PROGRESS: 1.0}
+            yield {TASK_DOING: "Re-tiling", TASK_PROGRESS: 1.0}
             self.didRetilingCalcs.emit(uuid, preferred_stride, tile_box, tiles_info, vertices, tex_coords)
         self.workspace.bgnd_task_complete()  # FUTURE: consider a threading context manager for this??
 
     def _set_retiled(self, uuid, preferred_stride, tile_box, tiles_info, vertices, tex_coords):
-        """Slot to take data from background thread and apply it to the layer living in the image layer.
-        """
-        child = self.image_elements.get(uuid, None)
+        """Slot to take data from background thread and apply it to the dataset living in the image dataset."""
+        child = self.dataset_nodes.get(uuid, None)
         if child is None:
-            LOG.warning('unable to find uuid %s in image_elements' % uuid)
+            LOG.warning("unable to find uuid %s in dataset_nodes" % uuid)
             return
         child.set_retiled(preferred_stride, tile_box, tiles_info, vertices, tex_coords)
         child.update()
 
-    def on_layer_visible_toggle(self, visible):
-        pass
 
-    def on_layer_change(self, event):
-        pass
+# TODO move these defaults to common config defaults location
+LATLON_GRID_RESOLUTION_MIN: float = 0.1
+LATLON_GRID_RESOLUTION_DEFAULT: float = 5.0
+LATLON_GRID_RESOLUTION_MAX: float = 10.0
+
+
+def get_configured_latlon_grid_resolution() -> float:
+    resolution: float = config.get("latlon_grid.resolution", LATLON_GRID_RESOLUTION_DEFAULT)
+
+    if not isinstance(resolution, Number):
+        LOG.warning(
+            f"Invalid configuration for lat/lon grid resolution"
+            f" (='{resolution}') found."
+            f" Using the default {LATLON_GRID_RESOLUTION_DEFAULT}."
+        )
+        return LATLON_GRID_RESOLUTION_DEFAULT
+
+    if resolution > LATLON_GRID_RESOLUTION_MAX:
+        LOG.warning(
+            f"Configured lat/lon grid resolution {resolution}"
+            f" is greater than allowed maximum."
+            f" Using the maximum {LATLON_GRID_RESOLUTION_MAX}."
+        )
+        return LATLON_GRID_RESOLUTION_MAX
+
+    if resolution < LATLON_GRID_RESOLUTION_MIN:
+        LOG.warning(
+            f"Configured lat/lon grid resolution {resolution}"
+            f" is less than allowed minimum."
+            f" Using the minimum {LATLON_GRID_RESOLUTION_MIN}."
+        )
+        return LATLON_GRID_RESOLUTION_MIN
 
-    def on_data_loaded(self, event):
-        pass
+    return resolution
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `uwsift-1.2.3/uwsift/view/test_visuals.py` & `uwsift-2.0.0b0/uwsift/view/test_visuals.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,54 +1,52 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
 """Tests for the MultiChannelImageVisual."""
 
-from uwsift.view.visuals import MultiChannelImage
-
 import numpy as np
-from vispy.testing import TestingCanvas, run_tests_if_main, requires_application
+from vispy.testing import TestingCanvas, requires_application, run_tests_if_main
+
+from uwsift.view.visuals import MultiChannelImage
 
 
 @requires_application()
 def test_multiband_visual():
     size = (400, 600)
     with TestingCanvas(size=size) as c:
         r_data = np.random.rand(*size)
         g_data = np.random.rand(*size)
         b_data = np.random.rand(*size)
-        image = MultiChannelImage(
-            [r_data, None, None],
-            parent=c.scene)
+        image = MultiChannelImage([r_data, None, None], parent=c.scene)
 
         # Assign only R
         result = c.render()
         r_result = result[..., 0]
         g_result = result[..., 1]
         b_result = result[..., 2]
-        assert not np.allclose(r_result, 0)
+        assert not np.allclose(r_result, 0)  # nosec B101
         np.testing.assert_allclose(g_result, 0)
         np.testing.assert_allclose(b_result, 0)
 
         # Add B
         image.set_data([r_data, None, b_data])
         image.clim = ("auto", "auto", "auto")
         result = c.render()
         r_result = result[..., 0]
         g_result = result[..., 1]
         b_result = result[..., 2]
-        assert not np.allclose(r_result, 0)
+        assert not np.allclose(r_result, 0)  # nosec B101
         np.testing.assert_allclose(g_result, 0)
-        assert not np.allclose(b_result, 0)
+        assert not np.allclose(b_result, 0)  # nosec B101
 
         # Unset R, add G
         image.set_data([None, g_data, b_data])
         image.clim = ("auto", "auto", "auto")
         result = c.render()
         r_result = result[..., 0]
         g_result = result[..., 1]
         b_result = result[..., 2]
         np.testing.assert_allclose(r_result, 0)
-        assert not np.allclose(g_result, 0)
-        assert not np.allclose(b_result, 0)
+        assert not np.allclose(g_result, 0)  # nosec B101
+        assert not np.allclose(b_result, 0)  # nosec B101
 
 
 run_tests_if_main()
```

### Comparing `uwsift-1.2.3/uwsift/view/texture_atlas.py` & `uwsift-2.0.0b0/uwsift/view/texture_atlas.py`

 * *Files 6% similar despite different names*

```diff
@@ -25,37 +25,40 @@
 import numpy as np
 from vispy.visuals._scalable_textures import GPUScaledTexture2D
 
 from uwsift.common import DEFAULT_TILE_HEIGHT, DEFAULT_TILE_WIDTH
 
 DEBUG_IMAGE_TILE = bool(os.environ.get("SIFT_DEBUG_TILES", False))
 
-__author__ = 'rayg'
-__docformat__ = 'reStructuredText'
+__author__ = "rayg"
+__docformat__ = "reStructuredText"
 
 LOG = logging.getLogger(__name__)
 
 
 class TextureAtlas2D(GPUScaledTexture2D):
-    """A 2D Texture Array structure implemented as a 2D Texture Atlas.
-    """
+    """A 2D Texture Array structure implemented as a 2D Texture Atlas."""
 
-    def __init__(self, texture_shape,
-                 tile_shape=(DEFAULT_TILE_HEIGHT, DEFAULT_TILE_WIDTH),
-                 **texture_kwargs):
+    def __init__(self, texture_shape, tile_shape=(DEFAULT_TILE_HEIGHT, DEFAULT_TILE_WIDTH), **texture_kwargs):
         # Number of tiles in each direction (y, x)
         self.texture_shape = self._check_texture_shape(texture_shape)
         # Number of rows and columns for each tile
         self.tile_shape = tile_shape
         # Number of rows and columns to hold all of these tiles in one texture
         shape = (self.texture_shape[0] * self.tile_shape[0], self.texture_shape[1] * self.tile_shape[1])
+        if len(tile_shape) == 3:
+            shape = (shape[0], shape[1], tile_shape[2])
         self.texture_size = shape
         self._fill_array = np.tile(np.float32(np.nan), self.tile_shape)
         # create a representative array so the texture can be initialized properly with the right dtype
-        rep_arr = np.zeros((10, 10), dtype=np.float32)
+        rep_arr = (
+            np.zeros((10, 10, tile_shape[2]), dtype=np.float32)
+            if len(tile_shape) == 3
+            else np.zeros((10, 10), dtype=np.float32)
+        )
         # will add self.shape:
         super(TextureAtlas2D, self).__init__(data=rep_arr, **texture_kwargs)
         # GPUScaledTexture2D always uses a "representative" size
         # we need to force the shape to our final size so we can start setting tiles right away
         self._resize(shape)
 
     def _check_texture_shape(self, texture_shape):
@@ -73,65 +76,59 @@
         This class presents a 1D indexing scheme, but internally can hold multiple tiles in both X and Y direction.
         """
         row = int(idx / self.texture_shape[1])
         col = idx % self.texture_shape[1]
         return row * self.tile_shape[0], col * self.tile_shape[1]
 
     def set_tile_data(self, tile_idx, data, copy=False):
-        """Write a single tile of data into the texture.
-        """
+        """Write a single tile of data into the texture."""
         offset = self._tex_offset(tile_idx)
         if data is None:
             # Special "fill" parameter
             data = self._fill_array
         else:
             # FIXME: Doesn't this always return the shape of the input data?
-            tile_offset = (min(self.tile_shape[0], data.shape[0]),
-                           min(self.tile_shape[1], data.shape[1]))
+            tile_offset = (min(self.tile_shape[0], data.shape[0]), min(self.tile_shape[1], data.shape[1]))
             if tile_offset[0] < self.tile_shape[0] or tile_offset[1] < self.tile_shape[1]:
                 # FIXME: This should be handled by the caller to expand the array to be NaN filled and aligned
                 # Assign a fill value, make sure to copy the data so that we don't overwrite the original
                 data_orig = data
                 data = np.zeros(self.tile_shape, dtype=data.dtype)
                 # data = data.copy()
                 data[:] = np.nan
-                data[:tile_offset[0], :tile_offset[1]] = data_orig[:tile_offset[0], :tile_offset[1]]
+                data[: tile_offset[0], : tile_offset[1]] = data_orig[: tile_offset[0], : tile_offset[1]]
         if DEBUG_IMAGE_TILE:
-            data[:5, :] = 1000.
-            data[-5:, :] = 1000.
-            data[:, :5] = 1000.
-            data[:, -5:] = 1000.
+            data[:5, :] = 1000.0
+            data[-5:, :] = 1000.0
+            data[:, :5] = 1000.0
+            data[:, -5:] = 1000.0
         super(TextureAtlas2D, self).scale_and_set_data(data, offset=offset, copy=copy)
 
 
 class MultiChannelGPUScaledTexture2D:
-    """Wrapper class around indiviual textures.
+    """Wrapper class around individual textures.
 
     This helper class allows for easier handling of multiple textures that
     represent individual R, G, and B channels of an image.
 
     """
+
     _singular_texture_class = GPUScaledTexture2D
     _ndim = 2
 
     def __init__(self, data, **texture_kwargs):
         # data to sent to texture when not being used
-        self._fill_arr = np.full((10, 10), np.float32(np.nan),
-                                 dtype=np.float32)
+        self._fill_arr = np.full((10, 10), np.float32(np.nan), dtype=np.float32)
 
         self.num_channels = len(data)
         data = [x if x is not None else self._fill_arr for x in data]
-        self._textures = self._create_textures(self.num_channels, data,
-                                               **texture_kwargs)
+        self._textures = self._create_textures(self.num_channels, data, **texture_kwargs)
 
     def _create_textures(self, num_channels, data, **texture_kwargs):
-        return [
-            self._singular_texture_class(data[i], **texture_kwargs)
-            for i in range(num_channels)
-        ]
+        return [self._singular_texture_class(data[i], **texture_kwargs) for i in range(num_channels)]
 
     @property
     def textures(self):
         return self._textures
 
     @property
     def clim(self):
@@ -200,24 +197,32 @@
             for all sub-textures. If a 3-element tuple then the first offset
             index represents the sub-texture to update.
 
         """
         is_multi = isinstance(data, (list, tuple))
         index_provided = offset is not None and len(offset) == self._ndim + 1
         if not is_multi and not index_provided:
-            raise ValueError("Setting texture data for a single sub-texture "
-                             "requires 'offset' to be passed with the first "
-                             "element specifying the sub-texture index.")
+            raise ValueError(
+                "Setting texture data for a single sub-texture "
+                "requires 'offset' to be passed with the first "
+                "element specifying the sub-texture index."
+            )
         elif is_multi and index_provided:
-            warnings.warn("Multiple texture arrays were passed, but so was "
-                          "sub-texture index in 'offset'. Ignoring that index.", UserWarning)
+            warnings.warn(
+                "Multiple texture arrays were passed, but so was "
+                "sub-texture index in 'offset'. Ignoring that index.",
+                UserWarning,
+                stacklevel=4,
+            )
             offset = offset[1:]
         if is_multi and len(data) != self.num_channels:
-            raise ValueError("Multiple provided arrays must match number of channels. "
-                             f"Got {len(data)}, expected {self.num_channels}.")
+            raise ValueError(
+                "Multiple provided arrays must match number of channels. "
+                f"Got {len(data)}, expected {self.num_channels}."
+            )
 
         if offset is not None and len(offset) == self._ndim + 1:
             tex_indexes = offset[:1]
             offset = offset[1:]
             data = [data]
         else:
             tex_indexes = range(self.num_channels)
```

### Comparing `uwsift-1.2.3/uwsift/view/tile_calculator.py` & `uwsift-2.0.0b0/uwsift/view/tile_calculator.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,45 +1,47 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
 """Supports calculations used throughout the library and application."""
 
 import numpy as np
-from numba import jit, float64, int64, float32, types as nb_types
+from numba import float32, float64, int64, jit
+from numba import types as nb_types
 from numba.extending import overload
 from pyproj import Proj
 
-from uwsift.common import (Resolution, Point, Box,
-                           CANVAS_EXTENTS_EPSILON,
-                           PREFERRED_SCREEN_TO_TEXTURE_RATIO,
-                           IMAGE_MESH_SIZE,
-                           DEFAULT_PROJECTION,
-                           DEFAULT_TEXTURE_HEIGHT,
-                           DEFAULT_TEXTURE_WIDTH,
-                           DEFAULT_TILE_HEIGHT,
-                           DEFAULT_TILE_WIDTH)
+from uwsift.common import (
+    CANVAS_EXTENTS_EPSILON,
+    DEFAULT_PROJECTION,
+    DEFAULT_TEXTURE_HEIGHT,
+    DEFAULT_TEXTURE_WIDTH,
+    DEFAULT_TILE_HEIGHT,
+    DEFAULT_TILE_WIDTH,
+    IMAGE_MESH_SIZE,
+    PREFERRED_SCREEN_TO_TEXTURE_RATIO,
+    Box,
+    IndexBox,
+    Point,
+    Resolution,
+)
 
 
 @overload(np.isclose)
 def isclose(a, b):
-    """Implementation of numpy.isclose() since it is currently not supported by numba.
-    """
+    """Implementation of numpy.isclose() since it is currently not supported by numba."""
 
     def isclose_impl(a, b):
         atol = 1e-8
         rtol = 1e-5
         res = np.abs(a - b) <= (atol + rtol * np.abs(b))
         return res
 
     return isclose_impl
 
 
-@jit(nb_types.UniTuple(int64, 2)(
-    float64[:, :],
-    float64[:, :]
-), nopython=True, cache=True, nogil=True)
+@jit(nb_types.UniTuple(int64, 2)(float64[:, :], float64[:, :]), nopython=True, cache=True, nogil=True)
 def get_reference_points(img_cmesh, img_vbox):
     """Get two image reference point indexes.
 
     This function will return the two nearest reference points to the
     center of the viewed canvas. The first argument `img_cmesh` is all
     valid image mesh points that were successfully transformed to the
     view projection. The second argument `img_vbox` is these same mesh
@@ -54,116 +56,83 @@
     # Sort points by nearest to further from the 0,0 center of the canvas
     # Uses a cheap Pythagorean theorem by summing X + Y
     near_points = np.sum(np.abs(img_cmesh), axis=1).argsort()
     ref_idx_1 = near_points[0]
 
     # pick a second reference point that isn't in the same row or column as the first
     near_points_2 = near_points[
-        ~np.isclose(img_vbox[near_points][:, 0], img_vbox[ref_idx_1][0]) & ~np.isclose(img_vbox[near_points][:, 1],
-                                                                                       img_vbox[ref_idx_1][1])]
+        ~np.isclose(img_vbox[near_points][:, 0], img_vbox[ref_idx_1][0])
+        & ~np.isclose(img_vbox[near_points][:, 1], img_vbox[ref_idx_1][1])
+    ]
     if near_points_2.shape[0] == 0:
         raise ValueError("Could not determine reference points")
 
     return ref_idx_1, near_points_2[0]
 
 
-# @jit(nb_types.UniTuple(int64, 2)(float64[:, :], boolean[:, :]))
-# def get_reference_points_image(img_dist, valid_mask):
-#     """Get two image reference point indexes close to image center.
-#
-#     This function will return the two nearest reference points to the
-#     center of the image. The argument `img_vbox` is an array of points
-#     across the image that can be successfully projected to the viewed
-#     projection.
-#
-#     :param img_dist: (N, 2) array of distances from the image center
-#                      in image space for points that can be projected
-#                      to the viewed projection
-#     :return: (reference array index 1, reference array index 2)
-#     :raises: ValueError if not enough valid points to create
-#              two reference points
-#     """
-#     # Sort points by nearest to further from the 0,0 center of the canvas
-#     # Uses a cheap Pythagorean theorem by summing X + Y
-#     near_points = np.sum(np.abs(img_dist), axis=1)
-#     near_points[~valid_mask] = np.inf
-#     near_points = near_points.argsort()
-#     ref_idx_1 = near_points[0]
-#     if np.isinf(near_points[ref_idx_1]):
-#         raise ValueError("Could not determine reference points")
-#     # pick a second reference point that isn't in the same row or column as the first
-#     near_points_2 = near_points[~np.isclose(img_dist[near_points][:, 0], img_dist[ref_idx_1][0]) &
-#                                 ~np.isclose(img_dist[near_points][:, 1], img_dist[ref_idx_1][1])]
-#     if near_points_2.shape[0] == 0:
-#         raise ValueError("Could not determine reference points")
-#
-#     return ref_idx_1, near_points_2[0]
-
-
-@jit(nb_types.UniTuple(float64, 2)(
-    float64[:, :],
-    float64[:, :],
-    nb_types.UniTuple(int64, 2)
-), nopython=True, cache=True, nogil=True)
+@jit(
+    nb_types.UniTuple(float64, 2)(float64[:, :], float64[:, :], nb_types.UniTuple(int64, 2)),
+    nopython=True,
+    cache=True,
+    nogil=True,
+)
 def calc_pixel_size(canvas_point, image_point, canvas_size):
     # Calculate the number of image meters per display pixel
     # That is, use the ratio of the distance in canvas space
     # between two points to the distance of the canvas
     # (1 - (-1) = 2). Use this ratio to calculate number of
     # screen pixels between the two reference points. Then
     # determine how many image units cover that number of pixels.
     dx = abs(
-        (image_point[1, 0] - image_point[0, 0]) / (canvas_size[0] * (canvas_point[1, 0] - canvas_point[0, 0]) / 2.))
+        (image_point[1, 0] - image_point[0, 0]) / (canvas_size[0] * (canvas_point[1, 0] - canvas_point[0, 0]) / 2.0)
+    )
     dy = abs(
-        (image_point[1, 1] - image_point[0, 1]) / (canvas_size[1] * (canvas_point[1, 1] - canvas_point[0, 1]) / 2.))
+        (image_point[1, 1] - image_point[0, 1]) / (canvas_size[1] * (canvas_point[1, 1] - canvas_point[0, 1]) / 2.0)
+    )
     return dx, dy
 
 
-@jit(nb_types.UniTuple(float64, 2)(
-    float64,
-    float64,
-    int64,
-    float64
-), nopython=True, cache=True, nogil=True)
+@jit(nb_types.UniTuple(float64, 2)(float64, float64, int64, float64), nopython=True, cache=True, nogil=True)
 def _calc_extent_component(canvas_point, image_point, num_pixels, meters_per_pixel):
-    """Calculate """
+    """Calculate"""
     # Find the distance in image space between the closest
     # reference point and the center of the canvas view (0, 0)
     # divide canvas_point coordinate by 2 to get the ratio of that distance to the entire canvas view (-1 to 1)
-    viewed_img_center_shift_x = (canvas_point / 2. * num_pixels * meters_per_pixel)
+    viewed_img_center_shift_x = canvas_point / 2.0 * num_pixels * meters_per_pixel
     # Find the theoretical center of the canvas in image space (X/Y)
     viewed_img_center_x = image_point - viewed_img_center_shift_x
     # Find the theoretical number of image units (meters) that
     # would cover an entire canvas in a perfect world
-    half_canvas_width = num_pixels * meters_per_pixel / 2.
+    half_canvas_width = num_pixels * meters_per_pixel / 2.0
     # Calculate the theoretical bounding box if the image was
     # perfectly centered on the closest reference point
     # Clip the bounding box to the extents of the image
     left = viewed_img_center_x - half_canvas_width
     right = viewed_img_center_x + half_canvas_width
     return left, right
 
 
-@jit(float64(
-    float64,
-    float64,
-    float64
-), nopython=True, cache=True, nogil=True)
+@jit(float64(float64, float64, float64), nopython=True, cache=True, nogil=True)
 def clip(v, n, x):
     return max(min(v, x), n)
 
 
-@jit(nb_types.NamedUniTuple(float64, 4, Box)(
-    nb_types.NamedUniTuple(float64, 4, Box),
-    nb_types.Array(float64, 1, 'C'),
-    nb_types.Array(float64, 1, 'C'),
-    nb_types.UniTuple(int64, 2),
-    float64,
-    float64
-), nopython=True, cache=True, nogil=True)
+@jit(
+    nb_types.NamedUniTuple(float64, 4, Box)(
+        nb_types.NamedUniTuple(float64, 4, Box),
+        nb_types.Array(float64, 1, "C"),
+        nb_types.Array(float64, 1, "C"),
+        nb_types.UniTuple(int64, 2),
+        float64,
+        float64,
+    ),
+    nopython=True,
+    cache=True,
+    nogil=True,
+)
 def calc_view_extents(image_extents_box: Box, canvas_point, image_point, canvas_size, dx, dy) -> Box:
     left, right = _calc_extent_component(canvas_point[0], image_point[0], canvas_size[0], dx)
     left = clip(left, image_extents_box.left, image_extents_box.right)
     right = clip(right, image_extents_box.left, image_extents_box.right)
 
     bot, top = _calc_extent_component(canvas_point[1], image_point[1], canvas_size[1], dy)
     bot = clip(bot, image_extents_box.bottom, image_extents_box.top)
@@ -171,71 +140,84 @@
 
     if (right - left) < CANVAS_EXTENTS_EPSILON or (top - bot) < CANVAS_EXTENTS_EPSILON:
         raise ValueError("Image is outside of canvas or empty")
 
     return Box(left=left, right=right, bottom=bot, top=top)
 
 
-@jit(nb_types.UniTuple(float64, 2)(
-    int64,
-    int64,
-    int64,
-    int64,
-    int64,
-    int64
-), nopython=True, cache=True, nogil=True)
+@jit(nb_types.UniTuple(float64, 2)(int64, int64, int64, int64, int64, int64), nopython=True, cache=True, nogil=True)
 def max_tiles_available(image_shape_y, image_shape_x, tile_shape_y, tile_shape_x, stride_y, stride_x):
     ath = (image_shape_y / float(stride_y)) / tile_shape_y
     atw = (image_shape_x / float(stride_x)) / tile_shape_x
     return ath, atw
 
 
-@jit(nb_types.NamedUniTuple(int64, 4, Box)(
-    float64,
-    float64,
-    float64,
-    float64,
-    float64,
-    float64,
-    int64,
-    int64,
-    int64,
-    int64,
-    float64,
-    float64,
-    float64,
-    float64,
-    float64,
-    float64,
-    int64,
-    int64,
-    int64,
-    int64,
-    int64,
-    int64
-), nopython=True, cache=True, nogil=True)
-def visible_tiles(z_dy, z_dx,
-                  tile_size_dy, tile_size_dx,
-                  image_center_y, image_center_x,
-                  image_shape_y, image_shape_x,
-                  tile_shape_y, tile_shape_x,
-                  v_bottom, v_left, v_top, v_right, v_dy, v_dx,
-                  stride_y, stride_x,
-                  x_bottom, x_left, x_top, x_right):
+@jit(
+    nb_types.NamedUniTuple(int64, 4, IndexBox)(
+        float64,
+        float64,
+        float64,
+        float64,
+        float64,
+        float64,
+        int64,
+        int64,
+        int64,
+        int64,
+        float64,
+        float64,
+        float64,
+        float64,
+        float64,
+        float64,
+        int64,
+        int64,
+        int64,
+        int64,
+        int64,
+        int64,
+    ),
+    nopython=True,
+    cache=True,
+    nogil=True,
+)
+def visible_tiles(
+    z_dy,
+    z_dx,
+    tile_size_dy,
+    tile_size_dx,
+    image_center_y,
+    image_center_x,
+    image_shape_y,
+    image_shape_x,
+    tile_shape_y,
+    tile_shape_x,
+    v_bottom,
+    v_left,
+    v_top,
+    v_right,
+    v_dy,
+    v_dx,
+    stride_y,
+    stride_x,
+    x_bottom,
+    x_left,
+    x_top,
+    x_right,
+):
     tile_size = Resolution(tile_size_dy * stride_y, tile_size_dx * stride_x)
     # should be the upper-left corner of the tile centered on the center of the image
-    to = Point(image_center_y + tile_size.dy / 2.,
-               image_center_x - tile_size.dx / 2.)  # tile origin
+    to = Point(image_center_y + tile_size.dy / 2.0, image_center_x - tile_size.dx / 2.0)  # tile origin
 
     # number of data pixels between view edge and originpoint
     pv = Box(
         bottom=(v_bottom - to.y) / -(z_dy * stride_y),
         top=(v_top - to.y) / -(z_dy * stride_y),
         left=(v_left - to.x) / (z_dx * stride_x),
-        right=(v_right - to.x) / (z_dx * stride_x)
+        right=(v_right - to.x) / (z_dx * stride_x),
     )
 
     th = tile_shape_y
     tw = tile_shape_x
     # first tile we'll need is (tiy0, tix0)
     # floor to make sure we get the upper-left of the theoretical tile
     tiy0 = np.floor(pv.top / th)
@@ -255,244 +237,256 @@
     if x_top > 0:
         tiy0 -= int(x_top)
         nth += int(x_top)
     if x_right > 0:
         ntw += int(x_right)
 
     # Total number of tiles in this image at this stride (could be fractional)
-    ath, atw = max_tiles_available(image_shape_y, image_shape_x,
-                                   tile_shape_y, tile_shape_x,
-                                   stride_y, stride_x)
+    ath, atw = max_tiles_available(image_shape_y, image_shape_x, tile_shape_y, tile_shape_x, stride_y, stride_x)
     # truncate to the available tiles
-    hw = atw / 2.
-    hh = ath / 2.
+    hw = atw / 2.0
+    hh = ath / 2.0
     # center tile is half pixel off because we want center of the center
     # tile to be at the center of the image
     if tix0 < -hw + 0.5:
         ntw += hw - 0.5 + tix0
         tix0 = -hw + 0.5
     if tiy0 < -hh + 0.5:
         nth += hh - 0.5 + tiy0
         tiy0 = -hh + 0.5
     # add 0.5 to include the "end of the tile" since the r and b are exclusive
     if tix0 + ntw > hw + 0.5:
         ntw = hw + 0.5 - tix0
     if tiy0 + nth > hh + 0.5:
         nth = hh + 0.5 - tiy0
 
-    tilebox = Box(
+    tilebox = IndexBox(
         bottom=np.int64(np.ceil(tiy0 + nth)),
         left=np.int64(np.floor(tix0)),
         top=np.int64(np.floor(tiy0)),
         right=np.int64(np.ceil(tix0 + ntw)),
     )
     return tilebox
 
 
-@jit(nb_types.UniTuple(nb_types.Tuple([int64, int64, int64]), 2)(
-    int64,
-    int64,
-    int64,
-    int64,
-    nb_types.NamedUniTuple(int64, 2, Point),
-    nb_types.NamedUniTuple(int64, 2, Point)
-), nopython=True, cache=True, nogil=True)
+@jit(
+    nb_types.UniTuple(nb_types.Tuple([int64, int64, int64]), 2)(
+        int64, int64, int64, int64, nb_types.NamedUniTuple(int64, 2, Point), nb_types.NamedUniTuple(int64, 2, Point)
+    ),
+    nopython=True,
+    cache=True,
+    nogil=True,
+)
 def calc_tile_slice(tiy, tix, stride_y, stride_x, image_shape, tile_shape):
-    y_offset = int(image_shape[0] / 2. / stride_y - tile_shape[0] / 2.)
+    y_offset = int(image_shape[0] / 2.0 / stride_y - tile_shape[0] / 2.0)
     y_start = int(tiy * tile_shape[0] + y_offset)
     if y_start < 0:
         row_slice = (0, max(0, y_start + tile_shape[0]), 1)
     else:
         row_slice = (y_start, y_start + tile_shape[0], 1)
 
-    x_offset = int(image_shape[1] / 2. / stride_x - tile_shape[1] / 2.)
+    x_offset = int(image_shape[1] / 2.0 / stride_x - tile_shape[1] / 2.0)
     x_start = int(tix * tile_shape[1] + x_offset)
     if x_start < 0:
         col_slice = (0, max(0, x_start + tile_shape[1]), 1)
     else:
         col_slice = (x_start, x_start + tile_shape[1], 1)
     return row_slice, col_slice
 
 
-@jit(nb_types.UniTuple(nb_types.NamedUniTuple(float64, 2, Resolution), 2)(
-    int64,
-    int64,
-    int64,
-    int64,
-    int64,
-    int64,
-    int64,
-    int64
-), nopython=True, cache=True, nogil=True)
+@jit(
+    nb_types.UniTuple(nb_types.NamedUniTuple(float64, 2, Resolution), 2)(
+        int64, int64, int64, int64, int64, int64, int64, int64
+    ),
+    nopython=True,
+    cache=True,
+    nogil=True,
+)
 def calc_tile_fraction(tiy, tix, stride_y, stride_x, image_y, image_x, tile_y, tile_x):
     mt = max_tiles_available(image_y, image_x, tile_y, tile_x, stride_y, stride_x)
 
-    if tix < -mt[1] / 2. + 0.5:
+    if tix < -mt[1] / 2.0 + 0.5:
         # left edge tile
-        offset_x = -mt[1] / 2. + 0.5 - tix
+        offset_x = -mt[1] / 2.0 + 0.5 - tix
         factor_x = 1 - offset_x
-    elif mt[1] / 2. + 0.5 - tix < 1:
+    elif mt[1] / 2.0 + 0.5 - tix < 1:
         # right edge tile
-        offset_x = 0.
-        factor_x = mt[1] / 2. + 0.5 - tix
+        offset_x = 0.0
+        factor_x = mt[1] / 2.0 + 0.5 - tix
     else:
         # full tile
-        offset_x = 0.
-        factor_x = 1.
+        offset_x = 0.0
+        factor_x = 1.0
 
-    if tiy < -mt[0] / 2. + 0.5:
+    if tiy < -mt[0] / 2.0 + 0.5:
         # left edge tile
-        offset_y = -mt[0] / 2. + 0.5 - tiy
+        offset_y = -mt[0] / 2.0 + 0.5 - tiy
         factor_y = 1 - offset_y
-    elif mt[0] / 2. + 0.5 - tiy < 1:
+    elif mt[0] / 2.0 + 0.5 - tiy < 1:
         # right edge tile
-        offset_y = 0.
-        factor_y = mt[0] / 2. + 0.5 - tiy
+        offset_y = 0.0
+        factor_y = mt[0] / 2.0 + 0.5 - tiy
     else:
         # full tile
-        offset_y = 0.
-        factor_y = 1.
+        offset_y = 0.0
+        factor_y = 1.0
 
     factor_rez = Resolution(dy=factor_y, dx=factor_x)
     offset_rez = Resolution(dy=offset_y, dx=offset_x)
     return factor_rez, offset_rez
 
 
-@jit(nb_types.NamedUniTuple(int64, 2, Point)(
-    float64,
-    float64,
-    float64,
-    float64,
-    int64,
-    int64
-), nopython=True, cache=True, nogil=True)
+@jit(
+    nb_types.NamedUniTuple(int64, 2, Point)(float64, float64, float64, float64, int64, int64),
+    nopython=True,
+    cache=True,
+    nogil=True,
+)
 def calc_stride(v_dx, v_dy, t_dx, t_dy, overview_stride_y, overview_stride_x):
     # screen dy,dx in world distance per pixel
     # world distance per pixel for our data
     # compute texture pixels per screen pixels
-    tsy = min(overview_stride_y,
-              max(1, np.ceil(v_dy * PREFERRED_SCREEN_TO_TEXTURE_RATIO / t_dy)))
-    tsx = min(overview_stride_x,
-              max(1, np.ceil(v_dx * PREFERRED_SCREEN_TO_TEXTURE_RATIO / t_dx)))
+    tsy = min(overview_stride_y, max(1, np.ceil(v_dy * PREFERRED_SCREEN_TO_TEXTURE_RATIO / t_dy)))
+    tsx = min(overview_stride_x, max(1, np.ceil(v_dx * PREFERRED_SCREEN_TO_TEXTURE_RATIO / t_dx)))
 
     return Point(np.int64(tsy), np.int64(tsx))
 
 
-@jit(nb_types.UniTuple(int64, 2)(
-    int64,
-    int64,
-    nb_types.NamedUniTuple(int64, 2, Point)
-), nopython=True, cache=True, nogil=True)
+@jit(
+    nb_types.UniTuple(int64, 2)(int64, int64, nb_types.NamedUniTuple(int64, 2, Point)),
+    nopython=True,
+    cache=True,
+    nogil=True,
+)
 def calc_overview_stride(image_shape_y, image_shape_x, tile_shape):
     tsy = max(1, int(np.floor(image_shape_y / tile_shape[0])))
     tsx = max(1, int(np.floor(image_shape_x / tile_shape[1])))
     return tsy, tsx
 
 
-@jit(float32[:, :](
-    int64,
-    int64,
-    int64,
-    int64,
-    float64,
-    float64,
-    float64,
-    float64,
-    int64,
-    float64,
-    float64,
-    int64,
-    int64,
-    float64,
-    float64,
-    float32[:, :]
-), nopython=True, cache=True, nogil=True)
-def calc_vertex_coordinates(tiy, tix, stridey, stridex,
-                            factor_rez_dy, factor_rez_dx,
-                            offset_rez_dy, offset_rez_dx,
-                            tessellation_level, p_dx, p_dy,
-                            tile_shape_y, tile_shape_x,
-                            image_center_y, image_center_x,
-                            quads):
+@jit(
+    float32[:, :](
+        int64,
+        int64,
+        int64,
+        int64,
+        float64,
+        float64,
+        float64,
+        float64,
+        int64,
+        float64,
+        float64,
+        int64,
+        int64,
+        float64,
+        float64,
+        float32[:, :],
+    ),
+    nopython=True,
+    cache=True,
+    nogil=True,
+)
+def calc_vertex_coordinates(
+    tiy,
+    tix,
+    stridey,
+    stridex,
+    factor_rez_dy,
+    factor_rez_dx,
+    offset_rez_dy,
+    offset_rez_dx,
+    tessellation_level,
+    p_dx,
+    p_dy,
+    tile_shape_y,
+    tile_shape_x,
+    image_center_y,
+    image_center_x,
+    quads,
+):
     tile_w = p_dx * tile_shape_x * stridex
     tile_h = p_dy * tile_shape_y * stridey
-    origin_x = image_center_x - tile_w / 2.
-    origin_y = image_center_y + tile_h / 2.
+    origin_x = image_center_x - tile_w / 2.0
+    origin_y = image_center_y + tile_h / 2.0
     for x_idx in range(tessellation_level):
         for y_idx in range(tessellation_level):
             start_idx = x_idx * tessellation_level + y_idx
-            quads[start_idx * 6:(start_idx + 1) * 6, 0] *= tile_w * factor_rez_dx / tessellation_level
-            quads[start_idx * 6:(start_idx + 1) * 6, 0] += origin_x + tile_w * (
-                tix + offset_rez_dx + factor_rez_dx * x_idx / tessellation_level)
+            quads[start_idx * 6 : (start_idx + 1) * 6, 0] *= tile_w * factor_rez_dx / tessellation_level
+            quads[start_idx * 6 : (start_idx + 1) * 6, 0] += origin_x + tile_w * (
+                tix + offset_rez_dx + factor_rez_dx * x_idx / tessellation_level
+            )
             # Origin is upper-left so image goes dow,n
-            quads[start_idx * 6:(start_idx + 1) * 6, 1] *= -tile_h * factor_rez_dy / tessellation_level
-            quads[start_idx * 6:(start_idx + 1) * 6, 1] += origin_y - tile_h * (
-                tiy + offset_rez_dy + factor_rez_dy * y_idx / tessellation_level)
+            quads[start_idx * 6 : (start_idx + 1) * 6, 1] *= -tile_h * factor_rez_dy / tessellation_level
+            quads[start_idx * 6 : (start_idx + 1) * 6, 1] += origin_y - tile_h * (
+                tiy + offset_rez_dy + factor_rez_dy * y_idx / tessellation_level
+            )
     return quads
 
 
-@jit(float32[:, :](
-    int64,
-    int64,
-    float64,
-    float64,
-    int64,
-    int64,
-    int64,
-    int64,
-    int64,
-    float32[:, :]
-), nopython=True, cache=True, nogil=True)
-def calc_texture_coordinates(tiy, tix, factor_rez_dy, factor_rez_dx,
-                             tessellation_level,
-                             texture_size_y, texture_size_x,
-                             tile_shape_y, tile_shape_x,
-                             quads):
+@jit(
+    float32[:, :](int64, int64, float64, float64, int64, int64, int64, int64, int64, float32[:, :]),
+    nopython=True,
+    cache=True,
+    nogil=True,
+)
+def calc_texture_coordinates(
+    tiy,
+    tix,
+    factor_rez_dy,
+    factor_rez_dx,
+    tessellation_level,
+    texture_size_y,
+    texture_size_x,
+    tile_shape_y,
+    tile_shape_x,
+    quads,
+):
     # Now scale and translate the coordinates so they only apply to one tile in the texture
     one_tile_tex_width = 1.0 / texture_size_x * tile_shape_x
     one_tile_tex_height = 1.0 / texture_size_y * tile_shape_y
     for x_idx in range(tessellation_level):
         for y_idx in range(tessellation_level):
             start_idx = x_idx * tessellation_level + y_idx
             # offset for this tile isn't needed because the data should
             # have been inserted as close to the top-left of the texture
             # location as possible
-            quads[start_idx * 6:(start_idx + 1) * 6, 0] *= one_tile_tex_width * factor_rez_dx / tessellation_level
-            quads[start_idx * 6:(start_idx + 1) * 6, 0] += one_tile_tex_width * (
-                tix + factor_rez_dx * x_idx / tessellation_level)
-            quads[start_idx * 6:(start_idx + 1) * 6, 1] *= one_tile_tex_height * factor_rez_dy / tessellation_level
-            quads[start_idx * 6:(start_idx + 1) * 6, 1] += one_tile_tex_height * (
-                tiy + factor_rez_dy * y_idx / tessellation_level)
+            quads[start_idx * 6 : (start_idx + 1) * 6, 0] *= one_tile_tex_width * factor_rez_dx / tessellation_level
+            quads[start_idx * 6 : (start_idx + 1) * 6, 0] += one_tile_tex_width * (
+                tix + factor_rez_dx * x_idx / tessellation_level
+            )
+            quads[start_idx * 6 : (start_idx + 1) * 6, 1] *= one_tile_tex_height * factor_rez_dy / tessellation_level
+            quads[start_idx * 6 : (start_idx + 1) * 6, 1] += one_tile_tex_height * (
+                tiy + factor_rez_dy * y_idx / tessellation_level
+            )
     return quads
 
 
 class TileCalculator(object):
     """Common calculations for geographic image tile groups in an array or file
 
     Tiles are identified by (iy,ix) zero-based indicators.
 
     """
-    OVERSAMPLED = 'oversampled'
-    UNDERSAMPLED = 'undersampled'
-    WELLSAMPLED = 'wellsampled'
-
-    name = None
-    image_shape = None
-    pixel_rez = None
-    zero_point = None
-    tile_shape = None
-    # derived
-    image_extents_box = None  # word coordinates that this image and its tiles corresponds to
-    tiles_avail = None  # (ny,nx) available tile count for this image
-
-    def __init__(self, name, image_shape, ul_origin, pixel_rez,
-                 tile_shape=(DEFAULT_TILE_HEIGHT, DEFAULT_TILE_WIDTH),
-                 texture_shape=(DEFAULT_TEXTURE_HEIGHT, DEFAULT_TEXTURE_WIDTH),
-                 projection=DEFAULT_PROJECTION,
-                 wrap_lon=False):
+
+    OVERSAMPLED = "oversampled"
+    UNDERSAMPLED = "undersampled"
+    WELLSAMPLED = "wellsampled"
+
+    def __init__(
+        self,
+        name,
+        image_shape,
+        ul_origin,
+        pixel_rez,
+        tile_shape=(DEFAULT_TILE_HEIGHT, DEFAULT_TILE_WIDTH),
+        texture_shape=(DEFAULT_TEXTURE_HEIGHT, DEFAULT_TEXTURE_WIDTH),
+        projection=DEFAULT_PROJECTION,
+        wrap_lon=False,
+    ):
         """Initialize numbers used by multiple calculations.
 
         Args:
             name (str): the 'name' of the tile, typically the path of the file it represents
             image_shape (int, int): (height, width) in pixels
             ul_origin (float, float): (y, x) in world coords specifies upper-left coordinate of the image
             pixel_rez (float, float): (dy, dx) in world coords per pixel ascending from corner [0,0],
@@ -513,62 +507,84 @@
         self.ul_origin = Point(*ul_origin)
         self.pixel_rez = Resolution(np.float64(pixel_rez[0]), np.float64(pixel_rez[1]))
         self.tile_shape = Point(np.int64(tile_shape[0]), np.int64(tile_shape[1]))
         # in units of tiles:
         self.texture_shape = texture_shape
         # in units of data elements (float32):
         self.texture_size = (self.texture_shape[0] * self.tile_shape[0], self.texture_shape[1] * self.tile_shape[1])
+        # (ny,nx) available tile count for this image:
         self.image_tiles_avail = (self.image_shape[0] / self.tile_shape[0], self.image_shape[1] / self.tile_shape[1])
         self.wrap_lon = wrap_lon
 
         self.proj = Proj(projection)
+        # word coordinates that this image and its tiles corresponds to
         self.image_extents_box = e = Box(
             bottom=np.float64(self.ul_origin[0] - self.image_shape[0] * self.pixel_rez.dy),
             top=np.float64(self.ul_origin[0]),
             left=np.float64(self.ul_origin[1]),
             right=np.float64(self.ul_origin[1] + self.image_shape[1] * self.pixel_rez.dx),
         )
         # Array of points across the image space to be used as an estimate of image coverage
         # Used when checking if the image is viewable on the current canvas's projection
-        self.image_mesh = np.meshgrid(np.linspace(e.left, e.right, IMAGE_MESH_SIZE),
-                                      np.linspace(e.bottom, e.top, IMAGE_MESH_SIZE))
-        self.image_mesh = np.column_stack((self.image_mesh[0].ravel(), self.image_mesh[1].ravel(),))
-        self.image_center = Point(self.ul_origin.y - self.image_shape[0] / 2. * self.pixel_rez.dy,
-                                  self.ul_origin.x + self.image_shape[1] / 2. * self.pixel_rez.dx)
+        self.image_mesh = np.meshgrid(
+            np.linspace(e.left, e.right, IMAGE_MESH_SIZE), np.linspace(e.bottom, e.top, IMAGE_MESH_SIZE)
+        )
+        self.image_mesh = np.column_stack(
+            (
+                self.image_mesh[0].ravel(),
+                self.image_mesh[1].ravel(),
+            )
+        )
+        self.image_center = Point(
+            self.ul_origin.y - self.image_shape[0] / 2.0 * self.pixel_rez.dy,
+            self.ul_origin.x + self.image_shape[1] / 2.0 * self.pixel_rez.dx,
+        )
         # size of tile in image projection
         self.tile_size = Resolution(self.pixel_rez.dy * self.tile_shape[0], self.pixel_rez.dx * self.tile_shape[1])
         # maximum stride that we shouldn't lower resolution beyond
-        self.overview_stride = self.calc_overview_stride()
+        self.overview_stride = self._calc_overview_stride()
+
+    def visible_tiles(self, visible_geom, stride=None, extra_tiles_box=None) -> IndexBox:
+        """Get box of tile indexes for visible tiles that should be drawn.
+
+        Box indexes should be iterated with typical Python start:stop style
+        (inclusive start index, exclusive stop index). Tiles are expected to
+        be indexed (iy, ix) integer pairs. The ``extra_tiles_box`` value
+        specifies how many extra tiles to include around each edge.
 
-    def visible_tiles(self, visible_geom, stride=None, extra_tiles_box=None) -> Box:
-        """
-        given a visible world geometry and sampling, return (sampling-state, [Box-of-tiles-to-draw])
-        sampling state is WELLSAMPLED/OVERSAMPLED/UNDERSAMPLED
-        returned Box should be iterated per standard start:stop style
-        tiles are specified as (iy,ix) integer pairs
-        extra_box value says how many extra tiles to include around each edge
         """
         if stride is None:
             stride = Point(1, 1)
         if extra_tiles_box is None:
             extra_tiles_box = Box(0, 0, 0, 0)
         v = visible_geom
         e = extra_tiles_box
         return visible_tiles(
-            float(self.pixel_rez[0]), float(self.pixel_rez[1]),
-            float(self.tile_size[0]), float(self.tile_size[1]),
-            float(self.image_center[0]), float(self.image_center[1]),
-            int(self.image_shape[0]), int(self.image_shape[1]),
-            int(self.tile_shape[0]), int(self.tile_shape[1]),
-            float(v[0]), float(v[1]),
-            float(v[2]), float(v[3]),
-            float(v[4]), float(v[5]),
-            int(stride[0]), int(stride[1]),
-            int(e[0]), int(e[1]),
-            int(e[2]), int(e[3])
+            float(self.pixel_rez[0]),
+            float(self.pixel_rez[1]),
+            float(self.tile_size[0]),
+            float(self.tile_size[1]),
+            float(self.image_center[0]),
+            float(self.image_center[1]),
+            int(self.image_shape[0]),
+            int(self.image_shape[1]),
+            int(self.tile_shape[0]),
+            int(self.tile_shape[1]),
+            float(v[0]),
+            float(v[1]),
+            float(v[2]),
+            float(v[3]),
+            float(v[4]),
+            float(v[5]),
+            int(stride[0]),
+            int(stride[1]),
+            int(e[0]),
+            int(e[1]),
+            int(e[2]),
+            int(e[3]),
         )
 
     def calc_tile_slice(self, tiy, tix, stride):
         """Calculate the slice needed to get data.
 
         The returned slice assumes the original image data has already
         been reduced by the provided stride.
@@ -586,62 +602,88 @@
         """Calculate the fractional components of the specified tile
 
         Returns:
             (factor, offset): Two `Resolution` objects stating the relative size
                               of the tile compared to a whole tile and the
                               offset from the origin of a whole tile.
         """
-        return calc_tile_fraction(tiy, tix, stride[0], stride[1],
-                                  self.image_shape[0], self.image_shape[1], self.tile_shape[0], self.tile_shape[1])
+        return calc_tile_fraction(
+            tiy,
+            tix,
+            stride[0],
+            stride[1],
+            self.image_shape[0],
+            self.image_shape[1],
+            self.tile_shape[0],
+            self.tile_shape[1],
+        )
 
     def calc_stride(self, visible, texture=None):
         """
         given world geometry and sampling as a ViewBox or Resolution tuple
         calculate a conservative stride value for rendering a set of tiles
         :param visible: ViewBox or Resolution with world pixels per screen pixel
         :param texture: ViewBox or Resolution with texture resolution as world pixels per screen pixel
         """
         texture = texture or self.pixel_rez
-        return calc_stride(visible.dx, visible.dy, texture.dx, texture.dy, self.overview_stride[0].step,
-                           self.overview_stride[1].step)
+        return calc_stride(
+            visible.dx, visible.dy, texture.dx, texture.dy, self.overview_stride[0].step, self.overview_stride[1].step
+        )
 
-    def calc_overview_stride(self, image_shape=None):
+    def _calc_overview_stride(self, image_shape=None):
         image_shape = image_shape or self.image_shape
         # FUTURE: Come up with a fancier way of doing overviews like averaging each strided section, if needed
         tsy, tsx = calc_overview_stride(image_shape[0], image_shape[1], self.tile_shape)
         return slice(0, image_shape[0], tsy), slice(0, image_shape[1], tsx)
 
-    def calc_vertex_coordinates(self, tiy, tix, stridey, stridex,
-                                factor_rez, offset_rez, tessellation_level=1):
-        quad = np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0],
-                         [0, 0, 0], [1, 1, 0], [0, 1, 0]],
-                        dtype=np.float32)
+    def calc_vertex_coordinates(self, tiy, tix, stridey, stridex, factor_rez, offset_rez, tessellation_level=1):
+        quad = np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 0, 0], [1, 1, 0], [0, 1, 0]], dtype=np.float32)
         quads = np.tile(quad, (tessellation_level * tessellation_level, 1))
-        quads = calc_vertex_coordinates(tiy, tix, stridey, stridex, factor_rez.dy, factor_rez.dx,
-                                        offset_rez.dy, offset_rez.dx, tessellation_level,
-                                        self.pixel_rez.dx, self.pixel_rez.dy,
-                                        self.tile_shape.y, self.tile_shape.x,
-                                        self.image_center.y, self.image_center.x, quads)
+        quads = calc_vertex_coordinates(
+            tiy,
+            tix,
+            stridey,
+            stridex,
+            factor_rez.dy,
+            factor_rez.dx,
+            offset_rez.dy,
+            offset_rez.dx,
+            tessellation_level,
+            self.pixel_rez.dx,
+            self.pixel_rez.dy,
+            self.tile_shape.y,
+            self.tile_shape.x,
+            self.image_center.y,
+            self.image_center.x,
+            quads,
+        )
         quads = quads.reshape(tessellation_level * tessellation_level * 6, 3)
         return quads[:, :2]
 
     def calc_texture_coordinates(self, ttile_idx, factor_rez, offset_rez, tessellation_level=1):
         """Get texture coordinates for one tile as a quad.
 
         :param ttile_idx: int, texture 1D index that maps to some internal texture tile location
         """
         tiy = int(ttile_idx / self.texture_shape[1])
         tix = ttile_idx % self.texture_shape[1]
         # start with basic quad describing the entire texture
-        quad = np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0],
-                         [0, 0, 0], [1, 1, 0], [0, 1, 0]],
-                        dtype=np.float32)
+        quad = np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 0, 0], [1, 1, 0], [0, 1, 0]], dtype=np.float32)
         quads = np.tile(quad, (tessellation_level * tessellation_level, 1))
-        quads = calc_texture_coordinates(tiy, tix, factor_rez.dy, factor_rez.dx, tessellation_level,
-                                         self.texture_size[0], self.texture_size[1],
-                                         self.tile_shape.y, self.tile_shape.x, quads)
+        quads = calc_texture_coordinates(
+            tiy,
+            tix,
+            factor_rez.dy,
+            factor_rez.dx,
+            tessellation_level,
+            self.texture_size[0],
+            self.texture_size[1],
+            self.tile_shape.y,
+            self.tile_shape.x,
+            quads,
+        )
         quads = quads.reshape(6 * tessellation_level * tessellation_level, 3)
         quads = np.ascontiguousarray(quads[:, :2])
         return quads
 
     def calc_view_extents(self, canvas_point, image_point, canvas_size, dx, dy):
         return calc_view_extents(self.image_extents_box, canvas_point, image_point, canvas_size, dx, dy)
```

### Comparing `uwsift-1.2.3/uwsift/view/transform.py` & `uwsift-2.0.0b0/uwsift/view/transform.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,228 +4,304 @@
 
 SIFT uses PROJ.4 to define geographic projections and these are rarely
 possible to implement in Matrix transforms that come with VisPy.
 
 """
 
 import re
+from os import linesep as os_linesep
+from typing import List
 
 import numpy as np
 from pyproj import Proj, pj_ellps
+from vispy import glsl
 from vispy.visuals.shaders import Function
 from vispy.visuals.shaders.expression import TextExpression
+from vispy.visuals.shaders.parsing import find_program_variables
 from vispy.visuals.transforms import BaseTransform
 from vispy.visuals.transforms._util import arg_to_vec4
 
 
-# FIXME: This is the wrong usage of TextExpression. See if we can switch to
-#        doing what vispy math/constants.glsl does and how #define uses it
-class MacroExpression(TextExpression):
-    macro_regex = re.compile(r'^#define\s+(?P<name>\w+)\s+(?P<expression>[^\s]+)')
-
-    def __init__(self, text):
-        match = self.macro_regex.match(text)
-        if match is None:
-            raise ValueError("Invalid macro definition: {}".format(text))
-        match_dict = match.groupdict()
-        self._name = match_dict['name']
-        super(MacroExpression, self).__init__(text)
+class VariableDeclaration(TextExpression):
+    """TextExpression subclass for exposing GLSL variables to vispy glsl interface.
 
-    def definition(self, names, version=None, shader=None):
+    Parameters
+    ----------
+    name : str
+        Name of the variable.
+    text : str
+        Rvalue to be assigned to the variable.
+    """
+
+    def __init__(self, name: str, text: str) -> None:
+        self._name = name
+        super().__init__(text)
+
+    def definition(self, names, version=None, shader=None) -> str:
         return self.text
 
     @property
-    def name(self):
+    def name(self) -> str:
         return self._name
 
 
-COMMON_DEFINITIONS = """#define SPI     3.14159265359
-#define TWOPI   6.2831853071795864769
-#define ONEPI   3.14159265358979323846
-#define M_PI    3.14159265358979310
-#define M_PI_2  1.57079632679489660
-#define M_PI_4  0.78539816339744828
-#define M_FORTPI        M_PI_4                   /* pi/4 */
-#define M_HALFPI        M_PI_2                   /* pi/2 */
-#define M_PI_HALFPI     4.71238898038468985769   /* 1.5*pi */
-#define M_TWOPI         6.28318530717958647693   /* 2*pi */
-#define M_TWO_D_PI      M_2_PI                   /* 2/pi */
-#define M_TWOPI_HALFPI  7.85398163397448309616   /* 2.5*pi */
+class GLSL_Adapter(TextExpression):
+    """TextExpression subclass for parsing Macro definitions from .glsl header files and exposing them to vispy.
+
+    This class makes macro definitions accessible to vispy's shader code
+    processing. Assumes .glsl code to be parsed is accessible as python string. For reading
+    .glsl header code from a file see GLSL_FileAdapter subclass.
+
+    Parameters
+    ----------
+    text : str
+        Actual .glsl code string.
+    """
+
+    _expr_list: list = []
+
+    def __init__(self, text: str) -> None:
+        # Regular expression for parsing possibly include-guarded macro definitions from .glsl
+        # header files; makes strong assumptions about formatting of macro names by assuming
+        # underscores in front of and behind the macro name. In line with vispy .glsl shader code.
+        guard_pattern = re.compile(r"^#ifndef\s*(?P<guard>_[A-Za-z]*_)")
+        _guard_flag = False
+        for line in text.splitlines():
+            match_guard = guard_pattern.match(line)
+            var_match = find_program_variables(line)
+            if match_guard is not None:
+                _name = match_guard["guard"]
+                _text = match_guard.group(0) + os_linesep + f"#define {_name}"
+                self._expr_list.append(VariableDeclaration(_name, _text))
+                self._expr_list.append(VariableDeclaration(_name + "EIF", "#endif"))
+                _guard_flag = True
+            elif var_match is not None:
+                key_list = list(var_match.keys())
+                if len(key_list) > 1:
+                    raise ValueError("More than one variable definition per line " "not supported.")
+                elif len(key_list) != 0:
+                    self._expr_list.append(VariableDeclaration(key_list[0], line))
+        if _guard_flag:
+            # in case of include guards, shift #endif to bottom of
+            # expression list to match #ifndef
+            eif_token = self._expr_list[1]
+            self._expr_list[1:-1] = self._expr_list[2:]
+            self._expr_list[-1] = eif_token
+
+    @property
+    def expr_list(self) -> List[VariableDeclaration]:
+        return self._expr_list
+
+
+class GLSL_FileAdapter(GLSL_Adapter):
+    """GLSL_Adapter subclass adding the functionality to read .glsl header code from files.
+
+    Parameters
+    ----------
+    file_path : str
+        Path to .glsl header file.
+    """
+
+    def __init__(self, file_path: str) -> None:
+        text = glsl.get(file_path)
+        super(GLSL_FileAdapter, self).__init__(text)
+
+
+COMMON_VALUES_DEF = """const float SPI = 3.14159265359;
+const float TWOPI = 6.2831853071795864769;
+const float ONEPI = 3.14159265358979323846;
+const float M_FORTPI = M_PI_4;                      /* pi/4 */
+const float M_HALFPI = M_PI_2;                      /* pi/2 */
+const float M_PI_HALFPI = 4.71238898038468985769;   /* 1.5*pi */
+const float M_TWOPI = 6.28318530717958647693;       /* 2*pi */
+const float M_TWO_D_PI = 2.0/M_PI;                  /* 2/pi */
+const float M_TWOPI_HALFPI = 2.5 / M_PI;            /* 2.5*pi */
 """
-COMMON_DEFINITIONS = tuple(MacroExpression(line) for line in COMMON_DEFINITIONS.splitlines())
+
+math_consts = GLSL_FileAdapter("math/constants.glsl").expr_list
+COMMON_VALUES = GLSL_Adapter(COMMON_VALUES_DEF).expr_list
 M_FORTPI = M_PI_4 = 0.78539816339744828
 M_HALFPI = M_PI_2 = 1.57079632679489660
 
 
 def merc_init(proj_dict):
-    proj_dict.setdefault('lon_0', 0.)
-    proj_dict.setdefault('k0', 1.)
+    proj_dict.setdefault("lon_0", 0.0)
+    proj_dict.setdefault("k0", 1.0)
 
-    phits = 0.
-    is_phits = 'lat_ts' in proj_dict
+    phits = 0.0
+    is_phits = "lat_ts" in proj_dict
     if is_phits:
-        phits = np.radians(proj_dict['lat_ts'])
+        phits = np.radians(proj_dict["lat_ts"])
         if phits >= M_HALFPI:
             raise ValueError("PROJ.4 'lat_ts' parameter must be greater than PI/2")
 
-    if proj_dict['a'] != proj_dict['b']:
+    if proj_dict["a"] != proj_dict["b"]:
         # ellipsoid
         if is_phits:
-            proj_dict['k0'] = pj_msfn_py(np.sin(phits), np.cos(phits), proj_dict['es'])
+            proj_dict["k0"] = pj_msfn_py(np.sin(phits), np.cos(phits), proj_dict["es"])
     elif is_phits:
         # spheroid
-        proj_dict['k0'] = np.cos(phits)
+        proj_dict["k0"] = np.cos(phits)
 
     return proj_dict
 
 
 def lcc_init(proj_dict):
-    if 'lat_1' not in proj_dict:
+    if "lat_1" not in proj_dict:
         raise ValueError("PROJ.4 'lat_1' parameter is required for 'lcc' projection")
 
-    proj_dict.setdefault('lon_0', 0.)
-    if 'lat_2' not in proj_dict:
-        proj_dict['lat_2'] = proj_dict['lat_1']
-        if 'lat_0' not in proj_dict:
-            proj_dict['lat_0'] = proj_dict['lat_1']
-    proj_dict['phi1'] = np.radians(proj_dict['lat_1'])
-    proj_dict['phi2'] = np.radians(proj_dict['lat_2'])
-    proj_dict['phi0'] = np.radians(proj_dict['lat_0'])
+    proj_dict.setdefault("lon_0", 0.0)
+    if "lat_2" not in proj_dict:
+        proj_dict["lat_2"] = proj_dict["lat_1"]
+        if "lat_0" not in proj_dict:
+            proj_dict["lat_0"] = proj_dict["lat_1"]
+    proj_dict["phi1"] = np.radians(proj_dict["lat_1"])
+    proj_dict["phi2"] = np.radians(proj_dict["lat_2"])
+    proj_dict["phi0"] = np.radians(proj_dict["lat_0"])
 
-    if abs(proj_dict['phi1'] + proj_dict['phi2']) < 1e-10:
+    if abs(proj_dict["phi1"] + proj_dict["phi2"]) < 1e-10:
         raise ValueError("'lat_1' + 'lat_2' for 'lcc' projection when converted to radians must be greater than 1e-10.")
 
-    proj_dict['n'] = sinphi = np.sin(proj_dict['phi1'])
-    cosphi = np.cos(proj_dict['phi1'])
-    secant = abs(proj_dict['phi1'] - proj_dict['phi2']) >= 1e-10
-    proj_dict['ellips'] = proj_dict['a'] != proj_dict['b']
-    if proj_dict['ellips']:
+    proj_dict["n"] = sinphi = np.sin(proj_dict["phi1"])
+    cosphi = np.cos(proj_dict["phi1"])
+    secant = abs(proj_dict["phi1"] - proj_dict["phi2"]) >= 1e-10
+    proj_dict["ellips"] = proj_dict["a"] != proj_dict["b"]
+    if proj_dict["ellips"]:
         # ellipsoid
-        m1 = pj_msfn_py(sinphi, cosphi, proj_dict['es'])
-        ml1 = pj_tsfn_py(proj_dict['phi1'], sinphi, proj_dict['e'])
+        m1 = pj_msfn_py(sinphi, cosphi, proj_dict["es"])
+        ml1 = pj_tsfn_py(proj_dict["phi1"], sinphi, proj_dict["e"])
         if secant:
-            sinphi = np.sin(proj_dict['phi2'])
-            proj_dict['n'] = np.log(m1 / pj_msfn_py(sinphi, np.cos(proj_dict['phi2']), proj_dict['es']))
-            proj_dict['n'] /= np.log(ml1 / pj_tsfn_py(proj_dict['phi2'], sinphi, proj_dict['e']))
-        proj_dict['c'] = proj_dict['rho0'] = m1 * pow(ml1, -proj_dict['n']) / proj_dict['n']
-        proj_dict['rho0'] *= 0. if abs(abs(proj_dict['phi0']) - M_HALFPI) < 1e-10 else \
-            pow(pj_tsfn_py(proj_dict['phi0'], np.sin(proj_dict['phi0']), proj_dict['e']), proj_dict['n'])
+            sinphi = np.sin(proj_dict["phi2"])
+            proj_dict["n"] = np.log(m1 / pj_msfn_py(sinphi, np.cos(proj_dict["phi2"]), proj_dict["es"]))
+            proj_dict["n"] /= np.log(ml1 / pj_tsfn_py(proj_dict["phi2"], sinphi, proj_dict["e"]))
+        proj_dict["c"] = proj_dict["rho0"] = m1 * pow(ml1, -proj_dict["n"]) / proj_dict["n"]
+        proj_dict["rho0"] *= (
+            0.0
+            if abs(abs(proj_dict["phi0"]) - M_HALFPI) < 1e-10
+            else pow(pj_tsfn_py(proj_dict["phi0"], np.sin(proj_dict["phi0"]), proj_dict["e"]), proj_dict["n"])
+        )
     else:
         # spheroid
         if secant:
-            proj_dict['n'] = np.log(cosphi / np.cos(proj_dict['phi2'])) / np.log(
-                np.tan(M_FORTPI + 0.5 * proj_dict['phi2']) / np.tan(M_FORTPI + 0.5 * proj_dict['phi1'])
+            proj_dict["n"] = np.log(cosphi / np.cos(proj_dict["phi2"])) / np.log(
+                np.tan(M_FORTPI + 0.5 * proj_dict["phi2"]) / np.tan(M_FORTPI + 0.5 * proj_dict["phi1"])
             )
-        proj_dict['c'] = cosphi * pow(np.tan(M_FORTPI + 0.5 * proj_dict['phi1']), proj_dict['n']) / proj_dict['n']
-        proj_dict['rho0'] = 0. if abs(abs(proj_dict['phi0']) - M_HALFPI) < 1e-10 else \
-            proj_dict['c'] * pow(np.tan(M_FORTPI + 0.5 * proj_dict['phi0']), -proj_dict['n'])
-    proj_dict['ellips'] = 'true' if proj_dict['ellips'] else 'false'
+        proj_dict["c"] = cosphi * pow(np.tan(M_FORTPI + 0.5 * proj_dict["phi1"]), proj_dict["n"]) / proj_dict["n"]
+        proj_dict["rho0"] = (
+            0.0
+            if abs(abs(proj_dict["phi0"]) - M_HALFPI) < 1e-10
+            else proj_dict["c"] * pow(np.tan(M_FORTPI + 0.5 * proj_dict["phi0"]), -proj_dict["n"])
+        )
+    proj_dict["ellips"] = "true" if proj_dict["ellips"] else "false"
     return proj_dict
 
 
 def geos_init(proj_dict):
-    if 'h' not in proj_dict:
+    if "h" not in proj_dict:
         raise ValueError("PROJ.4 'h' parameter is required for 'geos' projection")
 
-    proj_dict.setdefault('lat_0', 0.)
-    proj_dict.setdefault('lon_0', 0.)
+    proj_dict.setdefault("lat_0", 0.0)
+    proj_dict.setdefault("lon_0", 0.0)
     # lat_0 is set to phi0 in the PROJ.4 C source code
     # if 'lat_0' not in proj_dict:
     #     raise ValueError("PROJ.4 'lat_0' parameter is required for 'geos' projection")
 
-    if 'sweep' not in proj_dict or proj_dict['sweep'] is None:
-        proj_dict['flip_axis'] = 'false'
-    elif proj_dict['sweep'] not in ['x', 'y']:
+    if "sweep" not in proj_dict or proj_dict["sweep"] is None:
+        proj_dict["flip_axis"] = "false"
+    elif proj_dict["sweep"] not in ["x", "y"]:
         raise ValueError("PROJ.4 'sweep' parameter must be 'x' or 'y'")
-    elif proj_dict['sweep'] == 'x':
-        proj_dict['flip_axis'] = 'true'
+    elif proj_dict["sweep"] == "x":
+        proj_dict["flip_axis"] = "true"
     else:
-        proj_dict['flip_axis'] = 'false'
+        proj_dict["flip_axis"] = "false"
 
-    proj_dict['radius_g_1'] = proj_dict['h'] / proj_dict['a']
-    proj_dict['radius_g'] = 1. + proj_dict['radius_g_1']
-    proj_dict['C'] = proj_dict['radius_g'] * proj_dict['radius_g'] - 1.0
-    if proj_dict['a'] != proj_dict['b']:
+    proj_dict["radius_g_1"] = proj_dict["h"] / proj_dict["a"]
+    proj_dict["radius_g"] = 1.0 + proj_dict["radius_g_1"]
+    proj_dict["C"] = proj_dict["radius_g"] * proj_dict["radius_g"] - 1.0
+    if proj_dict["a"] != proj_dict["b"]:
         # ellipsoid
-        proj_dict['one_es'] = 1. - proj_dict['es']
-        proj_dict['rone_es'] = 1. / proj_dict['one_es']
-        proj_dict['radius_p'] = np.sqrt(proj_dict['one_es'])
-        proj_dict['radius_p2'] = proj_dict['one_es']
-        proj_dict['radius_p_inv2'] = proj_dict['rone_es']
+        proj_dict["one_es"] = 1.0 - proj_dict["es"]
+        proj_dict["rone_es"] = 1.0 / proj_dict["one_es"]
+        proj_dict["radius_p"] = np.sqrt(proj_dict["one_es"])
+        proj_dict["radius_p2"] = proj_dict["one_es"]
+        proj_dict["radius_p_inv2"] = proj_dict["rone_es"]
     else:
-        proj_dict['radius_p'] = proj_dict['radius_p2'] = proj_dict['radius_p_inv2'] = 1.0
+        proj_dict["radius_p"] = proj_dict["radius_p2"] = proj_dict["radius_p_inv2"] = 1.0
     return proj_dict
 
 
 def stere_init(proj_dict):
     # Calculate phits
-    phits = abs(np.radians(proj_dict['lat_ts']) if 'lat_ts' in proj_dict else M_HALFPI)
+    phits = abs(np.radians(proj_dict["lat_ts"]) if "lat_ts" in proj_dict else M_HALFPI)
     # Determine mode
-    if abs(abs(np.radians(proj_dict['lat_0'])) - M_HALFPI) < 1e-10:
+    if abs(abs(np.radians(proj_dict["lat_0"])) - M_HALFPI) < 1e-10:
         # Assign "mode" in proj_dict to be GLSL for specific case (make sure to handle C-code case fallthrough):
         # 0 = n_pole, 1 = s_pole.
-        proj_dict['mode'] = 1 if proj_dict['lat_0'] < 0 else 0
-        if proj_dict['a'] != proj_dict['b']:
+        proj_dict["mode"] = 1 if proj_dict["lat_0"] < 0 else 0
+        if proj_dict["a"] != proj_dict["b"]:
             # ellipsoid
-            e = proj_dict['e']
+            e = proj_dict["e"]
             if abs(phits - M_HALFPI) < 1e-10:
-                proj_dict['akm1'] = 2. / np.sqrt((1 + e) ** (1 + e) * (1 - e) ** (1 - e))
+                proj_dict["akm1"] = 2.0 / np.sqrt((1 + e) ** (1 + e) * (1 - e) ** (1 - e))
             else:
-                proj_dict['akm1'] = np.cos(phits) / (
-                    pj_tsfn_py(phits, np.sin(phits), e) * np.sqrt(1. - (np.sin(phits) * e) ** 2))
+                proj_dict["akm1"] = np.cos(phits) / (
+                    pj_tsfn_py(phits, np.sin(phits), e) * np.sqrt(1.0 - (np.sin(phits) * e) ** 2)
+                )
         else:
             # sphere
-            proj_dict['akm1'] = np.cos(phits) / np.tan(M_FORTPI - .5 * phits) if abs(phits - M_HALFPI) >= 1e-10 else 2.
+            proj_dict["akm1"] = (
+                np.cos(phits) / np.tan(M_FORTPI - 0.5 * phits) if abs(phits - M_HALFPI) >= 1e-10 else 2.0
+            )
     else:
         # If EQUIT or OBLIQ mode:
         raise NotImplementedError("This projection mode is not supported yet.")
     return proj_dict
 
 
 def eqc_init(proj_dict):
-    proj_dict.setdefault('lat_0', 0.)
-    proj_dict.setdefault('lat_ts', proj_dict['lat_0'])
-    proj_dict['rc'] = np.cos(np.radians(proj_dict['lat_ts']))
-    if (proj_dict['rc'] <= 0.):
+    proj_dict.setdefault("lat_0", 0.0)
+    proj_dict.setdefault("lat_ts", proj_dict["lat_0"])
+    proj_dict["rc"] = np.cos(np.radians(proj_dict["lat_ts"]))
+    if proj_dict["rc"] <= 0.0:
         raise ValueError("PROJ.4 'lat_ts' parameter must be in range (-PI/2,PI/2)")
-    proj_dict['phi0'] = np.radians(proj_dict['lat_0'])
-    proj_dict['es'] = 0.
+    proj_dict["phi0"] = np.radians(proj_dict["lat_0"])
+    proj_dict["es"] = 0.0
     return proj_dict
 
 
 def latlong_init(proj_dict):
-    if 'over' in proj_dict:
+    if "over" in proj_dict:
         # proj_dict['offset'] = '360.'
-        proj_dict['offset'] = '0.'
+        proj_dict["offset"] = "0."
     else:
-        proj_dict['offset'] = '0.'
+        proj_dict["offset"] = "0."
     return proj_dict
 
 
 # proj_name -> (proj_init, map_ellps, map_spher, imap_ellps, imap_spher)
 # where 'map' is lon/lat to X/Y
 # and 'imap' is X/Y to lon/lat
 # WARNING: Need double {{ }} for functions for string formatting to work properly
 PROJECTIONS = {
-    'latlong': (
+    "longlat": (
         latlong_init,
         """vec4 latlong_map(vec4 pos) {{
-            return vec4(pos.x + {offset}, y, pos.z, pos.w);
+            return vec4(pos.x + {offset}, pos.y, pos.z, pos.w);
         }}""",
         """vec4 latlong_map(vec4 pos) {{
-            return vec4(pos.x + {offset}, y, pos.z, pos.w);
+            return vec4(pos.x + {offset}, pos.y, pos.z, pos.w);
         }}""",
         """vec4 latlong_imap(vec4 pos) {{
             return pos;
         }}""",
         """vec4 latlong_imap(vec4 pos) {{
             return pos;
         }}""",
     ),
-    'merc': (
+    "merc": (
         merc_init,
         """vec4 merc_map_e(vec4 pos) {{
             float lambda = radians(pos.x);
             {over}
             float phi = radians(pos.y);
             if (abs(abs(phi) - M_HALFPI) <= 1.e-10) {{
                 return vec4(1. / 0., 1. / 0., pos.z, pos.w);
@@ -260,15 +336,15 @@
             float lambda = {lon_0}f + x / ({a} * {k0});
             {over}
             lambda = degrees(lambda);
             float phi = degrees(2.f * atan(exp(y / ({a} * {k0}))) - M_PI / 2.f);
             return vec4(lambda, phi, pos.z, pos.w);
         }}""",
     ),
-    'lcc': (
+    "lcc": (
         lcc_init,
         """vec4 lcc_map_e(vec4 pos) {{
             float rho;
             float lambda = radians(pos.x - {lon_0});
             float phi = radians(pos.y);
             {over}
 
@@ -313,33 +389,33 @@
                 phi = {n} > 0. ? M_HALFPI : - M_HALFPI;
             }}
             {over}
             return vec4(degrees(lambda) + {lon_0}, degrees(phi), pos.z, pos.w);
         }}""",
         None,
     ),
-    'geos': (
+    "geos": (
         geos_init,
         """vec4 geos_map_e(vec4 pos) {{
             float lambda, phi, r, Vx, Vy, Vz, tmp, x, y;
             lambda = radians(pos.x - {lon_0});
             {over}
             phi = atan({radius_p2} * tan(radians(pos.y)));
             r = {radius_p} / hypot({radius_p} * cos(phi), sin(phi));
             Vx = r * cos(lambda) * cos(phi);
             Vy = r * sin(lambda) * cos(phi);
             Vz = r * sin(phi);
-        
+
             // TODO: Best way to 'discard' a vertex
             if ((({radius_g} - Vx) * Vx - Vy * Vy - Vz * Vz * {radius_p_inv2}) < 0.) {{
                return vec4(1. / 0., 1. / 0., pos.z, pos.w);
             }}
-        
+
             tmp = {radius_g} - Vx;
-        
+
             if ({flip_axis}) {{
                 x = {radius_g_1} * atan(Vy / hypot(Vz, tmp));
                 y = {radius_g_1} * atan(Vz / tmp);
             }} else {{
                 x = {radius_g_1} * atan(Vy / tmp);
                 y = {radius_g_1} * atan(Vz / hypot(Vy, tmp));
             }}
@@ -368,38 +444,38 @@
             }}
             return vec4(x, y, pos.z, pos.w);
         }}""",
         """vec4 geos_imap_e(vec4 pos) {{
             float a, b, k, det, x, y, Vx, Vy, Vz, lambda, phi;
             x = pos.x / {a};
             y = pos.y / {a};
-        
+
             Vx = -1.0;
             if ({flip_axis}) {{
                 Vz = tan(y / {radius_g_1});
                 Vy = tan(x / {radius_g_1}) * hypot(1.0, Vz);
             }} else {{
                 Vy = tan(x / {radius_g_1});
                 Vz = tan(y / {radius_g_1}) * hypot(1.0, Vy);
             }}
-        
+
             a = Vz / {radius_p};
             a = Vy * Vy + a * a + Vx * Vx;
             b = 2 * {radius_g} * Vx;
             det = ((b * b) - 4 * a * {C});
             if (det < 0.) {{
                 // FIXME
                 return vec4(1. / 0., 1. / 0., pos.z, pos.w);
             }}
-        
+
             k = (-b - sqrt(det)) / (2. * a);
             Vx = {radius_g} + k * Vx;
             Vy *= k;
             Vz *= k;
-        
+
             // atan2 in C
             lambda = atan(Vy, Vx);
             {over}
             phi = atan(Vz * cos(lambda) / Vx);
             phi = atan({radius_p_inv2} * tan(phi));
             return vec4(degrees(lambda) + {lon_0}, degrees(phi), pos.z, pos.w);
         }}""",
@@ -428,15 +504,15 @@
             Vz *= k;
             lambda = atan(Vy, Vx);
             {over}
             phi = atan(Vz * cos(lambda) / Vx);
             return vec4(degrees(lambda) + {lon_0}, degrees(phi), pos.z, pos.w);
         }}""",
     ),
-    'stere': (
+    "stere": (
         stere_init,
         """vec4 stere_map_e(vec4 pos) {{
             float lambda, phi, coslam, sinlam, sinphi, x, y;
             lambda = radians(pos.x - {lon_0});
             {over}
             phi = radians(pos.y);
             coslam = cos(lambda);
@@ -514,15 +590,15 @@
                 phi = asin({mode} == 1 ? -cosc : cosc);
             }}
             lambda = (x == 0. && y == 0.) ? 0. : atan(x, y);
             {over}
             return vec4(degrees(lambda) + {lon_0}, degrees(phi), pos.z, pos.w);
         }}""",
     ),
-    'eqc': (
+    "eqc": (
         eqc_init,
         None,
         """vec4 eqc_map_s(vec4 pos) {{
         {es};
         float lambda = radians(pos.x);
         {over}
         float phi = radians(pos.y);
@@ -537,66 +613,73 @@
             float lambda = x / {rc};
             {over}
             float phi = y + {phi0};
             return vec4(degrees(lambda), degrees(phi), pos.z, pos.w);
         }}""",
     ),
 }
-PROJECTIONS['lcc'] = (lcc_init,
-                      PROJECTIONS['lcc'][1],
-                      PROJECTIONS['lcc'][1],
-                      PROJECTIONS['lcc'][3],
-                      PROJECTIONS['lcc'][3],
-                      )
+PROJECTIONS["lcc"] = (
+    lcc_init,
+    PROJECTIONS["lcc"][1],
+    PROJECTIONS["lcc"][1],
+    PROJECTIONS["lcc"][3],
+    PROJECTIONS["lcc"][3],
+)
 
 # Misc GLSL functions used in one or more mapping functions above
-adjlon_func = Function("""
+adjlon_func = Function(
+    """
     float adjlon(float lon) {
         if (abs(lon) <= M_PI) return (lon);
         lon += M_PI; // adjust to 0..2pi rad
         lon -= M_TWOPI * floor(lon / M_TWOPI); // remove integral # of 'revolutions'
         lon -= M_PI;  // adjust back to -pi..pi rad
         return( lon );
     }
-    """)
+    """
+)
 
 # handle prime meridian shifts
 pm_func_str = """
     float adjlon(float lon) {{
         return lon + radians({pm});
     }}
 """
 
-pj_msfn = Function("""
+pj_msfn = Function(
+    """
     float pj_msfn(float sinphi, float cosphi, float es) {
         return (cosphi / sqrt (1. - es * sinphi * sinphi));
     }
-    """)
+    """
+)
 
 
 def pj_msfn_py(sinphi, cosphi, es):
-    return cosphi / np.sqrt(1. - es * sinphi * sinphi)
+    return cosphi / np.sqrt(1.0 - es * sinphi * sinphi)
 
 
-pj_tsfn = Function("""
+pj_tsfn = Function(
+    """
     float pj_tsfn(float phi, float sinphi, float e) {
         sinphi *= e;
         return (tan (.5 * (M_HALFPI - phi)) /
            pow((1. - sinphi) / (1. + sinphi), .5 * e));
     }
-    """)
+    """
+)
 
 
 def pj_tsfn_py(phi, sinphi, e):
     sinphi *= e
-    return (np.tan(.5 * (M_HALFPI - phi)) /
-            pow((1. - sinphi) / (1. + sinphi), .5 * e))
+    return np.tan(0.5 * (M_HALFPI - phi)) / pow((1.0 - sinphi) / (1.0 + sinphi), 0.5 * e)
 
 
-pj_phi2 = Function("""
+pj_phi2 = Function(
+    """
     float pj_phi2(float ts, float e) {
         float eccnth, Phi, con, dphi;
 
         eccnth = .5 * e;
         Phi = M_HALFPI - 2. * atan (ts);
         for (int i=15; i >= 0; --i) {
             con = e * sin(Phi);
@@ -606,17 +689,19 @@
                 break;
             }
         }
         //if (i <= 0)
         //    pj_ctx_set_errno( ctx, -18 );
         return Phi;
     }
-    """)
+    """
+)
 
-hypot = Function("""
+hypot = Function(
+    """
 float hypot(float x, float y) {
     if ( x < 0.)
         x = -x;
     else if (x == 0.)
         return (y < 0. ? -y : y);
     if (y < 0.)
         y = -y;
@@ -626,15 +711,16 @@
         x /= y;
         return ( y * sqrt( 1. + x * x ) );
     } else {
         y /= x;
         return ( x * sqrt( 1. + y * y ) );
     }
 }
-""")
+"""
+)
 
 
 class PROJ4Transform(BaseTransform):
     glsl_map = None
 
     glsl_imap = None
 
@@ -657,135 +743,137 @@
     # Scale factors are applied equally to all axes.
     Isometric = False
 
     def __init__(self, proj4_str, inverse=False):
         self.proj4_str = proj4_str
         self.proj = Proj(proj4_str)
         self._proj4_inverse = inverse
-        proj_dict = self.create_proj_dict(proj4_str)
+        proj_dict = self._create_proj_dict(proj4_str)
 
         # Get the specific functions for this projection
-        proj_funcs = PROJECTIONS[proj_dict['proj']]
+        proj_funcs = PROJECTIONS[proj_dict["proj"]]
         # set default function parameters
         proj_init = proj_funcs[0]
         proj_args = proj_init(proj_dict)
 
-        if 'pm' in proj_args:
+        if "pm" in proj_args:
             # force to float
-            proj_args['pm'] = float(proj_args['pm'])
-            proj_args['over'] = 'lambda = adjlon(lambda);'
-        elif proj_args.get('over'):
-            proj_args['over'] = ''
+            proj_args["pm"] = float(proj_args["pm"])
+            proj_args["over"] = "lambda = adjlon(lambda);"
+        elif proj_args.get("over"):
+            proj_args["over"] = ""
         else:
-            proj_args['over'] = 'lambda = adjlon(lambda);'
+            proj_args["over"] = "lambda = adjlon(lambda);"
 
-        if proj_dict['a'] == proj_dict['b']:
+        if proj_dict["a"] == proj_dict["b"]:
             # spheroid
             self.glsl_map = proj_funcs[2]
             self.glsl_imap = proj_funcs[4]
             if self.glsl_map is None or self.glsl_imap is None:
-                raise ValueError("Spheroid transform for {} not implemented yet".format(proj_dict['proj']))
+                raise ValueError("Spheroid transform for {} not implemented yet".format(proj_dict["proj"]))
         else:
             # ellipsoid
             self.glsl_map = proj_funcs[1]
             self.glsl_imap = proj_funcs[3]
             if self.glsl_map is None or self.glsl_imap is None:
-                raise ValueError("Ellipsoid transform for {} not implemented yet".format(proj_dict['proj']))
+                raise ValueError("Ellipsoid transform for {} not implemented yet".format(proj_dict["proj"]))
 
         self.glsl_map = self.glsl_map.format(**proj_args)
         self.glsl_imap = self.glsl_imap.format(**proj_args)
 
         if self._proj4_inverse:
             self.glsl_map, self.glsl_imap = self.glsl_imap, self.glsl_map
 
         super(PROJ4Transform, self).__init__()
 
         # Add common definitions and functions
-        for d in COMMON_DEFINITIONS + (pj_tsfn, pj_phi2, hypot):
+        for d in math_consts + COMMON_VALUES + [pj_tsfn, pj_phi2, hypot]:
             self._shader_map._add_dep(d)
             self._shader_imap._add_dep(d)
 
-        if 'pm' in proj_args:
+        if "pm" in proj_args:
             pm_func = Function(pm_func_str.format(**proj_args))
             self._shader_map._add_dep(pm_func)
             self._shader_imap._add_dep(pm_func)
-        elif proj_args['over']:
+        elif proj_args["over"]:
             self._shader_map._add_dep(adjlon_func)
             self._shader_imap._add_dep(adjlon_func)
 
         # Add special handling of possible infinity lon/lat values
-        self._shader_map['pre'] = """
+        self._shader_map[
+            "pre"
+        ] = """
     if (abs(pos.x) > 1e30 || abs(pos.y) > 1e30)
         return vec4(1. / 0., 1. / 0., pos.z, pos.w);
         """
 
         # print(self._shader_map.compile())
 
     @property
     def is_geographic(self):
-        if hasattr(self.proj, 'crs'):
+        if hasattr(self.proj, "crs"):
             # pyproj 2.0+
             return self.proj.crs.is_geographic
         return self.proj.is_latlong()
 
-    def create_proj_dict(self, proj_str):
+    def _create_proj_dict(self, proj_str):  # noqa: C901
         d = tuple(x.replace("+", "").split("=") for x in proj_str.split(" "))
-        d = dict((x[0], x[1] if len(x) > 1 else 'true') for x in d)
+        d = dict((x[0], x[1] if len(x) > 1 else "true") for x in d)
 
         # convert numerical parameters to floats
         for k in d.keys():
             try:
                 d[k] = float(d[k])
             except ValueError:
                 pass
 
-        d['proj4_str'] = proj_str
+        d["proj4_str"] = proj_str
 
         # if they haven't provided a radius then they must have provided a datum or ellps
-        if 'R' in d:
+        if "R" in d:
             # spheroid
-            d.setdefault('a', d['R'])
-            d.setdefault('b', d['R'])
-        if 'a' not in d:
-            if 'datum' not in d:
-                d.setdefault('ellps', d.setdefault('datum', 'WGS84'))
+            d.setdefault("a", d["R"])
+            d.setdefault("b", d["R"])
+        if "a" not in d:
+            if "datum" not in d:
+                d.setdefault("ellps", d.setdefault("datum", "WGS84"))
             else:
-                d.setdefault('ellps', d.get('datum'))
+                d.setdefault("ellps", d.get("datum"))
 
         # if they provided an ellps/datum fill in information we know about it
-        if d.get('ellps') is not None:
+        if d.get("ellps") is not None:
             # get information on the ellps being used
-            ellps_info = pj_ellps[d['ellps']]
-            for k in ['a', 'b', 'rf']:
+            ellps_info = pj_ellps[d["ellps"]]
+            for k in ["a", "b", "rf"]:
                 if k in ellps_info:
                     d.setdefault(k, ellps_info[k])
 
         # derive b, es, f, e
-        if 'rf' not in d:
-            if 'f' in d:
-                d['rf'] = 1. / d['f']
-            elif d['a'] == d['b']:
-                d['rf'] = 0.
+        if "rf" not in d:
+            if "f" in d:
+                d["rf"] = 1.0 / d["f"]
+            elif d["a"] == d["b"]:
+                d["rf"] = 0.0
             else:
-                d['rf'] = d['a'] / (d['a'] - d['b'])
-        if 'f' not in d:
-            if d['rf']:
-                d['f'] = 1. / d['rf']
+                d["rf"] = d["a"] / (d["a"] - d["b"])
+        if "f" not in d:
+            if d["rf"]:
+                d["f"] = 1.0 / d["rf"]
             else:
-                d['f'] = 0.
-        if 'b' not in d:
+                d["f"] = 0.0
+        if "b" not in d:
             # a and rf must be in the dict
-            d['b'] = d['a'] * (1. - d['f'])
-        if 'es' not in d:
-            if 'e' in d:
-                d['es'] = d['e'] ** 2
+            d["b"] = d["a"] * (1.0 - d["f"])
+        if "es" not in d:
+            if "e" in d:
+                d["es"] = d["e"] ** 2
             else:
-                d['es'] = 2 * d['f'] - d['f'] ** 2
-        if 'e' not in d:
-            d['e'] = d['es'] ** 0.5
+                d["es"] = 2 * d["f"] - d["f"] ** 2
+        if "e" not in d:
+            d["e"] = d["es"] ** 0.5
 
         return d
 
     @arg_to_vec4
     def map(self, coords):
         """Map coordinates
```

### Comparing `uwsift-1.2.3/uwsift/view/visuals.py` & `uwsift-2.0.0b0/uwsift/view/visuals.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,57 +1,72 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
 """
 visuals.py
 ~~~~~~~~~~~
 
 PURPOSE
-Layer representation - the "physical" realization of content to draw on the map.
-A layer representation can have multiple levels of detail
-
-A factory will convert URIs into LayerReps
-LayerReps are managed by document, and handed off to the MapWidget as part of a LayerDrawingPlan
+Dataset representation - the "physical" realization of content to draw on the map.
+A dataset representation can have multiple levels of detail.
 
 REFERENCES
 
 
 REQUIRES
 
 
 :author: R.K.Garcia <rayg@ssec.wisc.edu>
 :copyright: 2014 by University of Wisconsin Regents, see AUTHORS for more details
 :license: GPLv3, see LICENSE for more details
 """
 
 import logging
 from datetime import datetime
+from typing import Optional
 
 import numpy as np
 import shapefile
+from vispy.color import Color
+from vispy.gloo import VertexBuffer
 from vispy.scene.visuals import create_visual_node
-from vispy.visuals import LineVisual, ImageVisual, IsocurveVisual
-# The below imports are needed because we subclassed the ImageVisual
-from vispy.visuals.shaders import Function, FunctionChain
-from vispy.gloo.texture import should_cast_to_f32
+from vispy.util.profiler import Profiler
+from vispy.visuals import ImageVisual, IsocurveVisual, LineVisual
 
+# The below imports are needed because we subclassed ImageVisual and ArrowVisual
+from vispy.visuals.line.arrow import ArrowVisual, _ArrowHeadVisual
+from vispy.visuals.line.line import _AggLineVisual, _GLLineVisual
+from vispy.visuals.shaders import Function, FunctionChain
+from vispy.visuals.transforms import as_vec4
 
 from uwsift.common import (
     DEFAULT_PROJECTION,
-    DEFAULT_TILE_HEIGHT,
-    DEFAULT_TILE_WIDTH,
     DEFAULT_TEXTURE_HEIGHT,
     DEFAULT_TEXTURE_WIDTH,
+    DEFAULT_TILE_HEIGHT,
+    DEFAULT_TILE_WIDTH,
     TESS_LEVEL,
-    Box, Point, Resolution, ViewBox,
+    Box,
+    IndexBox,
+    Point,
+    Resolution,
+    ViewBox,
+)
+from uwsift.view.texture_atlas import (
+    MultiChannelGPUScaledTexture2D,
+    MultiChannelTextureAtlas2D,
+    TextureAtlas2D,
+)
+from uwsift.view.tile_calculator import (
+    TileCalculator,
+    calc_pixel_size,
+    get_reference_points,
 )
-from uwsift.view.texture_atlas import TextureAtlas2D, MultiChannelTextureAtlas2D, MultiChannelGPUScaledTexture2D
-from uwsift.view.tile_calculator import TileCalculator, calc_pixel_size, get_reference_points
 
-__author__ = 'rayg'
-__docformat__ = 'reStructuredText'
+__author__ = "rayg"
+__docformat__ = "reStructuredText"
 
 LOG = logging.getLogger(__name__)
 # if the absolute value of a vertex coordinate is beyond 'CANVAS_EPSILON'
 # then we consider it invalid
 # these values can get large when zoomed way in
 CANVAS_EPSILON = 1e5
 
@@ -79,43 +94,40 @@
     """
 
     def __init__(self, num_tiles):
         self.num_tiles = num_tiles
         self.reset()
 
     def __getitem__(self, item):
-        """Get the texture index associated with this image tile index.
-        """
+        """Get the texture index associated with this image tile index."""
         return self.itile_cache[item]
 
     def __contains__(self, item):
-        """Have we already added this image tile index (yidx, xidx).
-        """
+        """Have we already added this image tile index (yidx, xidx)."""
         return item in self.itile_cache
 
     def reset(self):
         self.itile_cache = {}
         self._rev_cache = {}
         # True if the data doesn't matter, False if data matters
         self.tile_free = [True] * self.num_tiles
         self.itile_age = []
 
-    def next_available_tile(self):
+    def _next_available_tile(self):
         for idx, tile_free in enumerate(self.tile_free):
             if tile_free:
                 return idx
 
         # We don't have any free tiles, remove the oldest one
         LOG.debug("Expiring image tile from texture atlas: %r", self.itile_age[0])
-        ttile_idx = self.remove_tile(self.itile_age[0])
+        ttile_idx = self._remove_tile(self.itile_age[0])
         return ttile_idx
 
-    def refresh_age(self, itile_idx):
-        """Update the age of an image tile so it is less likely to expire.
-        """
+    def _refresh_age(self, itile_idx):
+        """Update the age of an image tile so it is less likely to expire."""
         try:
             # Remove it from wherever it is
             self.itile_age.remove(itile_idx)
         except ValueError:
             # we haven't heard about this tile, that's ok, we'll add it to the list
             pass
 
@@ -126,40 +138,45 @@
         """Get texture index for new tile. If tile is already known return its current location.
 
         Note, this should be called even when the caller knows the tile exists to refresh the "age".
         """
         # Have we already added this tile, get the tile index
         if itile_idx in self:
             if expires:
-                self.refresh_age(itile_idx)
+                self._refresh_age(itile_idx)
             return self[itile_idx]
 
-        ttile_idx = self.next_available_tile()
+        ttile_idx = self._next_available_tile()
 
         self.itile_cache[itile_idx] = ttile_idx
         self._rev_cache[ttile_idx] = itile_idx
         self.tile_free[ttile_idx] = False
         if expires:
-            self.refresh_age(itile_idx)
+            self._refresh_age(itile_idx)
         return ttile_idx
 
-    def remove_tile(self, itile_idx):
+    def _remove_tile(self, itile_idx):
         ttile_idx = self.itile_cache.pop(itile_idx)
         self._rev_cache.pop(ttile_idx)
         self.tile_free[ttile_idx] = True
         self.itile_age.remove(itile_idx)
         return ttile_idx
 
 
 class SIFTTiledGeolocatedMixin:
-    def __init__(self, data, *area_params,
-                 tile_shape=(DEFAULT_TILE_HEIGHT, DEFAULT_TILE_WIDTH),
-                 texture_shape=(DEFAULT_TEXTURE_HEIGHT, DEFAULT_TEXTURE_WIDTH),
-                 wrap_lon=False, projection=DEFAULT_PROJECTION,
-                 **visual_kwargs):
+    def __init__(
+        self,
+        data,
+        *area_params,
+        tile_shape=(DEFAULT_TILE_HEIGHT, DEFAULT_TILE_WIDTH),
+        texture_shape=(DEFAULT_TEXTURE_HEIGHT, DEFAULT_TEXTURE_WIDTH),
+        wrap_lon=False,
+        projection=DEFAULT_PROJECTION,
+        **visual_kwargs,
+    ):
         origin_x, origin_y, cell_width, cell_height = area_params
         if visual_kwargs.get("method", "subdivide") != "subdivide":
             raise ValueError("Only 'subdivide' drawing method is supported.")
         visual_kwargs["method"] = "subdivide"
         if "grid" in visual_kwargs:
             raise ValueError("The 'grid' keyword argument is not supported with the tiled mixin.")
 
@@ -172,33 +189,23 @@
             origin_y,
             cell_width,
             cell_height,
             projection,
             texture_shape,
             tile_shape,
             wrap_lon,
-            visual_kwargs.get('shape'),
+            visual_kwargs.get("shape"),
             data,
         )
 
         # Call the init of the Visual
         super().__init__(data, **visual_kwargs)
 
     def _init_geo_parameters(
-            self,
-            origin_x,
-            origin_y,
-            cell_width,
-            cell_height,
-            projection,
-            texture_shape,
-            tile_shape,
-            wrap_lon,
-            shape,
-            data
+        self, origin_x, origin_y, cell_width, cell_height, projection, texture_shape, tile_shape, wrap_lon, shape, data
     ):
         self._viewable_mesh_mask = None
         self._ref1 = None
         self._ref2 = None
 
         self.origin_x = origin_x
         self.origin_y = origin_y
@@ -207,15 +214,15 @@
         self.texture_shape = texture_shape
         self.tile_shape = tile_shape
         self.num_tex_tiles = self.texture_shape[0] * self.texture_shape[1]
         self._stride = (0, 0)
         self._latest_tile_box = None
         self.wrap_lon = wrap_lon
         self._tiles = {}
-        assert (shape or data is not None), "`data` or `shape` must be provided"
+        assert shape or data is not None, "`data` or `shape` must be provided"  # nosec B101
         self.shape = shape or data.shape
         self.ndim = len(self.shape) or data.ndim
 
         # Where does this image lie in this lonely world
         self.calc = TileCalculator(
             self.name,
             self.shape,
@@ -230,21 +237,23 @@
         self.texture_state = TextureTileState(self.num_tex_tiles)
 
     def _normalize_data(self, data):
         if data is not None and data.dtype == np.float64:
             data = data.astype(np.float32)
         return data
 
-    def _build_texture_tiles(self, data, stride, tile_box: Box):
-        """Prepare and organize strided data in to individual tiles with associated information.
-        """
+    def _build_texture_tiles(self, data, stride, tile_box: IndexBox):
+        """Prepare and organize strided data in to individual tiles with associated information."""
         data = self._normalize_data(data)
 
-        LOG.debug("Uploading texture data for %d tiles (%r)",
-                  (tile_box.bottom - tile_box.top) * (tile_box.right - tile_box.left), tile_box)
+        LOG.debug(
+            "Uploading texture data for %d tiles (%r)",
+            (tile_box.bottom - tile_box.top) * (tile_box.right - tile_box.left),
+            tile_box,
+        )
         # Tiles start at upper-left so go from top to bottom
         tiles_info = []
         for tiy in range(tile_box.top, tile_box.bottom):
             for tix in range(tile_box.left, tile_box.right):
                 already_in = (stride, tiy, tix) in self.texture_state
                 # Update the age if already in there
                 # Assume that texture_state does not change from the main thread if this is run in another
@@ -269,15 +278,15 @@
         return np.array(data[y_slice, x_slice], dtype=np.float32)
 
     def _set_texture_tiles(self, tiles_info):
         for tile_info in tiles_info:
             stride, tiy, tix, tex_tile_idx, data = tile_info
             self._texture.set_tile_data(tex_tile_idx, data)
 
-    def _build_vertex_tiles(self, preferred_stride, tile_box: Box):
+    def _build_vertex_tiles(self, preferred_stride, tile_box: IndexBox):
         """Rebuild the vertex buffers used for rendering the image when using
         the subdivide method.
 
         SIFT Note: Copied from 0.5.0dev original ImageVisual class
         """
         total_num_tiles = (tile_box.bottom - tile_box.top) * (tile_box.right - tile_box.left)
 
@@ -304,35 +313,41 @@
 
                 # Check if the tile we want to draw is actually in the GPU
                 # if not (atlas too small?) fill with zeros and keep going
                 if (preferred_stride, tiy, tix) not in self.texture_state:
                     # THIS SHOULD NEVER HAPPEN IF TEXTURE BUILDING IS DONE CORRECTLY AND THE ATLAS IS BIG ENOUGH
                     tile_start = TESS_LEVEL * TESS_LEVEL * used_tile_idx * 6
                     tile_end = TESS_LEVEL * TESS_LEVEL * (used_tile_idx + 1) * 6
-                    tex_coords[tile_start: tile_end, :] = 0
-                    vertices[tile_start: tile_end, :] = 0
+                    tex_coords[tile_start:tile_end, :] = 0
+                    vertices[tile_start:tile_end, :] = 0
                     continue
 
                 # we should have already loaded the texture data in to the GPU so get the index of that texture
                 tex_tile_idx = self.texture_state[(preferred_stride, tiy, tix)]
                 factor_rez, offset_rez = self.calc.calc_tile_fraction(tiy, tix, preferred_stride)
-                tex_coords[tl * used_tile_idx * 6: tl * (used_tile_idx + 1) * 6, :] = \
-                    self.calc.calc_texture_coordinates(tex_tile_idx, factor_rez, offset_rez,
-                                                       tessellation_level=TESS_LEVEL)
-                vertices[tl * used_tile_idx * 6: tl * (used_tile_idx + 1) * 6, :] = self.calc.calc_vertex_coordinates(
-                    tiy, tix,
-                    preferred_stride[0], preferred_stride[1],
-                    factor_rez, offset_rez,
-                    tessellation_level=TESS_LEVEL)
+                tex_coords[
+                    tl * used_tile_idx * 6 : tl * (used_tile_idx + 1) * 6, :
+                ] = self.calc.calc_texture_coordinates(
+                    tex_tile_idx, factor_rez, offset_rez, tessellation_level=TESS_LEVEL
+                )
+                vertices[tl * used_tile_idx * 6 : tl * (used_tile_idx + 1) * 6, :] = self.calc.calc_vertex_coordinates(
+                    tiy,
+                    tix,
+                    preferred_stride[0],
+                    preferred_stride[1],
+                    factor_rez,
+                    offset_rez,
+                    tessellation_level=TESS_LEVEL,
+                )
 
         return vertices, tex_coords
 
     def _set_vertex_tiles(self, vertices, tex_coords):
-        self._subdiv_position.set_data(vertices.astype('float32'))
-        self._subdiv_texcoord.set_data(tex_coords.astype('float32'))
+        self._subdiv_position.set_data(vertices.astype("float32"))
+        self._subdiv_texcoord.set_data(tex_coords.astype("float32"))
 
     def determine_reference_points(self):
         # Image points transformed to canvas coordinates
         img_cmesh = self.transforms.get_transform().map(self.calc.image_mesh)
         # Mask any points that are really far off screen (can't be transformed)
         valid_mask = (np.abs(img_cmesh[:, 0]) < CANVAS_EPSILON) & (np.abs(img_cmesh[:, 1]) < CANVAS_EPSILON)
         # The image mesh projected to canvas coordinates (valid only)
@@ -343,16 +358,16 @@
         if not img_cmesh[:, 0].size or not img_cmesh[:, 1].size:
             self._viewable_mesh_mask = None
             self._ref1, self._ref2 = None, None
             return
 
         x_cmin, x_cmax = img_cmesh[:, 0].min(), img_cmesh[:, 0].max()
         y_cmin, y_cmax = img_cmesh[:, 1].min(), img_cmesh[:, 1].max()
-        center_x = (x_cmax - x_cmin) / 2. + x_cmin
-        center_y = (y_cmax - y_cmin) / 2. + y_cmin
+        center_x = (x_cmax - x_cmin) / 2.0 + x_cmin
+        center_y = (y_cmax - y_cmin) / 2.0 + y_cmin
         dist = img_cmesh.copy()
         dist[:, 0] = center_x - img_cmesh[:, 0]
         dist[:, 1] = center_y - img_cmesh[:, 1]
         self._viewable_mesh_mask = valid_mask
         self._ref1, self._ref2 = get_reference_points(dist, img_vbox)
 
     def get_view_box(self):
@@ -374,17 +389,17 @@
         img_cmesh = self.transforms.get_transform().map(self.calc.image_mesh)
         # The image mesh projected to canvas coordinates (valid only)
         img_cmesh = img_cmesh[self._viewable_mesh_mask]
         # The image mesh of only valid "viewable" projected coordinates
         img_vbox = self.calc.image_mesh[self._viewable_mesh_mask]
 
         ref_idx_1, ref_idx_2 = get_reference_points(img_cmesh, img_vbox)
-        dx, dy = calc_pixel_size(img_cmesh[(self._ref1, self._ref2), :],
-                                 img_vbox[(self._ref1, self._ref2), :],
-                                 self.canvas.size)
+        dx, dy = calc_pixel_size(
+            img_cmesh[(self._ref1, self._ref2), :], img_vbox[(self._ref1, self._ref2), :], self.canvas.size
+        )
         view_extents = self.calc.calc_view_extents(img_cmesh[ref_idx_1], img_vbox[ref_idx_1], self.canvas.size, dx, dy)
         return ViewBox(*view_extents, dx=dx, dy=dy)
 
     def _get_stride(self, view_box):
         return self.calc.calc_stride(view_box)
 
     def assess(self):
@@ -398,25 +413,29 @@
             tile_box = self.calc.visible_tiles(view_box, stride=preferred_stride, extra_tiles_box=Box(1, 1, 1, 1))
         except ValueError as e:
             # If image is outside of canvas, then an exception will be raised
             LOG.warning("Could not determine viewable image area for '{}': {}".format(self.name, e))
             return False, self._stride, self._latest_tile_box
 
         num_tiles = (tile_box.bottom - tile_box.top) * (tile_box.right - tile_box.left)
-        LOG.debug("Assessment: Prefer '%s' have '%s', was looking at %r, now looking at %r",
-                  preferred_stride, self._stride, self._latest_tile_box, tile_box)
+        LOG.debug(
+            "Assessment: Prefer '%s' have '%s', was looking at %r, now looking at %r",
+            preferred_stride,
+            self._stride,
+            self._latest_tile_box,
+            tile_box,
+        )
 
         # If we zoomed out or we panned
         need_retile = (num_tiles > 0) and (preferred_stride != self._stride or self._latest_tile_box != tile_box)
 
         return need_retile, preferred_stride, tile_box
 
     def retile(self, data, preferred_stride, tile_box):
-        """Get data from workspace and retile/retexture as needed.
-        """
+        """Get data from workspace and retile/retexture as needed."""
         tiles_info = self._build_texture_tiles(data, preferred_stride, tile_box)
         vertices, tex_coords = self._build_vertex_tiles(preferred_stride, tile_box)
         return tiles_info, vertices, tex_coords
 
     def set_retiled(self, preferred_stride, tile_box, tiles_info, vertices, tex_coords):
         self._set_texture_tiles(tiles_info)
         self._set_vertex_tiles(vertices, tex_coords)
@@ -424,28 +443,29 @@
         # don't update here, the caller will do that
         # Store the most recent level of detail that we've done
         self._stride = preferred_stride
         self._latest_tile_box = tile_box
 
 
 class TiledGeolocatedImageVisual(SIFTTiledGeolocatedMixin, ImageVisual):
-    def __init__(self, data, origin_x, origin_y, cell_width, cell_height,
-                 **image_kwargs):
+    def __init__(self, data, origin_x, origin_y, cell_width, cell_height, **image_kwargs):
         super().__init__(data, origin_x, origin_y, cell_width, cell_height, **image_kwargs)
 
     def _init_texture(self, data, texture_format):
-        if self._interpolation == 'bilinear':
-            texture_interpolation = 'linear'
+        if self._interpolation == "bilinear":
+            texture_interpolation = "linear"
         else:
-            texture_interpolation = 'nearest'
+            texture_interpolation = "nearest"
 
-        tex = TextureAtlas2D(self.texture_shape, tile_shape=self.tile_shape,
-                             interpolation=texture_interpolation,
-                             format="LUMINANCE", internalformat="R32F",
-                             )
+        tex = TextureAtlas2D(
+            self.texture_shape,
+            tile_shape=self.tile_shape,
+            interpolation=texture_interpolation,
+            internalformat="auto",
+        )
         return tex
 
     def set_data(self, image):
         """Set the data
 
         Parameters
         ----------
@@ -490,15 +510,15 @@
         // http://stackoverflow.com/questions/11810158/how-to-deal-with-nan-or-inf-in-opengl-es-2-0-shaders
         if (
             !(color.r <= 0.0 || 0.0 <= color.r) &&
             !(color.g <= 0.0 || 0.0 <= color.g) &&
             !(color.b <= 0.0 || 0.0 <= color.b)) {
             color.a = 0;
         }
-        
+
         // if color is NaN, set to minimum possible value
         color.r = !(color.r <= 0.0 || 0.0 <= color.r) ? min($clim_r.x, $clim_r.y) : color.r;
         color.g = !(color.g <= 0.0 || 0.0 <= color.g) ? min($clim_g.x, $clim_g.y) : color.g;
         color.b = !(color.b <= 0.0 || 0.0 <= color.b) ? min($clim_b.x, $clim_b.y) : color.b;
         // clamp data to minimum and maximum of clims
         color.r = clamp(color.r, min($clim_r.x, $clim_r.y), max($clim_r.x, $clim_r.y));
         color.g = clamp(color.g, min($clim_g.x, $clim_g.y), max($clim_g.x, $clim_g.y));
@@ -516,39 +536,39 @@
         color.r = pow(color.r, $gamma_r);
         color.g = pow(color.g, $gamma_g);
         color.b = pow(color.b, $gamma_b);
         return color;
     }
 """
 
-_null_color_transform = 'vec4 pass(vec4 color) { return color; }'
+_null_color_transform = "vec4 pass(vec4 color) { return color; }"
 
 
 class SIFTMultiChannelTiledGeolocatedMixin(SIFTTiledGeolocatedMixin):
     def _normalize_data(self, data_arrays):
         if not isinstance(data_arrays, (list, tuple)):
             return super()._normalize_data(data_arrays)
 
         new_data = []
         for data in data_arrays:
             new_data.append(super()._normalize_data(data))
         return new_data
 
     def _init_geo_parameters(
-            self,
-            origin_x,
-            origin_y,
-            cell_width,
-            cell_height,
-            projection,
-            texture_shape,
-            tile_shape,
-            wrap_lon,
-            shape,
-            data_arrays
+        self,
+        origin_x,
+        origin_y,
+        cell_width,
+        cell_height,
+        projection,
+        texture_shape,
+        tile_shape,
+        wrap_lon,
+        shape,
+        data_arrays,
     ):
         if shape is None:
             shape = self._compute_shape(shape, data_arrays)
         ndim = len(shape) or [x for x in data_arrays if x is not None][0].ndim
         data = ArrayProxy(ndim, shape)
         super()._init_geo_parameters(
             origin_x,
@@ -560,49 +580,53 @@
             tile_shape,
             wrap_lon,
             shape,
             data,
         )
 
         self.set_channels(
-            data_arrays, shape=shape, cell_width=cell_width,
-            cell_height=cell_height, origin_x=origin_x, origin_y=origin_y,
+            data_arrays,
+            shape=shape,
+            cell_width=cell_width,
+            cell_height=cell_height,
+            origin_x=origin_x,
+            origin_y=origin_y,
         )
 
-    def set_channels(self, data_arrays, shape=None,
-                     cell_width=None, cell_height=None,
-                     origin_x=None, origin_y=None):
-        assert (shape or data_arrays is not None), "`data` or `shape` must be provided"
+    def set_channels(self, data_arrays, shape=None, cell_width=None, cell_height=None, origin_x=None, origin_y=None):
+        assert shape or data_arrays is not None, "`data` or `shape` must be provided"  # nosec B101
         if cell_width is not None:
             self.cell_width = cell_width
         if cell_height:
             self.cell_height = cell_height  # Note: cell_height is usually negative
         if origin_x:
             self.origin_x = origin_x
         if origin_y:
             self.origin_y = origin_y
         self.shape = self._compute_shape(shape, data_arrays)
-        assert None not in (self.cell_width, self.cell_height, self.origin_x, self.origin_y, self.shape)
+        assert None not in (self.cell_width, self.cell_height, self.origin_x, self.origin_y, self.shape)  # nosec B101
         # how many of the higher resolution channel tiles (smaller geographic area) make
         # up a low resolution channel tile
         self._channel_factors = tuple(
-            self.shape[0] / float(chn.shape[0]) if chn is not None else 1. for chn in data_arrays)
+            self.shape[0] / float(chn.shape[0]) if chn is not None else 1.0 for chn in data_arrays
+        )
         self._lowest_factor = max(self._channel_factors)
-        self._lowest_rez = Resolution(abs(self.cell_height * self._lowest_factor),
-                                      abs(self.cell_width * self._lowest_factor))
+        self._lowest_rez = Resolution(
+            abs(self.cell_height * self._lowest_factor), abs(self.cell_width * self._lowest_factor)
+        )
 
         # Where does this image lie in this lonely world
         self.calc = TileCalculator(
             self.name,
             self.shape,
             Point(x=self.origin_x, y=self.origin_y),
             Resolution(dy=abs(self.cell_height), dx=abs(self.cell_width)),
             self.tile_shape,
             self.texture_shape,
-            wrap_lon=self.wrap_lon
+            wrap_lon=self.wrap_lon,
         )
 
         # Reset texture state, if we change things to know which texture
         # don't need to be updated then this can be removed/changed
         self.texture_state.reset()
         self._need_texture_upload = True
         self._need_vertex_update = True
@@ -670,57 +694,54 @@
     **kwargs : dict
         Keyword arguments to pass to :class:`~vispy.visuals.ImageVisual`. Note
         that this Visual does not allow for ``texture_format`` to be specified
         and is hardcoded to ``r32f`` internal texture format.
 
     """
 
-    def __init__(self, data_arrays, clim='auto', gamma=1.0, **kwargs):
+    def __init__(self, data_arrays, clim="auto", gamma=1.0, **kwargs):
         if kwargs.get("texture_format") is not None:
-            raise ValueError("'texture_format' can't be specified with the "
-                             "'MultiChannelImageVisual'.")
+            raise ValueError("'texture_format' can't be specified with the " "'MultiChannelImageVisual'.")
         kwargs["texture_format"] = "R32F"
         if kwargs.get("cmap") is not None:
-            raise ValueError("'cmap' can't be specified with the"
-                             "'MultiChannelImageVisual'.")
+            raise ValueError("'cmap' can't be specified with the" "'MultiChannelImageVisual'.")
         kwargs["cmap"] = None
         self.num_channels = len(data_arrays)
         super().__init__(data_arrays, clim=clim, gamma=gamma, **kwargs)
 
     def _init_texture(self, data_arrays, texture_format):
-        if self._interpolation == 'bilinear':
-            texture_interpolation = 'linear'
+        if self._interpolation == "bilinear":
+            texture_interpolation = "linear"
         else:
-            texture_interpolation = 'nearest'
+            texture_interpolation = "nearest"
 
         tex = MultiChannelGPUScaledTexture2D(
             data_arrays,
             internalformat=texture_format,
             format="LUMINANCE",
             interpolation=texture_interpolation,
         )
         return tex
 
     def _get_shapes(self, data_arrays):
         shapes = [x.shape for x in data_arrays if x is not None]
         if not shapes:
-            raise ValueError("List of data arrays must contain at least one "
-                             "numpy array.")
+            raise ValueError("List of data arrays must contain at least one " "numpy array.")
         return shapes
 
     def _get_min_shape(self, data_arrays):
         return min(self._get_shapes(data_arrays))
 
     def _get_max_shape(self, data_arrays):
         return max(self._get_shapes(data_arrays))
 
     @property
     def size(self):
         """Get size of the image (width, height)."""
-        return self._get_max_shape(self._data)
+        return self._get_max_shape(self._data)[:2][::-1]
 
     @property
     def clim(self):
         """Get color limits used when rendering the image (cmin, cmax)."""
         return self._texture.clim
 
     @clim.setter
@@ -737,19 +758,19 @@
             # we are going to rebuild anyway so just do it later
             return
         try:
             norm_clims = self._texture.clim_normalized
         except RuntimeError:
             return
         else:
-            clim_names = ('clim_r', 'clim_g', 'clim_b')
+            clim_names = ("clim_r", "clim_g", "clim_b")
             # shortcut so we don't have to rebuild the whole color transform
             for clim_name, clim in zip(clim_names, norm_clims):
                 # shortcut so we don't have to rebuild the whole color transform
-                self.shared_program.frag['color_transform'][1][clim_name] = clim
+                self.shared_program.frag["color_transform"][1][clim_name] = clim
 
     @property
     def gamma(self):
         """Get the gamma used when rendering the image."""
         return self._gamma
 
     @gamma.setter
@@ -757,58 +778,58 @@
         """Set gamma used when rendering the image."""
         if not isinstance(value, (list, tuple)):
             value = [value] * self.num_channels
         if any(val <= 0 for val in value):
             raise ValueError("gamma must be > 0")
         self._gamma = tuple(float(x) for x in value)
 
-        gamma_names = ('gamma_r', 'gamma_g', 'gamma_b')
+        gamma_names = ("gamma_r", "gamma_g", "gamma_b")
         for gamma_name, gam in zip(gamma_names, self._gamma):
             # shortcut so we don't have to rebuild the color transform
             if not self._need_colortransform_update:
-                self.shared_program.frag['color_transform'][2][gamma_name] = gam
+                self.shared_program.frag["color_transform"][2][gamma_name] = gam
         self.update()
 
     @ImageVisual.cmap.setter
     def cmap(self, cmap):
         if cmap is not None:
             raise ValueError("MultiChannelImageVisual does not support a colormap.")
         self._cmap = _NoColormap()
 
     def _build_interpolation(self):
         # assumes 'nearest' interpolation
         interpolation = self._interpolation
-        if interpolation != 'nearest':
+        if interpolation != "nearest":
             raise NotImplementedError("MultiChannelImageVisual only supports 'nearest' interpolation.")
-        texture_interpolation = 'nearest'
+        texture_interpolation = "nearest"
 
         self._data_lookup_fn = Function(_rgb_texture_lookup)
-        self.shared_program.frag['get_data'] = self._data_lookup_fn
+        self.shared_program.frag["get_data"] = self._data_lookup_fn
         if self._texture.interpolation != texture_interpolation:
             self._texture.interpolation = texture_interpolation
-        self._data_lookup_fn['texture_r'] = self._texture.textures[0]
-        self._data_lookup_fn['texture_g'] = self._texture.textures[1]
-        self._data_lookup_fn['texture_b'] = self._texture.textures[2]
+        self._data_lookup_fn["texture_r"] = self._texture.textures[0]
+        self._data_lookup_fn["texture_g"] = self._texture.textures[1]
+        self._data_lookup_fn["texture_b"] = self._texture.textures[2]
 
         self._need_interpolation_update = False
 
     def _build_color_transform(self):
         if self.num_channels != 3:
-            raise NotImplementedError("MultiChannelimageVisuals only support 3 channels.")
+            raise NotImplementedError("MultiChannelImageVisuals only support 3 channels.")
         else:
             # RGB/A image data (no colormap)
             fclim = Function(_apply_clim)
             fgamma = Function(_apply_gamma)
             fun = FunctionChain(None, [Function(_null_color_transform), fclim, fgamma])
-        fclim['clim_r'] = self._texture.textures[0].clim_normalized
-        fclim['clim_g'] = self._texture.textures[1].clim_normalized
-        fclim['clim_b'] = self._texture.textures[2].clim_normalized
-        fgamma['gamma_r'] = self.gamma[0]
-        fgamma['gamma_g'] = self.gamma[1]
-        fgamma['gamma_b'] = self.gamma[2]
+        fclim["clim_r"] = self._texture.textures[0].clim_normalized
+        fclim["clim_g"] = self._texture.textures[1].clim_normalized
+        fclim["clim_b"] = self._texture.textures[2].clim_normalized
+        fgamma["gamma_r"] = self.gamma[0]
+        fgamma["gamma_g"] = self.gamma[1]
+        fgamma["gamma_b"] = self.gamma[2]
         return fun
 
     def set_data(self, data_arrays):
         """Set the data
 
         Parameters
         ----------
@@ -820,17 +841,29 @@
         data_arrays = list(self._cast_arrays_if_needed(data_arrays))
         self._texture.check_data_format(data_arrays)
         self._data = data_arrays
         self._need_texture_upload = True
 
     @staticmethod
     def _cast_arrays_if_needed(data_arrays):
+        # FIXME: Remove the try/except and move the remaining import to the imports section of this file as soon as
+        #  support for VisPy < 0.11.0 is dropped.
         for data in data_arrays:
-            if data is not None and should_cast_to_f32(data.dtype):
-                data = data.astype(np.float32)
+            try:
+                # Since VisPy v0.12.0
+                from vispy.gloo.texture import downcast_to_32bit_if_needed
+
+                if data is not None:
+                    data = downcast_to_32bit_if_needed(data)
+            except ImportError:
+                # Before VisPy v0.12.0
+                from vispy.gloo.texture import should_cast_to_f32
+
+                if data is not None and should_cast_to_f32(data.dtype):
+                    data = data.astype(np.float32)
             yield data
 
     @staticmethod
     def _shape_differs(arr1, arr2):
         none_change1 = arr1 is not None and arr2 is None
         none_change2 = arr1 is None and arr2 is not None
         shape_change = False
@@ -854,32 +887,35 @@
             self._need_colortransform_update = True
         self._need_texture_upload = False
 
 
 MultiChannelImage = create_visual_node(MultiChannelImageVisual)
 
 
-class RGBCompositeLayerVisual(SIFTMultiChannelTiledGeolocatedMixin,
-                              TiledGeolocatedImageVisual,
-                              MultiChannelImageVisual):
+class RGBCompositeImageVisual(
+    SIFTMultiChannelTiledGeolocatedMixin, TiledGeolocatedImageVisual, MultiChannelImageVisual
+):
     def _init_texture(self, data_arrays, texture_format):
-        if self._interpolation == 'bilinear':
-            texture_interpolation = 'linear'
+        if self._interpolation == "bilinear":
+            texture_interpolation = "linear"
         else:
-            texture_interpolation = 'nearest'
+            texture_interpolation = "nearest"
 
         tex_shapes = [self.texture_shape] * len(data_arrays)
         tex = MultiChannelTextureAtlas2D(
-            tex_shapes, tile_shape=self.tile_shape,
-            interpolation=texture_interpolation, format="LUMINANCE", internalformat="R32F"
+            tex_shapes,
+            tile_shape=self.tile_shape,
+            interpolation=texture_interpolation,
+            format="LUMINANCE",
+            internalformat="R32F",
         )
         return tex
 
 
-RGBCompositeLayer = create_visual_node(RGBCompositeLayerVisual)
+RGBCompositeImage = create_visual_node(RGBCompositeImageVisual)
 
 
 class ShapefileLinesVisual(LineVisual):
     def __init__(self, filepath, double=False, **kwargs):
         LOG.debug("Using border shapefile '%s'", filepath)
         self.sf = shapefile.Reader(filepath)
 
@@ -894,16 +930,16 @@
         prev_idx = 0
         for one_shape in self.sf.iterShapes():
             # end_idx = prev_idx + len(one_shape.points) * 2 - len(one_shape.parts) * 2
             # vertex_buffer[prev_idx:end_idx:2] = one_shape.points[:-1]
             # for part_idx in one_shape.parts:
             for part_start, part_end in zip(one_shape.parts, list(one_shape.parts[1:]) + [len(one_shape.points)]):
                 end_idx = prev_idx + (part_end - part_start) * 2 - 2
-                vertex_buffer[prev_idx:end_idx:2] = one_shape.points[part_start:part_end - 1]
-                vertex_buffer[prev_idx + 1:end_idx:2] = one_shape.points[part_start + 1:part_end]
+                vertex_buffer[prev_idx:end_idx:2] = one_shape.points[part_start : part_end - 1]
+                vertex_buffer[prev_idx + 1 : end_idx : 2] = one_shape.points[part_start + 1 : part_end]
                 prev_idx = end_idx
 
         # Clip lats to +/- 89.9 otherwise PROJ.4 on mercator projection will fail
         np.clip(vertex_buffer[:, 1], -89.9, 89.9, out=vertex_buffer[:, 1])
         # vertex_buffer[:, 0], vertex_buffer[:, 1] = self.proj(vertex_buffer[:, 0], vertex_buffer[:, 1])
         if double:
             LOG.debug("Adding 180 to 540 double of shapefile")
@@ -918,49 +954,50 @@
         LOG.info("Done loading boundaries: %s", datetime.utcnow().isoformat(" "))
 
 
 ShapefileLines = create_visual_node(ShapefileLinesVisual)
 
 
 class NEShapefileLinesVisual(ShapefileLinesVisual):
-    """Layer class for handling shapefiles from Natural Earth.
+    """Visual class for handling shapefiles from Natural Earth.
 
     http://www.naturalearthdata.com/
 
     There should be no difference in the format of the file, but some
     assumptions can be made with data from Natural Earth about filenaming,
     data resolution, fields and other record information that is normally
     included in most Natural Earth files.
     """
+
     pass
 
 
 NEShapefileLines = create_visual_node(NEShapefileLinesVisual)
 
 
 class PrecomputedIsocurveVisual(IsocurveVisual):
     """IsocurveVisual that can use precomputed paths."""
 
     def __init__(self, verts, connects, level_indexes, levels, **kwargs):
         num_zoom_levels = len(levels)
         num_levels_per_zlevel = [len(x) for x in levels]
         self._zoom_level_indexes = [
-            level_indexes[:sum(num_levels_per_zlevel[:z_level + 1])]
-            for z_level in range(num_zoom_levels)]
+            level_indexes[: sum(num_levels_per_zlevel[: z_level + 1])] for z_level in range(num_zoom_levels)
+        ]
         self._zoom_level_size = [sum(z_level_indexes) for z_level_indexes in self._zoom_level_indexes]
 
         self._all_verts = []
         self._all_connects = []
         self._all_levels = []
         self._zoom_level = -1
         for zoom_level in range(num_zoom_levels):
             end_idx = self._zoom_level_size[zoom_level]
             self._all_verts.append(verts[:end_idx])
             self._all_connects.append(connects[:end_idx])
-            self._all_levels.append([x for y in levels[:zoom_level + 1] for x in y])
+            self._all_levels.append([x for y in levels[: zoom_level + 1] for x in y])
 
         super(PrecomputedIsocurveVisual, self).__init__(data=None, levels=levels, **kwargs)
 
         self._data = True
         self._level_min = 0
         self.zoom_level = kwargs.pop("zoom_level", 0)
 
@@ -981,20 +1018,386 @@
 
     @property
     def clim(self):
         return self._clim
 
     @clim.setter
     def clim(self, val):
-        assert len(val) == 2
+        assert len(val) == 2  # nosec B101
         self._clim = val
         self._need_level_update = True
         self._need_color_update = True
         self.update()
 
     def _compute_iso_line(self):
-        """ compute LineVisual vertices, connects and color-index
-        """
+        """compute LineVisual vertices, connects and color-index"""
         return
 
 
 PrecomputedIsocurve = create_visual_node(PrecomputedIsocurveVisual)
+
+
+class _GLGradientLineVisual(_GLLineVisual):
+    def __init__(self, arrow_size, *args, **kwargs):
+        self._arrow_size = arrow_size
+        super().__init__(*args, **kwargs)
+
+    def _prepare_draw(self, view):  # noqa: C901
+        prof = Profiler()
+
+        if self._parent._changed["pos"]:
+            if self._parent._pos is None:
+                return False
+            # todo: does this result in unnecessary copies?
+            pos = np.ascontiguousarray(self._parent._pos.astype(np.float32))
+            xf = view.transforms.get_transform("visual", "framebuffer")
+            # transform pos to pixel coords
+            pos_px = xf.map(pos)
+            # subtract necessary offset
+            line_dirs = pos_px[1::2, 0:2] - pos_px[0::2, 0:2]
+            line_dirs_normed = (1.0 / (np.linalg.norm(line_dirs, axis=1))) * line_dirs
+            offset = self._arrow_size / 2.0
+            pos_px[1::2, 0:2] -= offset * line_dirs_normed
+            # tranform back to visual coords
+            pos_re = xf.imap(pos_px)[:, 0:2].astype(np.float32)
+            self._pos_vbo.set_data(pos_re)
+            # self._pos_vbo.set_data(pos)
+            self._program.vert["position"] = self._pos_vbo
+            self._program.vert["to_vec4"] = self._ensure_vec4_func(pos.shape[-1])
+            self._parent._changed["pos"] = False
+
+        if self._parent._changed["color"]:
+            color, cmap = self._parent._interpret_color()
+            # If color is not visible, just quit now
+            if isinstance(color, Color) and color.is_blank:
+                return False
+            if isinstance(color, Function):
+                # TODO: Change to the parametric coordinate once that is done
+                self._program.vert["color"] = color("(gl_Position.x + 1.0) / 2.0")
+            else:
+                if color.ndim == 1:
+                    self._program.vert["color"] = color
+                else:
+                    self._color_vbo.set_data(color)
+                    self._program.vert["color"] = self._color_vbo
+            self._parent._changed["color"] = False
+
+            self.shared_program["texture2D_LUT"] = cmap and cmap.texture_lut()
+
+        # Do we want to use OpenGL, and can we?
+        self.update_gl_state(line_smooth=bool(self._parent._antialias))
+        px_scale = self.transforms.pixel_scale
+        width = px_scale * self._parent._width
+        self.update_gl_state(line_width=max(width, 1.0))
+
+        if self._parent._changed["connect"]:
+            self._connect = self._parent._interpret_connect()
+            if isinstance(self._connect, np.ndarray):
+                self._connect_ibo.set_data(self._connect)
+            self._parent._changed["connect"] = False
+        if self._connect is None:
+            return False
+
+        prof("prepare")
+
+        # Draw
+        if isinstance(self._connect, str) and self._connect == "strip":
+            self._draw_mode = "line_strip"
+            self._index_buffer = None
+        elif isinstance(self._connect, str) and self._connect == "segments":
+            self._draw_mode = "lines"
+            self._index_buffer = None
+        elif isinstance(self._connect, np.ndarray):
+            self._draw_mode = "lines"
+            self._index_buffer = self._connect_ibo
+        else:
+            raise ValueError("Invalid line connect mode: %r" % self._connect)
+
+        prof("draw")
+
+
+class GradientLineVisual(LineVisual):
+    """Gradient line visual
+
+    Parameters
+    ----------
+    pos : array
+        Array of shape (..., 2) or (..., 3) specifying vertex coordinates.
+    color : Color, tuple, or array
+        The color to use when drawing the line. If an array is given, it
+        must be of shape (..., 4) and provide one rgba color per vertex.
+        Can also be a colormap name, or appropriate `Function`.
+    width:
+        The width of the line in px. Line widths > 1px are only
+        guaranteed to work when using 'agg' method.
+    connect : str or array
+        Determines which vertices are connected by lines.
+
+            * "strip" causes the line to be drawn with each vertex
+              connected to the next.
+            * "segments" causes each pair of vertices to draw an
+              independent line segment
+            * numpy arrays specify the exact set of segment pairs to
+              connect.
+
+    method : str
+        Mode to use for drawing.
+
+            * "agg" uses anti-grain geometry to draw nicely antialiased lines
+              with proper joins and endcaps.
+            * "gl" uses OpenGL's built-in line rendering. This is much faster,
+              but produces much lower-quality results and is not guaranteed to
+              obey the requested line width or join/endcap styles.
+
+    antialias : bool
+        Enables or disables antialiasing.
+        For method='gl', this specifies whether to use GL's line smoothing,
+        which may be unavailable or inconsistent on some platforms.
+    """
+
+    def __init__(
+        self,
+        pos=None,
+        color=(0.5, 0.5, 0.5, 1),
+        width=1,
+        arrow_size=None,
+        connect="strip",
+        method="gl",
+        antialias=False,
+    ):
+        self._line_visual = None
+
+        self._changed = {"pos": False, "color": False, "width": False, "connect": False}
+
+        self._pos = None
+        self._color = None
+        self._width = None
+        self._arrow_size = arrow_size
+        self._connect = None
+        self._bounds = None
+        self._antialias = None
+        self._method = "none"
+
+        super(LineVisual, self).__init__([])
+
+        # don't call subclass set_data; these often have different
+        # signatures.
+        LineVisual.set_data(self, pos=pos, color=color, width=width, connect=connect)
+        self.antialias = antialias
+        self.method = method
+
+    def method(self, method):
+        if method not in ("agg", "gl"):
+            raise ValueError('method argument must be "agg" or "gl".')
+        if method == self._method:
+            return
+
+        self._method = method
+        if self._line_visual is not None:
+            self.remove_subvisual(self._line_visual)
+
+        if method == "gl":
+            self._line_visual = _GLGradientLineVisual(self, self._arrow_size)
+        elif method == "agg":
+            self._line_visual = _AggLineVisual(self)
+        self.add_subvisual(self._line_visual)
+
+        for k in self._changed:
+            self._changed[k] = True
+
+
+class _TipAlignedArrowHeadVisual(_ArrowHeadVisual):
+    def __index__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+    def _prepare_draw(self, view):
+        if self._parent._arrows_changed:
+            self._prepare_vertex_data(view)
+        self.shared_program.bind(self._arrow_vbo)
+        self.shared_program["antialias"] = 0.0
+        self.shared_program.frag["arrow_type"] = self._parent.arrow_type
+        self.shared_program.frag["fill_type"] = "filled"
+
+    def _prepare_vertex_data(self, view):
+        arrows = self._parent.arrows
+
+        xf = view.transforms.get_transform("visual", "framebuffer")
+
+        if arrows is None or arrows.size == 0:
+            self._arrow_vbo = VertexBuffer(np.array([], dtype=self._arrow_vtype))
+            return
+
+        # arrows present in (N/2 x 4), need (N x 2) where N is number of
+        # vertices
+        arrows = arrows.reshape(-1, 2)
+        # transform arrow positions to pixel coords
+        arrows_px = xf.map(arrows)
+        # subtract necessary offset
+        arrow_dirs = arrows_px[1::2, 0:2] - arrows_px[0::2, 0:2]
+        arrow_dirs_normed = (1.0 / (np.linalg.norm(arrow_dirs, axis=1)))[:, np.newaxis] * arrow_dirs
+        offset = self._parent.arrow_size / 2.0
+        arrows_px[1::2, 0:2] -= offset * arrow_dirs_normed
+        # tranform back to visual coords
+        arrows_re = xf.imap(arrows_px)[:, 0:2].astype(np.float32)
+        # arrows now again needed in (N/2 x 4)
+        arrows = arrows_re.reshape((-1, 4))
+        v = np.zeros(len(arrows), dtype=self._arrow_vtype)
+        # 2d // 3d v1 v2.
+        sh = int(arrows.shape[1] / 2)
+        v["v1"] = as_vec4(arrows[:, 0:sh])
+        v["v2"] = as_vec4(arrows[:, sh : int(2 * sh)])
+        v["size"][:] = self._parent.arrow_size
+        color, cmap = self._parent._interpret_color(self._parent.arrow_color)
+        v["color"][:] = color
+        v["linewidth"][:] = self._parent.width
+        self._arrow_vbo = VertexBuffer(v)
+
+
+class TipAlignedArrowVisual(ArrowVisual):
+    """
+        Almost exactly the same as vispy's ArrowVisual except
+        for the arrow's head not being centered at the end of the arrow's
+        line but the arrows tip pointing to the coordinate of the arrow's line.
+
+    Parameters
+    ----------
+    pos : array
+        Array of shape (..., 2) or (..., 3) specifying vertex coordinates.
+    color : Color, tuple, or array
+        The color to use when drawing the line. If an array is given, it
+        must be of shape (..., 4) and provide one rgba color per vertex.
+        Can also be a colormap name, or appropriate `Function`.
+    width:
+        The width of the line in px. Line widths > 1px are only
+        guaranteed to work when using 'agg' method.
+    connect : str or array
+        Determines which vertices are connected by lines.
+
+            * "strip" causes the line to be drawn with each vertex
+              connected to the next.
+            * "segments" causes each pair of vertices to draw an
+              independent line segment
+            * numpy arrays specify the exact set of segment pairs to
+              connect.
+    method : str
+        Mode to use for drawing.
+
+            * "agg" uses anti-grain geometry to draw nicely antialiased lines
+              with proper joins and endcaps.
+            * "gl" uses OpenGL's built-in line rendering. This is much faster,
+              but produces much lower-quality results and is not guaranteed to
+              obey the requested line width or join/endcap styles.
+    antialias : bool
+        Enables or disables antialiasing.
+        For method='gl', this specifies whether to use GL's line smoothing,
+        which may be unavailable or inconsistent on some platforms.
+    arrows : array
+        A (N, 4) or (N, 6) matrix where each row contains the (x, y) or the
+        (x, y, z) coordinate of the first and second vertex of the arrow
+        body. Remember that the second vertex is used as center point for
+        the arrow head, and the first vertex is only used for determining
+        the arrow head orientation.
+    arrow_type : string
+        Specify the arrow head type, the currently available arrow head types
+        are:
+
+            * stealth
+            * curved
+            * triangle_30
+            * triangle_60
+            * triangle_90
+            * angle_30
+            * angle_60
+            * angle_90
+            * inhibitor_round
+    arrow_size : float
+        Specify the arrow size
+    arrow_color : Color, tuple, or array
+        The arrow head color. If an array is given, it must be of shape
+        (..., 4) and provide one rgba color per arrow head. Can also be a
+        colormap name, or appropriate `Function`.
+    """
+
+    def __init__(
+        self,
+        pos=None,
+        color=(0.5, 0.5, 0.5, 1),
+        width=1,
+        connect="strip",
+        method="gl",
+        antialias=False,
+        arrows=None,
+        arrow_type="stealth",
+        arrow_size=None,
+        arrow_color=(0.5, 0.5, 0.5, 1),
+    ):
+        # Do not use the self._changed dictionary as it gets overwritten by
+        # the LineVisual constructor.
+        self._arrows_changed = False
+
+        self._arrow_type = None
+        self._arrow_size = None
+        self._arrows = None
+
+        self.arrow_type = arrow_type
+        self.arrow_size = arrow_size
+        self.arrow_color = arrow_color
+
+        self.arrow_head = _TipAlignedArrowHeadVisual(self)
+
+        # TODO: `LineVisual.__init__` also calls its own `set_data` method,
+        # which triggers an *update* event. This results in a redraw. After
+        # that we call our own `set_data` method, which triggers another
+        # redraw. This should be fixed.
+        GradientLineVisual.__init__(self, pos, color, width, arrow_size, connect, method, antialias)
+        TipAlignedArrowVisual.set_data(self, arrows=arrows)
+
+        # Add marker visual for the arrow head
+        self.add_subvisual(self.arrow_head)
+
+
+class LinesVisual(TipAlignedArrowVisual):
+    default_colors = {
+        "red": (1.0, 0.0, 0.0, 1.0),
+        "green": (0.0, 1.0, 0.0, 1.0),
+    }
+
+    def __init__(self, arrows: np.ndarray, colors: Optional[np.ndarray] = None):
+        # if colors not set, use green-red gradient
+        # if only one color given, use only one color
+        # otherwise two colors need to be provided by caller, one for each point
+        # in point pairs
+
+        n_points, n_coordinates = arrows.shape
+        if n_coordinates != 4:
+            raise AttributeError("Expected 4 coordinates per arrow.")
+
+        points = arrows.reshape(-1, 2)
+
+        # Remember that the second vertex is used as center point for the
+        # arrow head, and the first vertex is only used for determining
+        # the arrow head orientation.
+        if colors is None:
+            colors = np.array([np.array(self.default_colors["green"]), np.array(self.default_colors["red"])])
+            colors = np.tile(colors, (n_points, 1))
+        elif colors.ndim == 1 and colors.size == 4:
+            pass
+        elif colors.ndim == 2 and colors.shape[0] == 2:
+            colors = np.tile(colors, (n_points, 1))
+        else:
+            raise AttributeError(
+                "Too many colors provided or colors ill-formed, " "provide at-most two colors in RGBA format."
+            )
+
+        super().__init__(
+            pos=points,
+            arrows=arrows,
+            arrow_type="triangle_30",
+            color=colors,
+            arrow_color="w",
+            arrow_size=10,
+            connect="segments",
+            method="gl",
+        )
+
+
+Lines = create_visual_node(LinesVisual)
```

### Comparing `uwsift-1.2.3/uwsift/workspace/collector.py` & `uwsift-2.0.0b0/uwsift/workspace/collector.py`

 * *Files 10% similar despite different names*

```diff
@@ -20,31 +20,32 @@
 :license: GPLv3, see LICENSE for more details
 """
 import logging
 import os
 import sys
 import unittest
 from datetime import datetime
-from typing import List, Iterable, Mapping, Union
+from typing import Iterable, List, Mapping, Optional, Union
 
 from PyQt5.QtCore import QObject
+from satpy.readers import group_files
 
 from uwsift import config
-from satpy.readers import group_files
 from uwsift.queue import TASK_DOING, TASK_PROGRESS
-from .workspace import Workspace
-from .importer import available_satpy_readers
+
 from ..common import Info
+from .importer import available_satpy_readers
+from .workspace import BaseWorkspace
 
 LOG = logging.getLogger(__name__)
 
 
 class _workspace_test_proxy(object):
     def __init__(self):
-        self.cwd = '/tmp' if os.path.isdir("/tmp") else os.getcwd()
+        self.cwd = "/tmp" if os.path.isdir("/tmp") else os.getcwd()  # nosec B108
 
     def collect_product_metadata_for_paths(self, paths):
         LOG.debug("import metadata for files: {}".format(repr(paths)))
         for path in paths:
             yield 1, {Info.PATHNAME: path}
 
     class _emitsy(object):
@@ -56,21 +57,28 @@
 
 class ResourceSearchPathCollector(QObject):
     """Given a set of search paths,
     awaken for new files available within the directories,
     update the metadatabase for new resources,
     and mark for purge any files no longer available.
     """
-    _ws: Workspace = None
-    _paths: List[str] = None
-    _dir_mtimes: Mapping[str, datetime] = None
-    _timestamp_path: str = None  # path which tracks the last time we skimmed the paths
-    _is_posix: bool = None
-    _scheduled_dirs: List[str] = None
-    _scheduled_files: List[str] = None
+
+    def __init__(self, ws: Union[BaseWorkspace, _workspace_test_proxy]):
+        super(ResourceSearchPathCollector, self).__init__()
+        self._ws = ws
+        self._paths: List[str] = []
+        self._dir_mtimes: Mapping[str, datetime] = {}
+        self._scheduled_dirs: List[str] = []
+        self._scheduled_files: List[str] = []
+        # path which tracks the last time we skimmed the paths
+        self._timestamp_path = os.path.join(ws.cwd, ".last_collection_check")
+        self._is_posix = sys.platform in {"linux", "darwin"}
+        self.satpy_readers = config.get("data_reading.readers")
+        if not self.satpy_readers:
+            self.satpy_readers = available_satpy_readers()
 
     @property
     def paths(self):
         return list(self._paths)
 
     @paths.setter
     def paths(self, new_paths):
@@ -79,166 +87,167 @@
         removed = ol - nu
         added = nu - ol
         self._scheduled_dirs = []
         self._scheduled_files = []
         self._paths = list(new_paths)
         self._flush_dirs(removed)
         self._schedule_walk_dirs(added)
-        LOG.debug('old search directories removed: {}'.format(':'.join(sorted(removed))))
-        LOG.debug('new search directories added: {}'.format(':'.join(sorted(added))))
+        LOG.debug("old search directories removed: {}".format(":".join(sorted(removed))))
+        LOG.debug("new search directories added: {}".format(":".join(sorted(added))))
 
     def _flush_dirs(self, dirs: Iterable[str]):
         pass
 
     def _schedule_walk_dirs(self, dirs: Iterable[str]):
         self._scheduled_dirs += list(dirs)
 
     @property
     def has_pending_files(self):
         return len(self._scheduled_files)
 
     def __bool__(self):
         return len(self._paths) > 0
 
-    def _skim(self, last_checked: int = 0, dirs: Iterable[str] = None):
-        """skim directories for new mtimes
-        """
+    def _skim(self, last_checked: int = 0, dirs: Optional[Iterable[str]] = None):
+        """skim directories for new mtimes"""
         skipped_dirs = 0
-        for rawpath in (dirs or self._paths):
+        for rawpath in dirs or self._paths:
             path = os.path.abspath(rawpath)
             if not os.path.isdir(path):
                 LOG.warning("{} is not a directory".format(path))
                 continue
             for dirpath, _, filenames in os.walk(path):
                 if self._is_posix and (os.stat(dirpath).st_mtime < last_checked):
                     skipped_dirs += 1
                     continue
                 for filename in filenames:
-                    if filename.startswith('.'):
+                    if filename.startswith("."):
                         continue  # dammit Apple, ._*.nc files ...
                     filepath = os.path.join(dirpath, filename)
                     if os.path.isfile(filepath) and (os.stat(filepath).st_mtime >= last_checked):
                         yield filepath
         LOG.debug("skipped files in {} dirs due to POSIX directory mtime".format(skipped_dirs))
 
     def _touch(self):
         mtime = 0
         if os.path.isfile(self._timestamp_path):
             mtime = os.stat(self._timestamp_path).st_mtime
         else:
-            with open(self._timestamp_path, 'wb') as fp:
+            with open(self._timestamp_path, "wb") as fp:
                 fp.close()
         os.utime(self._timestamp_path)
         return mtime
 
-    def __init__(self, ws: Union[Workspace, _workspace_test_proxy]):
-        super(ResourceSearchPathCollector, self).__init__()
-        self._ws = ws
-        self._paths = []
-        self._dir_mtimes = {}
-        self._scheduled_files = []
-        self._timestamp_path = os.path.join(ws.cwd, '.last_collection_check')
-        self._is_posix = sys.platform in {'linux', 'darwin'}
-        self.satpy_readers = config.get('data_reading.readers')
-        if not self.satpy_readers:
-            self.satpy_readers = available_satpy_readers()
-
     def look_for_new_files(self):
         if len(self._scheduled_dirs):
             new_dirs, self._scheduled_dirs = self._scheduled_dirs, []
-            LOG.debug('giving special attention to new search paths {}'.format(':'.join(new_dirs)))
+            LOG.debug("giving special attention to new search paths {}".format(":".join(new_dirs)))
             new_files = list(self._skim(0, new_dirs))
-            LOG.debug('found {} files in new search paths'.format(len(new_files)))
+            LOG.debug("found {} files in new search paths".format(len(new_files)))
             self._scheduled_files += new_files
         when = self._touch()
         new_files = list(self._skim(when))
         if new_files:
-            LOG.info('found {} additional files to skim metadata for, for a total of {}'.format(len(new_files), len(
-                self._scheduled_files)))
+            LOG.info(
+                "found {} additional files to skim metadata for, for a total of {}".format(
+                    len(new_files), len(self._scheduled_files)
+                )
+            )
             self._scheduled_files += new_files
 
     def bgnd_look_for_new_files(self):
-        LOG.debug("searching for files in search path {}".format(':'.join(self._paths)))
-        yield {TASK_DOING: 'skimming', TASK_PROGRESS: 0.5}
+        LOG.debug("searching for files in search path {}".format(":".join(self._paths)))
+        yield {TASK_DOING: "skimming", TASK_PROGRESS: 0.5}
         self.look_for_new_files()
-        yield {TASK_DOING: 'skimming', TASK_PROGRESS: 1.0}
+        yield {TASK_DOING: "skimming", TASK_PROGRESS: 1.0}
 
     def bgnd_merge_new_file_metadata_into_mdb(self):
         todo, self._scheduled_files = self._scheduled_files, []
         ntodo = len(todo)
-        LOG.debug('collecting metadata from {} potential new files'.format(ntodo))
-        yield {TASK_DOING: 'collecting metadata 0/{}'.format(ntodo), TASK_PROGRESS: 0.0}
+        LOG.debug("collecting metadata from {} potential new files".format(ntodo))
+        yield {TASK_DOING: "collecting metadata 0/{}".format(ntodo), TASK_PROGRESS: 0.0}
         changed_uuids = set()
-        readers_and_files = group_files(todo, reader='abi_l1b')
+        readers_and_files = group_files(todo, reader="abi_l1b")
         num_seen = 0
         for reader_and_files in readers_and_files:
             for reader_name, filenames in reader_and_files.items():
                 product_infos = self._ws.collect_product_metadata_for_paths(filenames, reader=reader_name)
                 for _, product_info in product_infos:
                     changed_uuids.add(product_info[Info.UUID])
                     num_seen += len(filenames)
-                    status = {TASK_DOING: 'collecting metadata {}/{}'.format(num_seen, ntodo),
-                              TASK_PROGRESS: float(num_seen) / ntodo + 1}
+                    status = {
+                        TASK_DOING: "collecting metadata {}/{}".format(num_seen, ntodo),
+                        TASK_PROGRESS: float(num_seen) / ntodo + 1,
+                    }
                     yield status
-        yield {TASK_DOING: 'collecting metadata done', TASK_PROGRESS: 1.0}
+        yield {TASK_DOING: "collecting metadata done", TASK_PROGRESS: 1.0}
         if changed_uuids:
-            LOG.debug('{} changed UUIDs, signaling product updates'.format(len(changed_uuids)))
+            LOG.debug("{} changed UUIDs, signaling product updates".format(len(changed_uuids)))
             # FUTURE: decide whether signals for metadatabase should belong to metadatabase
             self._ws.didUpdateProductsMetadata.emit(changed_uuids)
 
 
 def _debug(type, value, tb):
     """Enable with sys.excepthook = debug."""
     if not sys.stdin.isatty():
         sys.__excepthook__(type, value, tb)
     else:
-        import traceback
         import pdb  # noqa
+        import traceback
+
         traceback.print_exception(type, value, tb)
         # then start the debugger in post-mortem mode.
         pdb.post_mortem(tb)  # more modern
 
 
 def main():
     import argparse
-    parser = argparse.ArgumentParser(
-        description="PURPOSE",
-        epilog="",
-        fromfile_prefix_chars='@')
-    parser.add_argument('-v', '--verbose', dest='verbosity', action="count", default=0,
-                        help='each occurrence increases verbosity 1 level through ERROR-WARNING-Info-DEBUG')
-    parser.add_argument('-d', '--debug', dest='debug', action='store_true',
-                        help="enable interactive PDB debugger on exception")
-    parser.add_argument('inputs', nargs='*',
-                        help="input files to process")
+
+    parser = argparse.ArgumentParser(description="PURPOSE", epilog="", fromfile_prefix_chars="@")
+    parser.add_argument(
+        "-v",
+        "--verbose",
+        dest="verbosity",
+        action="count",
+        default=0,
+        help="each occurrence increases verbosity 1 level through ERROR-WARNING-Info-DEBUG",
+    )
+    parser.add_argument(
+        "-d", "--debug", dest="debug", action="store_true", help="enable interactive PDB debugger on exception"
+    )
+    parser.add_argument("inputs", nargs="*", help="input files to process")
     args = parser.parse_args()
 
     levels = [logging.ERROR, logging.WARN, logging.INFO, logging.DEBUG]
-    logging.basicConfig(level=levels[min(3, args.verbosity)], datefmt='%Y-%m-%dT%H:%M:%S',
-                        format='%(levelname)s %(asctime)s %(module)s:%(funcName)s:L%(lineno)d %(message)s')
+    logging.basicConfig(
+        level=levels[min(3, args.verbosity)],
+        datefmt="%Y-%m-%dT%H:%M:%S",
+        format="%(levelname)s %(asctime)s %(module)s:%(funcName)s:L%(lineno)d %(message)s",
+    )
 
     if args.debug:
         sys.excepthook = _debug
 
     if not args.inputs:
         unittest.main()
         return 0
 
     ws = _workspace_test_proxy()
     collector = ResourceSearchPathCollector(ws)
     collector.paths = list(args.inputs)
 
     from time import sleep
+
     for i in range(3):
         if i > 0:
             sleep(5)
         LOG.info("poll #{}".format(i + 1))
         collector.look_for_new_files()
         if collector.has_pending_files:
             for progress in collector.bgnd_merge_new_file_metadata_into_mdb():
                 LOG.debug(repr(progress))
 
     return 0
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     sys.exit(main())
```

### Comparing `uwsift-1.2.3/uwsift/workspace/guidebook.py` & `uwsift-2.0.0b0/uwsift/workspace/guidebook.py`

 * *Files 10% similar despite different names*

```diff
@@ -9,24 +9,24 @@
 
 
 
 :author: R.K.Garcia <rayg@ssec.wisc.edu>
 :copyright: 2014 by University of Wisconsin Regents, see AUTHORS for more details
 :license: GPLv3, see LICENSE for more details
 """
-__author__ = 'rayg'
-__docformat__ = 'reStructuredText'
+__author__ = "rayg"
+__docformat__ = "reStructuredText"
 
 import logging
 
-from uwsift.common import Info, Platform, Instrument
-from uwsift.view.colormap import DEFAULT_IR, DEFAULT_VIS, DEFAULT_UNKNOWN
+from uwsift.common import INVALID_COLOR_LIMITS, Info, Instrument, Platform
+from uwsift.view.colormap import DEFAULT_IR, DEFAULT_UNKNOWN, DEFAULT_VIS
 
 LOG = logging.getLogger(__name__)
-GUIDEBOOKS = {}
+GUIDEBOOKS: dict = {}
 
 
 class Guidebook(object):
     """
     guidebook which knows about AHI, ABI, AMI bands, timing, file naming conventions
     """
 
@@ -48,20 +48,20 @@
         :return: (list,offset:int): list of [uuid,uuid,uuid] for siblings in order;
             offset of where the input is found in list
         """
         return None, None
 
 
 DEFAULT_COLORMAPS = {
-    'toa_bidirectional_reflectance': DEFAULT_VIS,
-    'toa_brightness_temperature': DEFAULT_IR,
-    'brightness_temperature': DEFAULT_IR,
-    'height_at_cloud_top': 'Cloud Top Height',
-    'air_temperature': DEFAULT_IR,
-    'relative_humidity': DEFAULT_IR,
+    "toa_bidirectional_reflectance": DEFAULT_VIS,
+    "toa_brightness_temperature": DEFAULT_IR,
+    "brightness_temperature": DEFAULT_IR,
+    "height_at_cloud_top": "Cloud Top Height",
+    "air_temperature": DEFAULT_IR,
+    "relative_humidity": DEFAULT_IR,
     # 'thermodynamic_phase_of_cloud_water_particles_at_cloud_top': 'Cloud Phase',
 }
 
 _NW_GOESR_ABI = {
     Instrument.ABI: {  # http://www.goes-r.gov/education/ABI-bands-quick-info.html
         1: 0.47,
         2: 0.64,
@@ -103,15 +103,14 @@
     },
 }
 
 # Instrument -> Band Number -> Nominal Wavelength
 NOMINAL_WAVELENGTHS = {
     Platform.HIMAWARI_8: _NW_HIMAWARI_AHI,
     Platform.HIMAWARI_9: _NW_HIMAWARI_AHI,
-
     Platform.GOES_16: _NW_GOESR_ABI,
     Platform.GOES_17: _NW_GOESR_ABI,
     Platform.GOES_18: _NW_GOESR_ABI,
     Platform.GOES_19: _NW_GOESR_ABI,
 }
 
 # CF compliant Standard Names (should be provided by input files or the workspace)
@@ -162,15 +161,15 @@
 STANDARD_NAMES = {
     Platform.HIMAWARI_8: _SN_HIMAWARI_AHI,
     Platform.HIMAWARI_9: _SN_HIMAWARI_AHI,
     Platform.GOES_16: _SN_GOESR_ABI,
     Platform.GOES_17: _SN_GOESR_ABI,
 }
 
-BT_STANDARD_NAMES = ["toa_brightness_temperature", 'brightness_temperature', 'air_temperature']
+BT_STANDARD_NAMES = ["toa_brightness_temperature", "brightness_temperature", "air_temperature"]
 
 
 class ABI_AHI_Guidebook(Guidebook):
     "e.g. HS_H08_20150714_0030_B10_FLDK_R20.merc.tif"
     _cache = None  # {uuid:metadata-dictionary, ...}
 
     def __init__(self):
@@ -178,104 +177,89 @@
 
     def collect_info(self, info):
         """Collect information that may not come from the dataset.
 
         This method should only be called once to "fill in" metadata
         that isn't originally known about an opened file. The provided `info`
         is used as a starting point, but is not modified by this method.
-        
+
         """
         z = {}
 
-        band_short_name = info.get(Info.DATASET_NAME, '???')
+        band_short_name = info.get(Info.DATASET_NAME, "???")
         # FIXME: Don't use pure DATASET_NAME since resolution should not be part of the SHORT_NAME
         #        And/or don't use SHORT_NAME for grouping
         if Info.SHORT_NAME not in info:
             z[Info.SHORT_NAME] = band_short_name
         else:
             z[Info.SHORT_NAME] = info[Info.SHORT_NAME]
         if Info.LONG_NAME not in info:
             z[Info.LONG_NAME] = info.get(Info.SHORT_NAME, z[Info.SHORT_NAME])
 
-        z.setdefault(Info.STANDARD_NAME, info.get(Info.STANDARD_NAME, 'unknown'))
-        if info.get(Info.UNITS, z.get(Info.UNITS)) in ['K', 'Kelvin']:
-            z[Info.UNITS] = 'kelvin'
+        z.setdefault(Info.STANDARD_NAME, info.get(Info.STANDARD_NAME, "unknown"))
+        if info.get(Info.UNITS, z.get(Info.UNITS)) in ["K", "Kelvin"]:
+            z[Info.UNITS] = "kelvin"
 
         return z
 
-    def _is_refl(self, dsi):
-        return dsi.get(Info.STANDARD_NAME) == "toa_bidirectional_reflectance"
+    def _is_refl(self, info):
+        return info.get(Info.STANDARD_NAME) == "toa_bidirectional_reflectance"
+
+    def _is_bt(self, info):
+        return info.get(Info.STANDARD_NAME) in BT_STANDARD_NAMES
 
-    def _is_bt(self, dsi):
-        return dsi.get(Info.STANDARD_NAME) in BT_STANDARD_NAMES
+    def valid_range(self, info):
+        from uwsift.workspace.utils.metadata_utils import get_default_climits
 
-    def climits(self, dsi):
-        # Valid min and max for colormap use for data values in file (unconverted)
-        if self._is_refl(dsi):
-            lims = (-0.012, 1.192)
-            if dsi[Info.UNITS] == '%':
+        configured_climits = get_default_climits(info)
+        if configured_climits != INVALID_COLOR_LIMITS:
+            valid_range = configured_climits
+        elif self._is_refl(info):
+            valid_range = (-0.012, 1.192)
+            if info[Info.UNITS] == "%":
                 # Reflectance/visible data limits
-                lims = (lims[0] * 100., lims[1] * 100.)
-            return lims
-        elif self._is_bt(dsi):
+                valid_range = (valid_range[0] * 100.0, valid_range[1] * 100.0)
+        elif self._is_bt(info):
             # BT data limits
-            return -109.0 + 273.15, 55 + 273.15
-        elif "valid_min" in dsi and "valid_max" in dsi:
-            return dsi["valid_min"], dsi["valid_max"]
-        elif "flag_values" in dsi:
-            return min(dsi["flag_values"]), max(dsi["flag_values"])
-        elif "valid_range" in dsi:
-            return dsi['valid_range']
-        elif Info.VALID_RANGE in dsi:
-            return dsi[Info.VALID_RANGE]
+            valid_range = (-109.0 + 273.15, 55 + 273.15)
+        elif "valid_min" in info:
+            valid_range = (info["valid_min"], info["valid_max"])
+        elif "valid_range" in info:
+            valid_range = tuple(info["valid_range"])
+        elif "flag_values" in info:
+            valid_range = (min(info["flag_values"]), max(info["flag_values"]))
         else:
-            # some kind of default
-            return 0., 255.
+            valid_range = None
+        return valid_range
 
-    def valid_range(self, dsi):
-        if 'valid_min' in dsi:
-            valid_range = (dsi['valid_min'], dsi['valid_max'])
-        elif 'valid_range' in dsi:
-            valid_range = dsi['valid_range']
-        else:
-            valid_range = dsi[Info.CLIM]
-        return dsi.setdefault(Info.VALID_RANGE, valid_range)
-
-    def default_colormap(self, dsi):
-        return DEFAULT_COLORMAPS.get(dsi.get(Info.STANDARD_NAME), DEFAULT_UNKNOWN)
+    def default_colormap(self, info):
+        return DEFAULT_COLORMAPS.get(info.get(Info.STANDARD_NAME), DEFAULT_UNKNOWN)
 
-    def _default_display_time(self, ds_info):
+    def _default_display_time(self, info):
         # FUTURE: This can be customized by the user
-        when = ds_info.get(Info.SCHED_TIME, ds_info.get(Info.OBS_TIME))
+        when = info.get(Info.SCHED_TIME, info.get(Info.OBS_TIME))
         if when is None:
-            dtime = '--:--:--'
-        elif 'model_time' in ds_info:
-            dtime = "{}Z +{}h".format(
-                ds_info['model_time'].strftime('%Y-%m-%d %H:%M'),
-                when.strftime('%H')
-            )
+            dtime = "--:--:--"
+        elif "model_time" in info:
+            dtime = "{}Z +{}h".format(info["model_time"].strftime("%Y-%m-%d %H:%M"), when.strftime("%H"))
         else:
-            dtime = when.strftime('%Y-%m-%d %H:%M:%S')
+            dtime = when.strftime("%Y-%m-%d %H:%M:%S")
         return dtime
 
-    def _default_display_name(self, ds_info, display_time=None):
+    def _default_display_name(self, info, display_time=None):
         # FUTURE: This can be customized by the user
-        sat = ds_info[Info.PLATFORM]
-        inst = ds_info[Info.INSTRUMENT]
-        name = ds_info.get(Info.SHORT_NAME, '-unknown-')
-
-        label = ds_info.get(Info.STANDARD_NAME, '')
-        if label == 'toa_bidirectional_reflectance':
-            label = 'Refl'
-        elif label == 'toa_brightness_temperature':
-            label = 'BT'
+        platform = info.get(Info.PLATFORM, "-unknown-")
+        instrument = info.get(Info.INSTRUMENT, "-unknown-")
+        short_name = info.get(Info.SHORT_NAME, "-unknown-")
+
+        standard_name = info.get(Info.STANDARD_NAME, "")
+        if standard_name == "toa_bidirectional_reflectance":
+            label = " Refl"  # Don't remove the leading space
+        elif standard_name == "toa_brightness_temperature":
+            label = " BT"  # Don't remove the leading space
         else:
-            label = ''
+            label = ""
 
         if display_time is None:
-            display_time = ds_info.get(Info.DISPLAY_TIME, self._default_display_time(ds_info))
-        name = "{sat} {inst} {name} {standard_name} {dtime}".format(
-            sat=sat.value, inst=inst.value, name=name, standard_name=label, dtime=display_time)
-        return name
-
-# if __name__ == '__main__':
-#     sys.exit(main())
+            display_time = info.get(Info.DISPLAY_TIME, self._default_display_time(info))
+        display_name = f"{platform.value} {instrument.value} {short_name}{label} {display_time}"
+        return display_name
```

### Comparing `uwsift-1.2.3/uwsift/workspace/importer.py` & `uwsift-2.0.0b0/uwsift/workspace/importer.py`

 * *Files 17% similar despite different names*

```diff
@@ -9,274 +9,371 @@
 
 :author: R.K.Garcia <rkgarcia@wisc.edu>
 :copyright: 2017 by University of Wisconsin Regents, see AUTHORS for more details
 :license: GPLv3, see LICENSE for more details
 """
 import logging
 import os
-import re
 from abc import ABC, abstractmethod
 from collections import namedtuple
 from datetime import datetime, timedelta
-from typing import Iterable, Generator, Mapping
+from typing import (
+    Callable,
+    Dict,
+    Generator,
+    Iterable,
+    List,
+    Mapping,
+    Optional,
+    Set,
+    Tuple,
+    Union,
+)
 
-import yaml
+import dask.array as da
 import numpy as np
-from pyproj import Proj
-from sqlalchemy.orm import Session
-
-from uwsift import config
-from uwsift.common import Platform, Info, Instrument, Kind, INSTRUMENT_MAP, PLATFORM_MAP
+import satpy.readers.yaml_reader
+import satpy.resample
+import yaml
+from pyresample.geometry import AreaDefinition, StackedAreaDefinition, SwathDefinition
+from satpy import DataQuery, Scene, available_readers
+from satpy.dataset import DatasetDict
+from satpy.writers import get_enhanced_image
+from xarray import DataArray
+
+from uwsift import USE_INVENTORY_DB, config
+from uwsift.common import INSTRUMENT_MAP, PLATFORM_MAP, Info, Instrument, Kind, Platform
+from uwsift.model.area_definitions_manager import AreaDefinitionsManager
+from uwsift.satpy_compat import DataID, get_id_items, get_id_value, id_from_attrs
 from uwsift.util import USER_CACHE_DIR
-from uwsift.workspace.guidebook import ABI_AHI_Guidebook, Guidebook
-from .metadatabase import Resource, Product, Content
+from uwsift.util.common import get_reader_kwargs_dict
+from uwsift.workspace.guidebook import ABI_AHI_Guidebook
 
-from satpy import Scene, available_readers, __version__ as satpy_version
-from uwsift.satpy_compat import DataID, get_id_value, get_id_items, id_from_attrs
+from .metadatabase import (
+    Content,
+    ContentImage,
+    ContentMultiChannelImage,
+    ContentUnstructuredPoints,
+    Product,
+    Resource,
+)
+from .utils import metadata_utils
 
 _SATPY_READERS = None  # cache: see `available_satpy_readers()` below
-SATPY_READER_CACHE_FILE = os.path.join(USER_CACHE_DIR,
-                                       'available_satpy_readers.yaml')
+SATPY_READER_CACHE_FILE = os.path.join(USER_CACHE_DIR, "available_satpy_readers.yaml")
 
 LOG = logging.getLogger(__name__)
 
-try:
-    from skimage.measure import find_contours
-except ImportError:
-    find_contours = None
+satpy_version = None
 
 DEFAULT_GTIFF_OBS_DURATION = timedelta(seconds=60)
 DEFAULT_GUIDEBOOK = ABI_AHI_Guidebook
 
 GUIDEBOOKS = {
     Platform.GOES_16: ABI_AHI_Guidebook,
     Platform.GOES_17: ABI_AHI_Guidebook,
     Platform.GOES_18: ABI_AHI_Guidebook,
     Platform.GOES_19: ABI_AHI_Guidebook,
     Platform.HIMAWARI_8: ABI_AHI_Guidebook,
     Platform.HIMAWARI_9: ABI_AHI_Guidebook,
 }
 
-import_progress = namedtuple('import_progress',
-                             ['uuid', 'stages', 'current_stage', 'completion', 'stage_desc', 'dataset_info', 'data'])
+import_progress = namedtuple(
+    "import_progress",
+    ["uuid", "stages", "current_stage", "completion", "stage_desc", "dataset_info", "data", "content"],
+)
 """
 # stages:int, number of stages this import requires
 # current_stage:int, 0..stages-1 , which stage we're on
 # completion:float, 0..1 how far we are along on this stage
 # stage_desc:tuple(str), brief description of each of the stages we'll be doing
 """
 
+try:
+    import hdf5plugin  # noqa F401 # suppress "'hdf5plugin' imported but unused"
+except ImportError:
+    LOG.warning(
+        "Your installation/environment doesn't include the package hdf5plugin. If you want to visualise "
+        'compressed FCI L1c data, please add this package using "pip install hdf5plugin" or '
+        '"conda install -c conda-forge hdf5plugin "..'
+    )
+
 
 def _load_satpy_readers_cache(force_refresh=None):
     """Get Satpy reader information from a cache file or Satpy itself."""
     if force_refresh is None:
         force_refresh = os.getenv("UWSIFT_SATPY_CACHE_REFRESH", "False").lower()
         force_refresh = force_refresh in [True, "true"]
 
     try:
         if force_refresh:
             raise RuntimeError("Forcing refresh of available Satpy readers list")
-        with open(SATPY_READER_CACHE_FILE, 'r') as cfile:
+        if not satpy_version:
+            raise RuntimeError("Satpy version cannot be determined, regenerating available readers...")
+        with open(SATPY_READER_CACHE_FILE, "r") as cfile:
             LOG.info("Loading cached available Satpy readers from {}".format(SATPY_READER_CACHE_FILE))
             cache_contents = yaml.load(cfile, yaml.SafeLoader)
         if cache_contents is None:
             raise RuntimeError("Cached reader list is empty, regenerating...")
-        if cache_contents['satpy_version'] < satpy_version:
-            raise RuntimeError("Satpy has been updated, regenerating available readers...")
+        if cache_contents["satpy_version"] != satpy_version:
+            raise RuntimeError("Satpy has different version, regenerating available readers...")
     except (FileNotFoundError, RuntimeError, KeyError) as cause:
         LOG.info("Updating list of available Satpy readers...")
         cause.__suppress_context__ = True
         readers = available_readers(as_dict=True)
         # sort list of readers just in case we depend on this in the future
-        readers = sorted(readers, key=lambda x: x['name'])
+        readers = sorted(readers, key=lambda x: x["name"])
         readers = list(_sanitize_reader_info_for_yaml(readers))
         cache_contents = {
-            'satpy_version': satpy_version,
-            'readers': readers,
+            "satpy_version": satpy_version,
+            "readers": readers,
         }
         _save_satpy_readers_cache(cache_contents)
-    return cache_contents['readers']
+    return cache_contents["readers"]
 
 
 def _sanitize_reader_info_for_yaml(readers):
     # filter out known python objects to simplify YAML serialization
     for reader_info in readers:
-        reader_info.pop('reader')
-        reader_info.pop('data_identification_keys', None)
-        reader_info['config_files'] = list(reader_info['config_files'])
+        reader_info.pop("reader")
+        reader_info.pop("data_identification_keys", None)
+        reader_info["config_files"] = list(reader_info["config_files"])
         yield reader_info
 
 
 def _save_satpy_readers_cache(cache_contents):
     """Write reader cache information to a file on disk."""
     cfile_dir = os.path.dirname(SATPY_READER_CACHE_FILE)
     os.makedirs(cfile_dir, exist_ok=True)
-    with open(SATPY_READER_CACHE_FILE, 'w') as cfile:
+    with open(SATPY_READER_CACHE_FILE, "w") as cfile:
         LOG.info("Caching available Satpy readers to {}".format(SATPY_READER_CACHE_FILE))
         yaml.dump(cache_contents, cfile)
 
 
 def available_satpy_readers(as_dict=False, force_cache_refresh=None):
     """Get a list of reader names or reader information."""
     global _SATPY_READERS
     if _SATPY_READERS is None or force_cache_refresh:
         _SATPY_READERS = _load_satpy_readers_cache(force_refresh=force_cache_refresh)
 
     if not as_dict:
-        return [r['name'] for r in _SATPY_READERS]
+        return [r["name"] for r in _SATPY_READERS]
     return _SATPY_READERS
 
 
 def filter_dataset_ids(ids_to_filter: Iterable[DataID]) -> Generator[DataID, None, None]:
     """Generate only non-filtered DataIDs based on EXCLUDE_DATASETS global filters."""
     # skip certain DataIDs
     for ds_id in ids_to_filter:
-        for filter_key, filtered_values in config.get('data_reading.exclude_datasets').items():
-            if get_id_value(ds_id, filter_key) in filtered_values:
+        for filter_key, filtered_values in config.get("open_file_wizard.exclude_datasets").items():
+            if filtered_values and get_id_value(ds_id, filter_key) in filtered_values:
                 break
         else:
             yield ds_id
 
 
-def get_guidebook_class(layer_info) -> Guidebook:
-    platform = layer_info.get(Info.PLATFORM)
+def get_guidebook_class(dataset_info) -> ABI_AHI_Guidebook:
+    platform = dataset_info.get(Info.PLATFORM)
     return GUIDEBOOKS.get(platform, DEFAULT_GUIDEBOOK)()
 
 
-def get_contour_increments(layer_info):
-    standard_name = layer_info[Info.STANDARD_NAME]
-    units = layer_info[Info.UNITS]
-    increments = {
-        'air_temperature': [10., 5., 2.5, 1., 0.5],
-        'brightness_temperature': [10., 5., 2.5, 1., 0.5],
-        'toa_brightness_temperature': [10., 5., 2.5, 1., 0.5],
-        'toa_bidirectional_reflectance': [.15, .10, .05, .02, .01],
-        'relative_humidity': [15., 10., 5., 2., 1.],
-        'eastward_wind': [15., 10., 5., 2., 1.],
-        'northward_wind': [15., 10., 5., 2., 1.],
-        'geopotential_height': [100., 50., 25., 10., 5.],
-    }
-
-    unit_increments = {
-        'kelvin': [10., 5., 2.5, 1., 0.5],
-        '1': [.15, .10, .05, .02, .01],
-        '%': [15., 10., 5., 2., 1.],
-        'kg m**-2': [20., 10., 5., 2., 1.],
-        'm s**-1': [15., 10., 5., 2., 1.],
-        'm**2 s**-1': [5000., 1000., 500., 200., 100.],
-        'gpm': [6000., 2000., 1000., 500., 200.],
-        'kg kg**-1': [16e-8, 8e-8, 4e-8, 2e-8, 1e-8],
-        'Pa s**-1': [6., 3., 2., 1., 0.5],
-        's**-1': [0.02, 0.01, 0.005, 0.002, 0.001],
-        'Pa': [5000., 1000., 500., 200., 100.],
-        'J kg**-1': [5000., 1000., 500., 200., 100.],
-        '(0 - 1)': [0.5, 0.2, 0.1, 0.05, 0.05],
-    }
-
-    contour_increments = increments.get(standard_name)
-    if contour_increments is None:
-        contour_increments = unit_increments.get(units)
-
-        # def _in_group(wg, grp):
-        #     return round(wg / grp) * grp >= grp
-        # contour_increments = [5., 2.5, 1., 0.5, 0.1]
-        # vmin, vmax = layer_info[Info.VALID_RANGE]
-        # width_guess = vmax - vmin
-        # if _in_group(width_guess, 10000.):
-        #     contour_increments = [x * 100. for x in contour_increments]
-        # elif _in_group(width_guess, 1000.):
-        #     contour_increments = [x * 50. for x in contour_increments]
-        # elif _in_group(width_guess, 100.):
-        #     contour_increments = [x * 10. for x in contour_increments]
-        # elif _in_group(width_guess, 10.):
-        #     contour_increments = [x * 1. for x in contour_increments]
-    if contour_increments is None:
-        LOG.warning("Unknown contour data type ({}, {}), guessing at contour "
-                    "levels...".format(standard_name, units))
-        return [5000., 1000., 500., 200., 100.]
-
-    LOG.debug("Contour increments for ({}, {}): {}".format(
-        standard_name, units, contour_increments))
-    return contour_increments
-
-
-def get_contour_levels(vmin, vmax, increments):
-    levels = []
-    mult = 1 / increments[-1]
-    for idx, inc in enumerate(increments):
-        vmin_round = np.ceil(vmin / inc) * inc
-        vmax_round = np.ceil(vmax / inc) * inc
-        inc_levels = np.arange(vmin_round, vmax_round, inc)
-        # round to the highest increment or modulo operations will be wrong
-        inc_levels = np.round(inc_levels / increments[-1]) * increments[-1]
-        if idx > 0:
-            # don't use coarse contours in the finer contour levels
-            # we multiple by 1 / increments[-1] to try to resolve precision
-            # errors which can be a big issue for very small increments.
-            mask = np.logical_or.reduce([np.isclose((inc_levels * mult) % (i * mult), 0) for i in increments[:idx]])
-            inc_levels = inc_levels[~mask]
-        levels.append(inc_levels)
-
-    return levels
-
-
-def generate_guidebook_metadata(layer_info) -> Mapping:
-    guidebook = get_guidebook_class(layer_info)
-    # also get info for this layer from the guidebook
-    gbinfo = guidebook.collect_info(layer_info)
-    layer_info.update(gbinfo)  # FUTURE: should guidebook be integrated into DocBasicLayer?
+def generate_guidebook_metadata(info) -> Mapping:
+    guidebook = get_guidebook_class(info)
+    # also get more values for this info from the guidebook
+    gbinfo = guidebook.collect_info(info)
+    info.update(gbinfo)  # FUTURE: should guidebook be integrated into suitable place?
 
     # add as visible to the front of the current set, and invisible to the rest of the available sets
-    layer_info[Info.COLORMAP] = guidebook.default_colormap(layer_info)
-    layer_info[Info.CLIM] = guidebook.climits(layer_info)
-    layer_info[Info.VALID_RANGE] = guidebook.valid_range(layer_info)
-    if Info.DISPLAY_TIME not in layer_info:
-        layer_info[Info.DISPLAY_TIME] = guidebook._default_display_time(layer_info)
-    if Info.DISPLAY_NAME not in layer_info:
-        layer_info[Info.DISPLAY_NAME] = guidebook._default_display_name(layer_info)
-
-    if 'level' in layer_info:
-        # calculate contour_levels and zoom levels
-        increments = get_contour_increments(layer_info)
-        vmin, vmax = layer_info[Info.VALID_RANGE]
-        contour_levels = get_contour_levels(vmin, vmax, increments)
-        layer_info['contour_levels'] = contour_levels
+    info[Info.COLORMAP] = metadata_utils.get_default_colormap(info, guidebook)
+    if info[Info.KIND] == Kind.POINTS:
+        info[Info.STYLE] = metadata_utils.get_default_point_style_name(info)
+    valid_range = guidebook.valid_range(info)
+    if valid_range is not None:
+        info[Info.VALID_RANGE] = valid_range
+    if Info.DISPLAY_TIME not in info:
+        info[Info.DISPLAY_TIME] = guidebook._default_display_time(info)
+    if Info.DISPLAY_NAME not in info:
+        info[Info.DISPLAY_NAME] = guidebook._default_display_name(info)
+
+    return info
+
+
+def _get_types_of_required_aux_files(scn: Scene) -> Set[str]:
+    """
+    Get the file types of required auxiliary files in the given Scene.
+
+    For example, the types 'HRIT_PRO', 'HRIT_EPI' for the prolog/epilog files of
+    SEVIRI in HRIT format.
+
+    :param scn: Scene object to analyse
+    :return: set of required file types
+    """
+    required_aux_file_types = set()
+    for reader in scn._readers.values():
+        for file_handlers in reader.file_handlers.values():
+            for file_handler in file_handlers:
+                required_aux_file_types |= set(file_handler.filetype_info.get("requires", ()))
+    return required_aux_file_types
+
+
+def _get_paths_of_required_aux_files(scn: Scene, required_aux_file_types) -> Set[str]:
+    """
+    Get set of paths of auxiliary files required to load data in Scene.
+    For example, prolog/epilog files for SEVIRI in HRIT format.
+
+    :param scn: Scene object to analyse
+    :param required_aux_file_types: the file types which are required
+    :return: set of required files
+    """
+    required_aux_paths = set()
+    if required_aux_file_types:
+        for reader in scn._readers.values():
+            for file_handlers in reader.file_handlers.values():
+                for file_handler in file_handlers:
+                    if file_handler.filetype_info["file_type"] in required_aux_file_types:
+                        required_aux_paths.add(file_handler.filename)
+    return required_aux_paths
 
-    return layer_info
+
+def _get_paths_in_scene_contributing_to_ds(scn: Scene, ds_name: str) -> List[str]:
+    """
+    Get list of paths in scene which actually contain data for the dataset given
+    by its name `ds_name`.
+    :param scn: scene object to analyse
+    :param ds_name: dataset name
+    :return: paths in scene which are required to load given dataset
+    """
+    contributing_paths = []
+    for reader in scn._readers.values():
+        for file_handlers in reader.file_handlers.values():
+            for file_handler in file_handlers:
+                if not ds_name or not hasattr(file_handler, "channel_name") or file_handler.channel_name == ds_name:
+                    contributing_paths.append(file_handler.filename)
+    return contributing_paths
 
 
 class aImporter(ABC):
     """
     Abstract Importer class creates or amends Resource, Product, Content entries in the metadatabase used by Workspace
     aImporter instances are backgrounded by the Workspace to bring Content into the workspace
     """
-    # dedicated sqlalchemy database session to use during this import instance;
-    # revert if necessary, commit as appropriate
-    _S: Session = None
-    # where content flat files should be imported to within the workspace, omit this from content path
-    _cwd: str = None
 
     def __init__(self, workspace_cwd, database_session, **kwargs):
         super(aImporter, self).__init__()
+        # dedicated sqlalchemy database session to use during this import instance;
+        # revert if necessary, commit as appropriate
         self._S = database_session
+        # where content flat files should be imported to within the workspace, omit this from content path
         self._cwd = workspace_cwd
 
     @classmethod
     def from_product(cls, prod: Product, workspace_cwd, database_session, **kwargs):
         # FIXME: deal with products that need more than one resource
         try:
             cls = prod.resource[0].format
         except IndexError:
-            LOG.error('no resources in {} {}'.format(repr(type(prod)), repr(prod)))
+            LOG.error("no resources in {} {}".format(repr(type(prod)), repr(prod)))
             raise
         paths = [r.path for r in prod.resource]
-        # HACK for Satpy importer
-        if 'reader' in prod.info:
-            kwargs.setdefault('reader', prod.info['reader'])
-        return cls(paths, workspace_cwd=workspace_cwd, database_session=database_session, **kwargs)
+
+        # Get the Satpy Scene for this set of paths
+        scn = kwargs["scenes"].get(tuple(paths), None)
+
+        # Only Satpy readers / SatpyImporter still supported, thus:
+        # There must be such a Scene and the prod must be from Satpy, we can
+        assert scn and "_satpy_id" in prod.info  # nosec B101
+
+        merge_target = kwargs.get("merge_target")
+        if merge_target:
+            # For the merging process it is crucial that it only has
+            # to deal with files which belong to the given Product `prod`,
+            # extraneous files would interfere with the merging process.
+            # Thus: Filter the files in `paths` and keep only those
+            # that contribute to the Satpy dataset for `prod`.
+            if "prerequisites" in prod.info:
+                paths = cls._collect_prerequisites_paths(prod, scn)
+            else:
+                paths = _get_paths_in_scene_contributing_to_ds(scn, prod.info["_satpy_id"].get("name"))
+
+            # Now we can analyse, which files must be loaded
+            paths = cls._extract_paths_to_merge(merge_target, paths, scn)
+            if not paths:
+                return None
+
+        # In order to "load" converted datasets, we have to reuse the existing
+        # Scene 'scn' from the first instantiation of SatpyImporter instead of
+        # loading the data again, of course: we need to 'rescue across' the
+        # converted data from the first SatpyImporter instantiation to the
+        # second one (which will be created in the very last statement of this
+        # method).
+        # Since only products of the kinds POINTS, LINES and VECTORS need
+        # conversion (at this time), the mechanism is only applied for these.
+        # For an unknown reason reusing the scene breaks for FCI data,
+        #
+        # this Importer data reading magic is too confused.
+        if prod.info[Info.KIND] in (Kind.POINTS, Kind.LINES, Kind.VECTORS):
+            del kwargs["scenes"]
+            kwargs["scene"] = scn
+
+        # TODO: ignore mypy error for now because in the future aImporter should be merged with SatpyImporter
+        return cls(paths, workspace_cwd=workspace_cwd, database_session=database_session, **kwargs)  # type: ignore
+
+    @classmethod
+    def _extract_paths_to_merge(cls, merge_target, paths, scn):
+        """
+        Get a subset of 'paths', filtering out all files that are already loaded
+        into the target product.
+
+        I.e., collect only files from the given 'paths' list that are not yet
+        loaded and, if necessary, auxiliary files required to load these.
+        """
+        new_paths = []
+        existing_content = merge_target.content[0]
+        for path in paths:
+            if path not in existing_content.source_files:
+                new_paths.append(path)
+        if not new_paths:
+            return None
+        if new_paths != paths:
+            required_aux_file_types = _get_types_of_required_aux_files(scn)
+            required_aux_file_paths = _get_paths_of_required_aux_files(scn, required_aux_file_types)
+            if set(new_paths) == required_aux_file_paths:
+                return None
+            return new_paths + list(required_aux_file_paths)  # TODO
+        return new_paths
+
+    @classmethod
+    def _collect_prerequisites_paths(cls, prod, scn):
+        # If the dataset has prerequisites - this is the case for
+        # RGB composites provided by Satpy - the files that are
+        # required by these must be collected.
+        paths = []
+        for prerequisite in prod.info.get("prerequisites"):
+            ds_name = prerequisite.get("name") if isinstance(prerequisite, DataQuery) else prerequisite
+            if ds_name not in scn.available_dataset_names():
+                # This is a not supported case: Merging should be
+                # done for an RGB composite of which prequisite
+                # datasets are missing in the scene. This can
+                # happen, when the composite depends on other
+                # composites - their datasets have not been created
+                # in the Scene at this stage, thus the Scene can't
+                # be queried for the files required by these datasets.
+                # We have to interrupt this loading process (which
+                # runs in its own thread) by raising an exception.
+                raise RuntimeError(
+                    f"The Satpy RGB Composite type '{prod.info[Info.SHORT_NAME]}'"
+                    f" does not work when merging of new data chunks"
+                    f" into existing data is active."
+                    f" Consider switching it off by configuring"
+                    f" 'data_reading.merge_with_existing: False'"
+                )
+            paths += _get_paths_in_scene_contributing_to_ds(scn, ds_name)
+        # remove possible duplicates in paths
+        paths = list(dict.fromkeys(paths))
+        return paths
 
     @classmethod
     @abstractmethod
     def is_relevant(cls, source_path=None, source_uri=None) -> bool:
         """
         return True if this importer is capable of reading this URI.
         """
@@ -308,1049 +405,997 @@
         Args:
             *products: sequence of products to import
 
         Returns:
             generator which yields status tuples as the content is imported
         """
         # FUTURE: this should be async def coroutine
-        return
-
-
-class aSingleFileWithSingleProductImporter(aImporter):
-    """
-    simplification of importer that handles a single-file with a single product
-    """
-    source_path: str = None
-    _resource: Resource = None
-
-    def __init__(self, source_path, workspace_cwd, database_session, **kwargs):
-        if isinstance(source_path, list) and len(source_path) == 1:
-            # backwards compatibility - we now expect a list
-            source_path = source_path[0]
-        super(aSingleFileWithSingleProductImporter, self).__init__(workspace_cwd, database_session)
-        self.source_path = source_path
-
-    @property
-    def num_products(self):
-        return 1
-
-    def merge_resources(self) -> Iterable[Resource]:
-        """
-        Returns:
-            sequence of Resources found at the source, typically one resource per file
-        """
-        now = datetime.utcnow()
-        if self._resource is not None:
-            res = self._resource
-        else:
-            self._resource = res = self._S.query(Resource).filter(Resource.path == self.source_path).first()
-        if res is None:
-            LOG.debug('creating new Resource entry for {}'.format(self.source_path))
-            self._resource = res = Resource(
-                format=type(self),
-                path=self.source_path,
-                mtime=now,
-                atime=now,
-            )
-            self._S.add(res)
-        return [self._resource]
-
-    @abstractmethod
-    def product_metadata(self):
-        """
-        Returns:
-            info dictionary for the single product available
-        """
-        return {}
-
-    def merge_products(self) -> Iterable[Product]:
-        """
-        products available in the resource, adding any metadata entries for Products within the resource
-        this may be run by the metadata collection agent, or by the workspace!
-        Returns:
-            sequence of Products that could be turned into Content in the workspace
-        """
-        now = datetime.utcnow()
-        if self._resource is not None:
-            res = self._resource
-        else:
-            self._resource = res = self._S.query(Resource).filter(Resource.path == self.source_path).first()
-        if res is None:
-            LOG.debug('no resources for {}'.format(self.source_path))
-            return []
-
-        if len(res.product):
-            zult = list(res.product)
-            # LOG.debug('pre-existing products {}'.format(repr(zult)))
-            return zult
-
-        # else probe the file and add product metadata, without importing content
-        from uuid import uuid1
-        uuid = uuid1()
-        meta = self.product_metadata()
-        meta[Info.UUID] = uuid
-
-        prod = Product(
-            uuid_str=str(uuid),
-            atime=now,
-        )
-        prod.resource.append(res)
-        assert (Info.OBS_TIME in meta)
-        assert (Info.OBS_DURATION in meta)
-        prod.update(meta)  # sets fields like obs_duration and obs_time transparently
-        assert (prod.info[Info.OBS_TIME] is not None and prod.obs_time is not None)
-        assert (prod.info[Info.VALID_RANGE] is not None)
-        LOG.debug('new product: {}'.format(repr(prod)))
-        self._S.add(prod)
-        self._S.commit()
-        return [prod]
-
-
-class GeoTiffImporter(aSingleFileWithSingleProductImporter):
-    """
-    GeoTIFF data importer
-    """
-
-    @classmethod
-    def is_relevant(self, source_path=None, source_uri=None):
-        source = source_path or source_uri
-        return True if (source.lower().endswith('.tif') or source.lower().endswith('.tiff')) else False
-
-    @staticmethod
-    def _metadata_for_path(pathname):
-        meta = {}
-        if not pathname:
-            return meta
-
-        # Old but still necesary, get some information from the filename instead of the content
-        m = re.match(r'HS_H(\d\d)_(\d{8})_(\d{4})_B(\d\d)_([A-Za-z0-9]+).*', os.path.split(pathname)[1])
-        if m is not None:
-            plat, yyyymmdd, hhmm, bb, scene = m.groups()
-            when = datetime.strptime(yyyymmdd + hhmm, '%Y%m%d%H%M')
-            plat = Platform('Himawari-{}'.format(int(plat)))
-            band = int(bb)
-            #
-            # # workaround to make old files work with new information
-            # from uwsift.model.guidebook import AHI_HSF_Guidebook
-            # if band in AHI_HSF_Guidebook.REFL_BANDS:
-            #     standard_name = "toa_bidirectional_reflectance"
-            # else:
-            #     standard_name = "toa_brightness_temperature"
-
-            meta.update({
-                Info.PLATFORM: plat,
-                Info.BAND: band,
-                Info.INSTRUMENT: Instrument.AHI,
-                Info.SCHED_TIME: when,
-                Info.OBS_TIME: when,
-                Info.OBS_DURATION: DEFAULT_GTIFF_OBS_DURATION,
-                Info.SCENE: scene,
-            })
-        return meta
-
-    @staticmethod
-    def _check_geotiff_metadata(gtiff):
-        gtiff_meta = gtiff.GetMetadata()
-        # Sanitize metadata from the file to use SIFT's Enums
-        if "name" in gtiff_meta:
-            gtiff_meta[Info.DATASET_NAME] = gtiff_meta.pop("name")
-        if "platform" in gtiff_meta:
-            plat = gtiff_meta.pop("platform")
-            try:
-                gtiff_meta[Info.PLATFORM] = Platform(plat)
-            except ValueError:
-                gtiff_meta[Info.PLATFORM] = Platform.UNKNOWN
-                LOG.warning("Unknown platform being loaded: {}".format(plat))
-        if "instrument" in gtiff_meta or "sensor" in gtiff_meta:
-            inst = gtiff_meta.pop("sensor", gtiff_meta.pop("instrument", None))
-            try:
-                gtiff_meta[Info.INSTRUMENT] = Instrument(inst)
-            except ValueError:
-                gtiff_meta[Info.INSTRUMENT] = Instrument.UNKNOWN
-                LOG.warning("Unknown instrument being loaded: {}".format(inst))
-        if "start_time" in gtiff_meta:
-            start_time = datetime.strptime(gtiff_meta["start_time"], "%Y-%m-%dT%H:%M:%SZ")
-            gtiff_meta[Info.SCHED_TIME] = start_time
-            gtiff_meta[Info.OBS_TIME] = start_time
-            if "end_time" in gtiff_meta:
-                end_time = datetime.strptime(gtiff_meta["end_time"], "%Y-%m-%dT%H:%M:%SZ")
-                gtiff_meta[Info.OBS_DURATION] = end_time - start_time
-        if "valid_min" in gtiff_meta:
-            gtiff_meta["valid_min"] = float(gtiff_meta["valid_min"])
-        if "valid_max" in gtiff_meta:
-            gtiff_meta["valid_max"] = float(gtiff_meta["valid_max"])
-        if "standard_name" in gtiff_meta:
-            gtiff_meta[Info.STANDARD_NAME] = gtiff_meta["standard_name"]
-        if "flag_values" in gtiff_meta:
-            gtiff_meta["flag_values"] = tuple(int(x) for x in gtiff_meta["flag_values"].split(','))
-        if "flag_masks" in gtiff_meta:
-            gtiff_meta["flag_masks"] = tuple(int(x) for x in gtiff_meta["flag_masks"].split(','))
-        if "flag_meanings" in gtiff_meta:
-            gtiff_meta["flag_meanings"] = gtiff_meta["flag_meanings"].split(' ')
-        if "units" in gtiff_meta:
-            gtiff_meta[Info.UNITS] = gtiff_meta.pop('units')
-        return gtiff_meta
-
-    @staticmethod
-    def get_metadata(source_path=None, source_uri=None, **kwargs):
-        import gdal
-        import osr
-        if source_uri is not None:
-            raise NotImplementedError("GeoTiffImporter cannot read from URIs yet")
-        d = GeoTiffImporter._metadata_for_path(source_path)
-        gtiff = gdal.Open(source_path)
-
-        ox, cw, _, oy, _, ch = gtiff.GetGeoTransform()
-        d[Info.KIND] = Kind.IMAGE
-
-        # FUTURE: this is Content metadata and not Product metadata:
-        d[Info.ORIGIN_X] = ox
-        d[Info.ORIGIN_Y] = oy
-        d[Info.CELL_WIDTH] = cw
-        d[Info.CELL_HEIGHT] = ch
-        # FUTURE: Should the Workspace normalize all input data or should the Image Layer handle any projection?
-        srs = osr.SpatialReference()
-        srs.ImportFromWkt(gtiff.GetProjection())
-        d[Info.PROJ] = srs.ExportToProj4().strip()  # remove extra whitespace
-
-        # Workaround for previously supported files
-        # give them some kind of name that means something
-        if Info.BAND in d:
-            d[Info.DATASET_NAME] = "B{:02d}".format(d[Info.BAND])
-        else:
-            # for new files, use this as a basic default
-            # FUTURE: Use Dataset name instead when we can read multi-dataset files
-            d[Info.DATASET_NAME] = os.path.split(source_path)[-1]
-
-        band = gtiff.GetRasterBand(1)
-        d[Info.SHAPE] = rows, cols = (band.YSize, band.XSize)
-
-        # Fix PROJ4 string if it needs an "+over" parameter
-        p = Proj(d[Info.PROJ])
-        lon_l, lat_u = p(ox, oy, inverse=True)
-        lon_r, lat_b = p(ox + cw * cols, oy + ch * rows, inverse=True)
-        if "+over" not in d[Info.PROJ] and lon_r < lon_l:
-            LOG.debug("Add '+over' to geotiff PROJ.4 because it seems to cross the anti-meridian")
-            d[Info.PROJ] += " +over"
-
-        bandtype = gdal.GetDataTypeName(band.DataType)
-        if bandtype.lower() != 'float32':
-            LOG.warning('attempting to read geotiff files with non-float32 content')
-
-        gtiff_meta = GeoTiffImporter._check_geotiff_metadata(gtiff)
-        d.update(gtiff_meta)
-        generate_guidebook_metadata(d)
-        LOG.debug("GeoTIFF metadata for {}: {}".format(source_path, repr(d)))
-        return d
-
-    def product_metadata(self):
-        return GeoTiffImporter.get_metadata(self.source_path)
-
-    # @asyncio.coroutine
-    def begin_import_products(self, *product_ids):  # FUTURE: allow product_ids to be uuids
-        import gdal
-        source_path = self.source_path
-        if product_ids:
-            products = [self._S.query(Product).filter_by(id=anid).one() for anid in product_ids]
-        else:
-            products = list(self._S.query(Resource, Product).filter(
-                Resource.path == source_path).filter(
-                Product.resource_id == Resource.id).all())
-            assert (products)
-        if len(products) > 1:
-            LOG.warning('only first product currently handled in geotiff loader')
-        prod = products[0]
-
-        if prod.content:
-            LOG.info('content is already available, skipping import')
-            return
-
-        now = datetime.utcnow()
-
-        # re-collect the metadata, which should be separated between Product vs Content metadata in the FUTURE
-        # principally we're not allowed to store ORIGIN_ or CELL_ metadata in the Product
-        info = GeoTiffImporter.get_metadata(source_path)
-
-        # Additional metadata that we've learned by loading the data
-        gtiff = gdal.Open(source_path)
-
-        band = gtiff.GetRasterBand(1)  # FUTURE may be an assumption
-        shape = rows, cols = band.YSize, band.XSize
-        blockw, blockh = band.GetBlockSize()  # non-blocked files will report [band.XSize,1]
+        pass
 
-        data_filename = '{}.image'.format(prod.uuid)
-        data_path = os.path.join(self._cwd, data_filename)
-
-        coverage_filename = '{}.coverage'.format(prod.uuid)
-        coverage_path = os.path.join(self._cwd, coverage_filename)
-        # no sparsity map
-
-        # shovel that data into the memmap incrementally
-        # http://geoinformaticstutorial.blogspot.com/2012/09/reading-raster-data-with-python-and-gdal.html
-        img_data = np.memmap(data_path, dtype=np.float32, shape=shape, mode='w+')
-
-        # load at an increment that matches the file's tile size if possible
-        IDEAL_INCREMENT = 512.0
-        increment = min(blockh * int(np.ceil(IDEAL_INCREMENT / blockh)), 2048)
-
-        # how many coverage states are we traversing during the load?
-        # for now let's go simple and have it be just image rows
-        # coverage_rows = int((rows + increment - 1) / increment) if we had an even increment but it's not guaranteed
-        cov_data = np.memmap(coverage_path, dtype=np.int8, shape=(rows,), mode='w+')
-        cov_data[:] = 0  # should not be needed except maybe in Windows?
-
-        # LOG.debug("keys in geotiff product: {}".format(repr(list(prod.info.keys()))))
-        LOG.debug(
-            "cell size in geotiff product: {} x {}".format(prod.info[Info.CELL_HEIGHT], prod.info[Info.CELL_WIDTH]))
-
-        # create and commit a Content entry pointing to where the content is in the workspace, even if coverage is empty
-        c = Content(
-            lod=0,
-            resolution=int(min(abs(info[Info.CELL_WIDTH]), abs(info[Info.CELL_HEIGHT]))),
-            atime=now,
-            mtime=now,
-
-            # info about the data array memmap
-            path=data_filename,
-            rows=rows,
-            cols=cols,
-            levels=0,
-            dtype='float32',
-
-            cell_width=info[Info.CELL_WIDTH],
-            cell_height=info[Info.CELL_HEIGHT],
-            origin_x=info[Info.ORIGIN_X],
-            origin_y=info[Info.ORIGIN_Y],
-            proj4=info[Info.PROJ],
-
-            # info about the coverage array memmap, which in our case just tells what rows are ready
-            coverage_rows=rows,
-            coverage_cols=1,
-            coverage_path=coverage_filename
-        )
-        # c.info.update(prod.info) would just make everything leak together so let's not do it
-        self._S.add(c)
-        prod.content.append(c)
-        self._S.commit()
-
-        # FIXME: yield initial status to announce content is available, even if it's empty
-
-        # now do the actual array filling from the geotiff file
-        # FUTURE: consider explicit block loads using band.ReadBlock(x,y) once
-        irow = 0
-        while irow < rows:
-            nrows = min(increment, rows - irow)
-            row_data = band.ReadAsArray(0, irow, cols, nrows)
-            img_data[irow:irow + nrows, :] = np.require(row_data, dtype=np.float32)
-            cov_data[irow:irow + nrows] = 1
-            irow += increment
-            status = import_progress(uuid=prod.uuid,
-                                     stages=1,
-                                     current_stage=0,
-                                     completion=float(irow) / float(rows),
-                                     stage_desc="importing geotiff",
-                                     dataset_info=None,
-                                     data=img_data)
-            yield status
-
-        # img_data = gtiff.GetRasterBand(1).ReadAsArray()
-        # img_data = np.require(img_data, dtype=np.float32, requirements=['C'])  # FIXME: is this necessary/correct?
-        # normally we would place a numpy.memmap in the workspace with the content of the geotiff raster band/s here
-
-        # single stage import with all the data for this simple case
-        zult = import_progress(uuid=prod.uuid,
-                               stages=1,
-                               current_stage=0,
-                               completion=1.0,
-                               stage_desc="done loading geotiff",
-                               dataset_info=None,
-                               data=img_data)
-
-        yield zult
-
-        # Finally, update content mtime and atime
-        c.atime = c.mtime = datetime.utcnow()
-        # self._S.commit()
-
-
-# map .platform_id in PUG format files to SIFT platform enum
-PLATFORM_ID_TO_PLATFORM = {
-    'G16': Platform.GOES_16,
-    'G17': Platform.GOES_17,
-    'G18': Platform.GOES_18,
-    'G19': Platform.GOES_19,
-    # hsd2nc export of AHI data as PUG format
-    'Himawari-8': Platform.HIMAWARI_8,
-    'Himawari-9': Platform.HIMAWARI_9,
-    # axi2cmi export as PUG, more consistent with other uses
-    'H8': Platform.HIMAWARI_8,
-    'H9': Platform.HIMAWARI_9
-}
 
+def determine_dynamic_dataset_kind(attrs: dict, reader_name: str) -> str:
+    """Determine kind of dataset dynamically based on dataset attributes.
 
-class GoesRPUGImporter(aSingleFileWithSingleProductImporter):
-    """
-    Import from PUG format GOES-16 netCDF4 files
+    This currently supports only the distinction between IMAGE and POINTS kinds.
+    It makes the assumption that if the dataset has a SwathDefinition, and is 1-D, it represents points.
     """
+    if isinstance(attrs["area"], SwathDefinition) and len(attrs["area"].shape) == 1:
+        data_kind = "POINTS"
+    else:
+        data_kind = "IMAGE"
+    LOG.info(f"Selected dynamically kind {data_kind} for dataset {attrs['name']} from reader {reader_name}")
+    return data_kind
 
-    @staticmethod
-    def _basic_pug_metadata(pug):
-        return {
-            Info.PLATFORM: PLATFORM_ID_TO_PLATFORM[pug.platform_id],  # e.g. G16, H8
-            Info.BAND: pug.band,
-            Info.DATASET_NAME: 'B{:02d}'.format(pug.band),
-            Info.INSTRUMENT: Instrument.AHI if 'Himawari' in pug.instrument_type else Instrument.ABI,
-            Info.SCHED_TIME: pug.sched_time,
-            Info.OBS_TIME: pug.time_span[0],
-            Info.OBS_DURATION: pug.time_span[1] - pug.time_span[0],
-            Info.DISPLAY_TIME: pug.display_time,
-            Info.SCENE: pug.scene_id,
-            Info.DISPLAY_NAME: pug.display_name,
-        }
-
-    @classmethod
-    def is_relevant(cls, source_path=None, source_uri=None):
-        source = source_path or source_uri
-        return True if (source.lower().endswith('.nc') or source.lower().endswith('.nc4')) else False
-
-    @staticmethod
-    def pug_factory(source_path):
-        dn, fn = os.path.split(source_path)
-        is_netcdf = (fn.lower().endswith('.nc') or fn.lower().endswith('.nc4'))
-        if not is_netcdf:
-            raise ValueError("PUG loader requires files ending in .nc or .nc4: {}".format(repr(source_path)))
-        return PugFile.attach(source_path)  # noqa
-        # if 'L1b' in fn:
-        #     LOG.debug('attaching {} as PUG L1b'.format(source_path))
-        #     return PugL1bTools(source_path)
-        # else:
-        #     LOG.debug('attaching {} as PUG CMI'.format(source_path))
-        #     return PugCmiTools(source_path)
-
-    @staticmethod
-    def get_metadata(source_path=None, source_uri=None, pug=None, **kwargs):
-        # yield successive levels of detail as we load
-        if source_uri is not None:
-            raise NotImplementedError("GoesRPUGImporter cannot read from URIs yet")
-
-        #
-        # step 1: get any additional metadata and an overview tile
-        #
-
-        d = {}
-        # nc = nc4.Dataset(source_path)
-        pug = pug or GoesRPUGImporter.pug_factory(source_path)
-
-        d.update(GoesRPUGImporter._basic_pug_metadata(pug))
-        # d[Info.DATASET_NAME] = os.path.split(source_path)[-1]
-        d[Info.KIND] = Kind.IMAGE
-
-        # FUTURE: this is Content metadata and not Product metadata:
-        d[Info.PROJ] = pug.proj4_string
-        # get nadir-meter-ish projection coordinate vectors to be used by proj4
-        y, x = pug.proj_y, pug.proj_x
-        d[Info.ORIGIN_X] = x[0]
-        d[Info.ORIGIN_Y] = y[0]
-
-        # midyi, midxi = int(y.shape[0] / 2), int(x.shape[0] / 2)
-        # PUG states radiance at index [0,0] extends between coordinates [0,0] to [1,1] on a quadrille
-        # centers of pixels are therefore at +0.5, +0.5
-        # for a (e.g.) H x W image this means [H/2,W/2] coordinates are image center
-        # for now assume all scenes are even-dimensioned (e.g. 5424x5424)
-        # given that coordinates are evenly spaced in angular -> nadir-meters space,
-        # technically this should work with any two neighbor values
-        # d[Info.CELL_WIDTH] = x[midxi+1] - x[midxi]
-        # d[Info.CELL_HEIGHT] = y[midyi+1] - y[midyi]
-        cell_size = pug.cell_size
-        d[Info.CELL_HEIGHT], d[Info.CELL_WIDTH] = cell_size
-
-        shape = pug.shape
-        d[Info.SHAPE] = shape
-        generate_guidebook_metadata(d)
-
-        d[Info.FAMILY] = '{}:{}:{}:{:5.2f}m'.format(Kind.IMAGE.name, 'geo', d[Info.STANDARD_NAME], d[
-            Info.CENTRAL_WAVELENGTH])  # kind:pointofreference:measurement:wavelength
-        d[Info.CATEGORY] = 'NOAA-PUG:{}:{}:{}'.format(d[Info.PLATFORM].name, d[Info.INSTRUMENT].name,
-                                                      d[Info.SCENE])  # system:platform:instrument:target
-        d[Info.SERIAL] = d[Info.SCHED_TIME].strftime("%Y%m%dT%H%M%S")
-        LOG.debug(repr(d))
-        return d
-
-    # def __init__(self, source_path, workspace_cwd, database_session, **kwargs):
-    #     super(GoesRPUGImporter, self).__init__(workspace_cwd, database_session)
-    #     self.source_path = source_path
-
-    def product_metadata(self):
-        return GoesRPUGImporter.get_metadata(self.source_path)
-
-    # @asyncio.coroutine
-    def begin_import_products(self, *product_ids):
-        source_path = self.source_path
-        if product_ids:
-            products = [self._S.query(Product).filter_by(id=anid).one() for anid in product_ids]
-            assert (products)
-        else:
-            products = list(self._S.query(Resource, Product).filter(
-                Resource.path == source_path).filter(
-                Product.resource_id == Resource.id).all())
-            assert (products)
-        if len(products) > 1:
-            LOG.warning('only first product currently handled in pug loader')
-        prod = products[0]
-
-        if prod.content:
-            LOG.warning('content was already available, skipping import')
-            return
-
-        pug = GoesRPUGImporter.pug_factory(source_path)
-        rows, cols = shape = pug.shape
-        cell_height, cell_width = pug.cell_size
-        origin_y, origin_x = pug.origin
-        proj4 = pug.proj4_string
-
-        now = datetime.utcnow()
-
-        data_filename = '{}.image'.format(prod.uuid)
-        data_path = os.path.join(self._cwd, data_filename)
-
-        # coverage_filename = '{}.coverage'.format(prod.uuid)
-        # coverage_path = os.path.join(self._cwd, coverage_filename)
-        # no sparsity map
 
-        # shovel that data into the memmap incrementally
-        img_data = np.memmap(data_path, dtype=np.float32, shape=shape, mode='w+')
-
-        LOG.info('converting radiance to %s' % pug.bt_or_refl)
-        image = pug.bt if 'bt' == pug.bt_or_refl else pug.refl
-        # bt_or_refl, image, units = pug.convert_from_nc()  # FIXME expensive
-        # overview_image = fixme  # FIXME, we need a properly navigated overview image here
-
-        # we got some metadata, let's yield progress
-        # yield    import_progress(uuid=dest_uuid,
-        #                          stages=1,
-        #                          current_stage=0,
-        #                          completion=1.0/3.0,
-        #                          stage_desc="calculating imagery",
-        #                          dataset_info=d,
-        #                          data=image)
-
-        #
-        # step 2: read and convert the image data
-        #   - in chunks if it's a huge image so we can show progress and/or cancel
-        #   - push the data into a workspace memmap
-        #   - record the content information in the workspace metadatabase
-        #
-
-        # FUTURE as we're doing so, also update coverage array (showing what sections of data are loaded)
-        # FUTURE and for some cases the sparsity array, if the data is interleaved (N/A for NetCDF imagery)
-        img_data[:] = np.ma.fix_invalid(image, copy=False, fill_value=np.NAN)  # FIXME: expensive
-
-        # create and commit a Content entry pointing to where the content is in the workspace, even if coverage is empty
-        c = Content(
-            lod=0,
-            resolution=int(min(abs(cell_width), abs(cell_height))),
-            atime=now,
-            mtime=now,
+def set_kind_metadata_from_reader_config(reader_name: str, reader_kind: str, attrs: dict) -> None:
+    """Determine the dataset kind starting from the reader configuration."""
+    data_kind = determine_dynamic_dataset_kind(attrs, reader_name) if reader_kind == "DYNAMIC" else reader_kind
 
-            # info about the data array memmap
-            path=data_filename,
-            rows=rows,
-            cols=cols,
-            proj4=proj4,
-            # levels = 0,
-            dtype='float32',
-
-            # info about the coverage array memmap, which in our case just tells what rows are ready
-            # coverage_rows = rows,
-            # coverage_cols = 1,
-            # coverage_path = coverage_filename
-
-            cell_width=cell_width,
-            cell_height=cell_height,
-            origin_x=origin_x,
-            origin_y=origin_y,
-        )
-        # c.info.update(prod.info) would just make everything leak together so let's not do it
-        self._S.add(c)
-        prod.content.append(c)
-        # prod.touch()
-        self._S.commit()
-
-        yield import_progress(uuid=prod.uuid,
-                              stages=1,
-                              current_stage=0,
-                              completion=1.0,
-                              stage_desc="GOES PUG data add to workspace",
-                              dataset_info=None,
-                              data=img_data)
+    try:
+        attrs[Info.KIND] = Kind[data_kind]
+    except KeyError:
+        raise KeyError(f"Unknown data kind '{data_kind}' used for reader {reader_name}.")
 
 
 class SatpyImporter(aImporter):
     """Generic SatPy importer"""
 
     def __init__(self, source_paths, workspace_cwd, database_session, **kwargs):
         super(SatpyImporter, self).__init__(workspace_cwd, database_session)
-        reader = kwargs.pop('reader', None)
+        reader = kwargs.pop("reader", None)
         if reader is None:
             raise NotImplementedError("Can't automatically determine reader.")
         if not isinstance(source_paths, (list, tuple)):
             source_paths = [source_paths]
 
         if Scene is None:
-            raise ImportError("SatPy is not available and can't be used as "
-                              "an importer")
+            raise ImportError("SatPy is not available and can't be used as " "an importer")
         self.filenames = list(source_paths)
         self.reader = reader
-        self.scn = kwargs.get('scene')
+        self.resampling_info = kwargs.get("resampling_info")
+        self.scn = kwargs.get("scene")
+        self.merge_target = kwargs.get("merge_target")
         if self.scn is None:
-            self.scn = Scene(reader=self.reader, filenames=self.filenames)
+            reader_kwargs = get_reader_kwargs_dict([self.reader])
+            self.scn = Scene(filenames={self.reader: self.filenames}, reader_kwargs=reader_kwargs)
         self._resources = []
         # DataID filters
         self.product_filters = {}
-        for k in ['resolution', 'calibration', 'level']:
+        for k in ["resolution", "calibration", "level"]:
             if k in kwargs:
                 self.product_filters[k] = kwargs.pop(k)
         # NOTE: product_filters don't do anything if the dataset_ids aren't
         #       specified since we are using all available dataset ids
-        self.dataset_ids = kwargs.get('dataset_ids')
+        self.dataset_ids = kwargs.get("dataset_ids")
         if self.dataset_ids is None:
             self.dataset_ids = filter_dataset_ids(self.scn.available_dataset_ids())
         self.dataset_ids = sorted(self.dataset_ids)
 
+        self.use_inventory_db = USE_INVENTORY_DB
+        self.required_aux_file_types = _get_types_of_required_aux_files(self.scn)
+
+        # NOTE: needed for an issue with resampling of NetCDF data.
+        self.scn_original = None  # noqa - Do not simply remove
+
     @classmethod
     def from_product(cls, prod: Product, workspace_cwd, database_session, **kwargs):
         # this overrides the base class because we assume that the product
         # has kwargs that we want
         # NOTE: The kwargs are currently provided by the caller in
         # workspace.py so this isn't needed right now
         # FIXME: deal with products that need more than one resource
         try:
             cls = prod.resource[0].format
         except IndexError:
-            LOG.error('no resources in {} {}'.format(repr(type(prod)), repr(prod)))
+            LOG.error("no resources in {} {}".format(repr(type(prod)), repr(prod)))
             raise
-        kwargs.pop('reader', None)
-        kwargs.pop('scenes', None)
-        kwargs.pop('scene', None)
-        kwargs['dataset_ids'] = [prod.info['_satpy_id']]
+        kwargs.pop("reader", None)
+        kwargs.pop("scenes", None)
+        kwargs.pop("scene", None)
+        kwargs["dataset_ids"] = [prod.info["_satpy_id"]]
         filenames = [r.path for r in prod.resource]
         return cls(filenames, workspace_cwd=workspace_cwd, database_session=database_session, **kwargs)
 
     @classmethod
     def is_relevant(cls, source_path=None, source_uri=None):
         # this importer should only be used if specifically requested
         return False
 
     def merge_resources(self):
         if len(self._resources) == len(self.filenames):
             return self._resources
-
-        resources = self._S.query(Resource).filter(
-            Resource.path.in_(self.filenames)).all()
-        if len(resources) == len(self.filenames):
-            self._resources = resources
-            return self._resources
+        if self.use_inventory_db:
+            resources = self._S.query(Resource).filter(Resource.path.in_(self.filenames)).all()
+            if len(resources) == len(self.filenames):
+                self._resources = resources
+                return self._resources
 
         now = datetime.utcnow()
         res_dict = {r.path: r for r in self._resources}
         for fn in self.filenames:
             if fn in res_dict:
                 continue
 
             res = Resource(
                 format=type(self),
                 path=fn,
                 mtime=now,
                 atime=now,
             )
-            self._S.add(res)
+            if self.use_inventory_db:
+                self._S.add(res)
             res_dict[fn] = res
 
         self._resources = res_dict.values()
         return self._resources
 
     def merge_products(self) -> Iterable[Product]:
         resources = self.merge_resources()
         if resources is None:
-            LOG.debug('no resources for {}'.format(self.filenames))
+            LOG.debug("no resources for {}".format(self.filenames))
             return
 
         existing_ids = {}
-        resources = list(resources)
-        for res in resources:
-            products = list(res.product)
-            existing_ids.update({prod.info['_satpy_id']: prod for prod in products})
-        existing_prods = {x: existing_ids[x] for x in self.dataset_ids if x in existing_ids}
-        if products and len(existing_prods) == len(self.dataset_ids):
-            products = existing_prods.values()
-            LOG.debug('pre-existing products {}'.format(repr(products)))
-            yield from products
-            return
+        if self.use_inventory_db:
+            resources = list(resources)
+            for res in resources:
+                products = list(res.product)
+                existing_ids.update({prod.info["_satpy_id"]: prod for prod in products})
+            existing_prods = {x: existing_ids[x] for x in self.dataset_ids if x in existing_ids}
+            # FIXME: products may be unset in next line, its use is suspicious anyway
+            if products and len(existing_prods) == len(self.dataset_ids):
+                products = list(existing_prods.values())
+                LOG.debug("pre-existing products {}".format(repr(products)))
+                yield from products
+                return
 
         from uuid import uuid1
-        scn = self.load_all_datasets()
-        for ds_id in scn.keys():
-            ds = scn[ds_id]
+
+        self._load_all_datasets()
+        revised_datasets = self._revise_all_datasets()
+        for ds_id, ds in revised_datasets.items():
             # don't recreate a Product for one we already have
             if ds_id in existing_ids:
                 yield existing_ids[ds_id]
                 continue
 
             meta = ds.attrs
             uuid = uuid1()
             meta[Info.UUID] = uuid
-            meta['_satpy_id'] = ds_id
+            meta["_satpy_id"] = ds_id
             now = datetime.utcnow()
             prod = Product(
                 uuid_str=str(uuid),
                 atime=now,
             )
             prod.resource.extend(resources)
 
-            assert (Info.OBS_TIME in meta)
-            assert (Info.OBS_DURATION in meta)
+            assert Info.OBS_TIME in meta  # nosec B101
+            assert Info.OBS_DURATION in meta  # nosec B101
             prod.update(meta)  # sets fields like obs_duration and obs_time transparently
-            assert (prod.info[Info.OBS_TIME] is not None and prod.obs_time is not None)
-            assert (prod.info[Info.VALID_RANGE] is not None)
-            LOG.debug('new product: {}'.format(repr(prod)))
-            self._S.add(prod)
-            self._S.commit()
+            assert prod.info[Info.OBS_TIME] is not None and prod.obs_time is not None  # nosec B101
+            LOG.debug("new product: {}".format(repr(prod)))
+            if self.use_inventory_db:
+                self._S.add(prod)
+                self._S.commit()
             yield prod
 
+    def _extract_segment_number(self):
+        """
+        Load the segment numbers loaded by this scene.
+        :return: list of all the segment numbers of all segments loaded by the scene
+        """
+        segments = []
+        collected_segment_count = 0
+        for reader in self.scn._readers.values():
+            for file_handlers in reader.file_handlers.values():
+                collected_segment_count += 1
+                for file_handler in file_handlers:
+                    if file_handler.filetype_info["file_type"] in self.required_aux_file_types:
+                        collected_segment_count -= 1
+                        continue
+                    seg = file_handler.filename_info["segment"]
+                    segments.append(seg)
+
+        filtered_segments = []
+        for segment in set(segments):
+            if segments.count(segment) == collected_segment_count:
+                filtered_segments.append(segment)
+
+        return sorted(filtered_segments)
+
+    def _extract_expected_segments(self) -> Optional[int]:
+        """
+        Load the number of expected segments for the products loaded by the given scene.
+        :return: number of expected segments or None if not found or products in scene
+        have different numbers of expected segments
+        """
+        expected_segments = None
+        for reader in self.scn._readers.values():
+            for file_handlers in reader.file_handlers.values():
+                for file_handler in file_handlers:
+                    if file_handler.filetype_info["file_type"] in self.required_aux_file_types:
+                        continue
+                    es = file_handler.filetype_info["expected_segments"]
+                    if expected_segments is None:
+                        expected_segments = es
+                    if expected_segments != es:
+                        return None
+        return expected_segments
+
     @property
     def num_products(self) -> int:
         # WARNING: This could provide radiances and higher level products
         #          which SIFT probably shouldn't care about
         return len(self.dataset_ids)
 
     @staticmethod
     def _get_platform_instrument(attrs: dict):
-        """Convert SatPy platform_name/sensor to """
-        attrs[Info.INSTRUMENT] = attrs.get('sensor')
-        attrs[Info.PLATFORM] = attrs.get('platform_name') or attrs.get('platform_shortname')
+        """Convert SatPy platform_name/sensor to"""
+        attrs[Info.INSTRUMENT] = attrs.get("sensor")
+        attrs[Info.PLATFORM] = attrs.get("platform_name") or attrs.get("platform_shortname")
 
         # Special handling of GRIB forecast data
-        if 'centreDescription' in attrs and \
-                attrs[Info.INSTRUMENT] == 'unknown':
-            description = attrs['centreDescription']
+        if "centreDescription" in attrs and attrs[Info.INSTRUMENT] == "unknown":
+            description = attrs["centreDescription"]
             if attrs.get(Info.PLATFORM) is None:
-                attrs[Info.PLATFORM] = 'NWP'
-            if 'NCEP' in description:
-                attrs[Info.INSTRUMENT] = 'GFS'
-        if attrs[Info.INSTRUMENT] in ['GFS', 'unknown']:
+                attrs[Info.PLATFORM] = "NWP"
+            if "NCEP" in description:
+                attrs[Info.INSTRUMENT] = "GFS"
+        if attrs[Info.INSTRUMENT] in ["GFS", "unknown"]:
             attrs[Info.INSTRUMENT] = Instrument.GFS
-        if attrs[Info.PLATFORM] in ['NWP', 'unknown']:
+        if attrs[Info.PLATFORM] in ["NWP", "unknown"]:
             attrs[Info.PLATFORM] = Platform.NWP
 
         # FUTURE: Use standard string names for platform/instrument
         #         instead of an Enum. Otherwise, could use a reverse
         #         Enum lookup to match Enum values to Enum keys.
         # if we haven't figured out what these are then give up and say they are unknown
         if isinstance(attrs[Info.PLATFORM], str):
-            plat_str = attrs[Info.PLATFORM].lower().replace('-', '')
+            plat_str = attrs[Info.PLATFORM].lower().replace("-", "")
             attrs[Info.PLATFORM] = PLATFORM_MAP.get(plat_str, attrs[Info.PLATFORM])
         if not attrs[Info.PLATFORM] or isinstance(attrs[Info.PLATFORM], str):
             attrs[Info.PLATFORM] = Platform.UNKNOWN
 
         if isinstance(attrs[Info.INSTRUMENT], str):
-            inst_str = attrs[Info.INSTRUMENT].lower().replace('-', '')
+            inst_str = attrs[Info.INSTRUMENT].lower().replace("-", "")
             attrs[Info.INSTRUMENT] = INSTRUMENT_MAP.get(inst_str, attrs[Info.INSTRUMENT])
         if not attrs[Info.INSTRUMENT] or isinstance(attrs[Info.INSTRUMENT], str):
             attrs[Info.INSTRUMENT] = Instrument.UNKNOWN
 
-    def load_all_datasets(self) -> Scene:
-        self.scn.load(self.dataset_ids, **self.product_filters)
+    def _load_all_datasets(self) -> None:
+        self.scn.load(self.dataset_ids, pad_data=False, upper_right_corner="NE", **self.product_filters)
         # copy satpy metadata keys to SIFT keys
+
+        if self.scn.missing_datasets:
+            # deactivating reduce_data, see https://github.com/pytroll/satpy/issues/2476
+            self.scn = self.scn.resample(resampler="native", reduce_data=False)
+
         for ds in self.scn:
-            start_time = ds.attrs['start_time']
-            id_str = ":".join(str(v[1]) for v in get_id_items(id_from_attrs(ds.attrs)))
-            ds.attrs[Info.DATASET_NAME] = id_str
-            ds.attrs[Info.OBS_TIME] = start_time
-            ds.attrs[Info.SCHED_TIME] = start_time
-            duration = ds.attrs.get('end_time', start_time) - start_time
-            if duration.total_seconds() <= 0:
-                duration = timedelta(minutes=60)
-            ds.attrs[Info.OBS_DURATION] = duration
-
-            # Handle GRIB platform/instrument
-            ds.attrs[Info.KIND] = Kind.IMAGE if self.reader != 'grib' else \
-                Kind.CONTOUR
-            self._get_platform_instrument(ds.attrs)
-            ds.attrs.setdefault(Info.STANDARD_NAME, ds.attrs.get('standard_name'))
-            if 'wavelength' in ds.attrs:
-                ds.attrs.setdefault(Info.CENTRAL_WAVELENGTH,
-                                    ds.attrs['wavelength'][1])
-
-            # Resolve anything else needed by SIFT
-            model_time = ds.attrs.get('model_time')
-            if model_time is not None:
-                ds.attrs[Info.DATASET_NAME] += " " + model_time.isoformat()
-            ds.attrs[Info.SHORT_NAME] = ds.attrs['name']
-            if ds.attrs.get('level') is not None:
-                ds.attrs[Info.SHORT_NAME] = "{} @ {}hPa".format(
-                    ds.attrs['name'], ds.attrs['level'])
-            ds.attrs[Info.SHAPE] = ds.shape
-            ds.attrs[Info.UNITS] = ds.attrs.get('units')
-            if ds.attrs[Info.UNITS] == 'unknown':
-                LOG.warning("Layer units are unknown, using '1'")
-                ds.attrs[Info.UNITS] = 1
-            generate_guidebook_metadata(ds.attrs)
-
-            # Generate FAMILY and CATEGORY
-            if 'model_time' in ds.attrs:
-                model_time = ds.attrs['model_time'].isoformat()
-            else:
-                model_time = None
-            ds.attrs[Info.SCENE] = ds.attrs.get('scene_id')
-            if ds.attrs[Info.SCENE] is None:
-                # compute a "good enough" hash for this Scene
-                area = ds.attrs['area']
-                extents = area.area_extent
-                # round extents to nearest 100 meters
-                extents = tuple(int(np.round(x / 100.0) * 100.0) for x in extents)
-                proj_str = area.proj4_string
-                ds.attrs[Info.SCENE] = "{}-{}".format(str(extents), proj_str)
-            if ds.attrs.get(Info.CENTRAL_WAVELENGTH) is None:
-                cw = ""
-            else:
-                cw = ":{:5.2f}m".format(ds.attrs[Info.CENTRAL_WAVELENGTH])
-            ds.attrs[Info.FAMILY] = '{}:{}:{}{}'.format(
-                ds.attrs[Info.KIND].name, ds.attrs[Info.STANDARD_NAME],
-                ds.attrs[Info.SHORT_NAME], cw)
-            ds.attrs[Info.CATEGORY] = 'SatPy:{}:{}:{}'.format(
-                ds.attrs[Info.PLATFORM].name, ds.attrs[Info.INSTRUMENT].name,
-                ds.attrs[Info.SCENE])  # system:platform:instrument:target
-            # TODO: Include level or something else in addition to time?
-            start_str = ds.attrs['start_time'].isoformat()
-            ds.attrs[Info.SERIAL] = start_str if model_time is None else model_time + ":" + start_str
-            ds.attrs.setdefault('reader', self.reader)
+            self._set_name_metadata(ds.attrs)
+            self._set_time_metadata(ds.attrs)
+            self._set_kind_metadata(ds.attrs)
+            self._set_wavelength_metadata(ds.attrs)
+            self._set_shape_metadata(ds.attrs, ds.shape)
+            self._set_scene_metadata(ds.attrs)
+            self._set_family_metadata(ds.attrs)
+            self._set_category_metadata(ds.attrs)
+            self._set_serial_metadata(ds.attrs)
+            ds.attrs.setdefault("reader", self.reader)
+
+    @staticmethod
+    def _set_name_metadata(
+        attrs: dict, name: Optional[str] = None, short_name: Optional[str] = None, long_name: Optional[str] = None
+    ) -> None:
+        if not name:
+            name = attrs["name"] or attrs.get(Info.STANDARD_NAME)
+
+        id_str = ":".join(str(v[1]) for v in get_id_items(id_from_attrs(attrs)))
+        attrs[Info.DATASET_NAME] = id_str
+
+        model_time = attrs.get("model_time")
+        if model_time is not None:
+            attrs[Info.DATASET_NAME] += " " + model_time.isoformat()
+
+        level = attrs.get("level")
+        if level is None:
+            attrs[Info.SHORT_NAME] = short_name or name
+        else:
+            attrs[Info.SHORT_NAME] = f"{short_name or name} @ {level}hPa"
+
+        attrs[Info.LONG_NAME] = long_name or name
+        attrs[Info.STANDARD_NAME] = attrs.get("standard_name") or name
+
+    @staticmethod
+    def _determine_sched_time(attrs: dict):
+        if "time_parameters" in attrs:
+            return attrs["time_parameters"]["nominal_start_time"]
+        if "nominal_start_time" in attrs:
+            return attrs["nominal_start_time"]
+        return attrs["start_time"]
+
+    @staticmethod
+    def _set_time_metadata(attrs: dict) -> None:
+        # TODO Review Satpy start/end times ("[{nominal,observation]_}{start,end}_times") and adjust SIFT
+        #  terminology regarding start/end times and durations accordingly.
+        start_time = attrs["start_time"]
+        attrs[Info.OBS_TIME] = start_time
+        attrs[Info.SCHED_TIME] = SatpyImporter._determine_sched_time(attrs)
+        duration = attrs.get("end_time", start_time) - start_time
+        if duration.total_seconds() <= 0:
+            duration = timedelta(minutes=60)
+        attrs[Info.OBS_DURATION] = duration
+
+    def _set_kind_metadata(self, attrs: dict) -> None:
+        if "prerequisites" in attrs:
+            attrs[Info.KIND] = Kind.MC_IMAGE
+            return
+
+        reader_kind = config.get(f"data_reading.{self.reader}.kind", None)
+
+        if reader_kind is None:
+            LOG.info(f"No data kind configured for reader '{self.reader}'. Falling back to 'IMAGE'.")
+            attrs[Info.KIND] = Kind.IMAGE
+        else:
+            set_kind_metadata_from_reader_config(self.reader, reader_kind, attrs)
+
+    def _set_wavelength_metadata(self, attrs: dict) -> None:
+        self._get_platform_instrument(attrs)
+        if "wavelength" in attrs and attrs.get("wavelength"):
+            attrs.setdefault(Info.CENTRAL_WAVELENGTH, attrs["wavelength"][1])
+
+    def _set_shape_metadata(self, attrs: dict, shape) -> None:
+        if len(shape) == 3 and "prerequisites" in attrs.keys():
+            shape = (shape[1], shape[2], shape[0])
+        attrs[Info.SHAPE] = shape if not self.resampling_info else self.resampling_info["shape"]
+        attrs[Info.UNITS] = attrs.get("units")
+        if attrs[Info.UNITS] == "unknown":
+            LOG.warning("Dataset units are unknown, using '1'")
+            attrs[Info.UNITS] = 1
+        generate_guidebook_metadata(attrs)
+
+    def _set_scene_metadata(self, attrs: dict) -> None:
+        attrs[Info.SCENE] = attrs.get("scene_id")
+        if attrs[Info.SCENE] is None:
+            self._compute_scene_hash(attrs)
+
+    @staticmethod
+    def _set_family_metadata(attrs: dict) -> None:
+        if attrs.get(Info.CENTRAL_WAVELENGTH) is None:
+            cw = ""
+        else:
+            cw = ":{:5.2f}m".format(attrs[Info.CENTRAL_WAVELENGTH])
+        attrs[Info.FAMILY] = "{}:{}:{}{}".format(
+            attrs[Info.KIND].name, attrs[Info.STANDARD_NAME], attrs[Info.SHORT_NAME], cw
+        )
+
+    @staticmethod
+    def _set_category_metadata(attrs: dict) -> None:
+        # system:platform:instrument:target
+        attrs[Info.CATEGORY] = "SatPy:{}:{}:{}".format(
+            attrs[Info.PLATFORM].name, attrs[Info.INSTRUMENT].name, attrs[Info.SCENE]
+        )
+
+    @staticmethod
+    def _set_serial_metadata(attrs: dict) -> None:
+        # TODO: Include level or something else in addition to time?
+        start_str = attrs["start_time"].isoformat()
+        if "model_time" in attrs:
+            model_time = attrs["model_time"].isoformat()
+            attrs[Info.SERIAL] = f"{model_time}:{start_str}"
+        else:
+            attrs[Info.SERIAL] = start_str
+
+    @staticmethod
+    def _get_area_extent(area: Union[AreaDefinition, StackedAreaDefinition]) -> List:
+        """
+        Return the area extent of the AreaDefinition or of the nominal
+        AreaDefinition of the StackedAreaDefinition. The nominal AreaDefinition
+        is the one referenced via 'area_id' by all AreaDefinitions listed in the
+        StackedAreaDefinition. If there are different 'area_id's in the
+        StackedAreaDefinition this function fails throwing an exception.
+
+        Use case: Get the extent of the full-disk to which a non-contiguous set
+        of segments loaded from a segmented GEOS file format (e.g. HRIT) belongs.
+
+        :param area: AreaDefinition ar StackedAreaDefinition
+        :return: Extent of the nominal AreaDefinition.
+        :raises ValueError: when a StackedAreaDefinition with incompatible
+        AreaDefinitions is passed
+        """
+        if isinstance(area, StackedAreaDefinition):
+            any_area_def = area.defs[0]
+            for area_def in area.defs:
+                if area_def.area_id != any_area_def.area_id:
+                    raise ValueError(f"Different area_ids found in StackedAreaDefinition: {area}.")
+            area = satpy.resample.get_area_def(any_area_def.area_id)
+        return area.area_extent
+
+    def _compute_scene_hash(self, attrs: dict):
+        """Compute a "good enough" hash and store it as
+        SCENE information.
+
+        The SCENE information is used at other locations to identify data that
+        has roughly the same extent and projection. That is a  pre-requisite
+        to allow to derive algebraics and compositions from them.
+
+        Unstructured data has no clear extent and not an intrinsic projection
+        (data locations are in latitude / longitude), thus something like a
+        SCENE (in the sense of describing a view on a section of the earth's
+        surface) cannot clearly be determined for it.
+        """
+        try:
+            area = (
+                attrs["area"]
+                if not self.resampling_info
+                else AreaDefinitionsManager.area_def_by_id(self.resampling_info["area_id"])
+            )
+            # round extents to nearest 100 meters
+            extents = tuple(int(np.round(x / 100.0) * 100.0) for x in self._get_area_extent(area))
+            attrs[Info.SCENE] = "{}-{}".format(str(extents), area.proj_str)
+        except (KeyError, AttributeError):
+            # Scattered data, this is not suitable to define a scene
+            attrs[Info.SCENE] = None
+
+    def _stack_data_arrays(
+        self, datasets: List[DataArray], attrs: dict, name_prefix: str = "", axis: int = 1
+    ) -> DataArray:
+        """
+        Merge multiple DataArrays into a single ``DataArray``. Use the
+        ``attrs`` dict for the DataArray metadata. This method also copies
+        the Satpy metadata fields into SIFT fields. The ``attrs`` dict won't
+        be modified.
+
+        :param datasets: List of DataArrays
+        :param attrs: metadata for the resulting DataArray
+        :param name_prefix: if given, a prefix for the name of the new DataArray
+        :param axis: numpy axis index
+        :return: stacked Dask array
+        """
+        # Workaround for a Dask bug: Convert all DataArrays to float32
+        # before calling into dask, because an int16 DataArray will be
+        # converted into a Series instead of a dask Array with newer
+        # versions. This then causes a TypeError.
+        meta = np.stack([da.utils.meta_from_array(ds) for ds in datasets], axis=axis)
+        datasets = [ds.astype(meta.dtype) for ds in datasets]
+        combined_data = da.stack(datasets, axis=axis)
+
+        attrs = attrs.copy()
+        ds_id = attrs["_satpy_id"]
+        name = f"{name_prefix or ''}{attrs['name']}"
+        attrs["_satpy_id"] = DataID(ds_id.id_keys, name=name)
+        self._set_name_metadata(attrs, name)
+        self._set_time_metadata(attrs)
+        self._set_kind_metadata(attrs)
+        self._set_wavelength_metadata(attrs)
+        self._set_shape_metadata(attrs, combined_data.shape)
+
+        guidebook = get_guidebook_class(attrs)
+        attrs[Info.DISPLAY_NAME] = guidebook._default_display_name(attrs)
+
+        self._set_scene_metadata(attrs)
+        self._set_family_metadata(attrs)
+        self._set_category_metadata(attrs)
+
+        return DataArray(combined_data, attrs=attrs)
+
+    def _parse_style_attributes_config(self) -> Dict[str, Set[str]]:
+        """
+        Extract the ``style_attributes`` section from the reader config.
+        This function doesn't validate whether the style attributes or the
+        product names exist.
+
+        The returned dictionary lists for each product, which dependent styles
+        it is configured for.
+
+        Maintenance: this swaps the mapping direction since the new one is more
+        useful for querying which features (style attributes) should be styled depending
+        on a given product
+
+        :return: mapping of products to style attributes
+        """
+        style_attributes = config.get(f"data_reading.{self.reader}.style_attributes", None)
+        if not style_attributes:
+            return {}
+
+        style_attributes_by_product = {}
+        for style_attribute, product_names in style_attributes.items():
+            for product_name in product_names:
+                if product_name not in style_attributes_by_product:
+                    style_attributes_by_product[product_name] = {style_attribute}
+                else:
+                    style_attributes_by_product[product_name].add(style_attribute)
+
+        return style_attributes_by_product
+
+    def _combine_points(self, datasets: DatasetDict, converter) -> Dict[DataID, DataArray]:
+        # Currently only the "fill" can be colored by a mapping from product values to colors (using a colormap) but in
+        # the future other features could be colored as well, e.g.  like "stroke".
+        supported_color_by_style_attrs = {"fill"}
+        style_attributes_by_product = self._parse_style_attributes_config()
+
+        converted_datasets = {}
+        for ds_id, ds in datasets.items():
+            if ds.attrs[Info.KIND] != Kind.POINTS:
+                continue
+
+            ds_name = ds_id["name"]
+            do_color_by_values = ds_name in style_attributes_by_product and bool(
+                style_attributes_by_product[ds_name] & supported_color_by_style_attrs
+            )
+            try:
+                # Only if we want to use the values of the data to color
+                # markers we need the dataset's array itself, otherwise the
+                # coordinates alone are sufficient.
+                convertible_ds = (
+                    [ds.area.lons, ds.area.lats, ds] if do_color_by_values else [ds.area.lons, ds.area.lats]
+                )
+            except AttributeError:
+                # Some products (e.g. `latitude` and `longitude`) may not
+                # (for whatever reason) have an associated SwathDefinition.
+                # Without it, there is no geolocation information per data
+                # point, because it is taken from its fields 'lats', 'lons'.
+                # This cannot be healed, point data loading fails.
+                LOG.error(
+                    f"Dataset '{ds.attrs['name']}' of kind POINTS has no point"
+                    f" coordinates (lats, lons) (Missing SwathDefinition)."
+                )
+                continue
+            converted_datasets[ds_id] = converter(convertible_ds, ds.attrs)
+        return converted_datasets
+
+    def _parse_coords_end_config(self) -> List[str]:
+        """
+        Parse the ``coordinates_end`` section of the reader config.
+
+        :return: List of ``coords`` identifiers
+        """
+        coords_end = config.get(f"data_reading.{self.reader}.coordinates_end", None)
+        if not coords_end:
+            return []
+
+        if len(coords_end) < 2 or len(coords_end) > 2:
+            LOG.warning("expected 2 end coordinates for LINES")
+        return coords_end
+
+    def _combine_lines(self, datasets: DatasetDict, converter) -> Dict[DataID, DataArray]:
+        """
+        Find convertible LINES datasets in the ``DatasetDict``. The ``converter``
+        function is then used to generate new DataArrays.
+
+        The positions for the tip and base of the lines are extracted from the
+        dataset metadata. The dataset itself will be discarded.
+
+        :param datasets: all loaded datasets and previously converted datasets
+        :param converter: function to convert a list of DataArrays
+        :return: mapping of converted DataID to new DataArray
+        """
+        coords_end = self._parse_coords_end_config()
+
+        converted_datasets = {}
+        for ds_id, ds in datasets.items():
+            if ds.attrs[Info.KIND] != Kind.LINES:
+                continue
+
+            convertible_ds = []
+            try:
+                for coord in coords_end:
+                    convertible_ds.append(ds.coords[coord])  # base
+            except KeyError:
+                LOG.error(f"dataset has no coordinates: {ds.attrs['name']}")
+                continue
+            if len(convertible_ds) < 2:
+                LOG.error(f"LINES dataset needs 4 coordinates: {ds.attrs['name']}")
+                continue
+
+            convertible_ds.extend([ds.area.lons, ds.area.lats])  # tip
+            converted_datasets[ds_id] = converter(convertible_ds, ds.attrs)
+        return converted_datasets
+
+    def _revise_all_datasets(self) -> DatasetDict:
+        """
+        Revise all datasets and convert the data representation of the POINTS
+        and LINES records found to the data format VisPy needs to display them
+        as such.
+
+        The original datasets are not kept in the scene but replaced by the
+        converted datasets.
+
+        :return: ``DatasetDict`` with the loaded and converted datasets
+        """
+        loaded_datasets = DatasetDict()
+        for ds_id in self.scn.keys():
+            ds = self.scn[ds_id]
+            loaded_datasets[ds_id] = ds
+
+        # Note: If you do not want the converted data sets to overwrite the
+        # original data sets in a future development of this software,
+        # you can pass a suitable prefix to self._stack_data_arrays() (which
+        # will be used to create a new name for the converted data set) by
+        # defining the appropriate converters, for example, as follows:
+        #   self._combine_points: lambda *args: self._stack_data_arrays(*args, "POINTS-"),
+        converters = {
+            self._combine_points: lambda *args: self._stack_data_arrays(*args),
+            self._combine_lines: lambda *args: self._stack_data_arrays(*args),
+        }
 
-        return self.scn
+        for detector, converter in converters.items():
+            converted_datasets = detector(loaded_datasets, converter)
+            for old_ds_id, new_ds in converted_datasets.items():
+                del loaded_datasets[old_ds_id]
+
+                new_ds_id = DataID.from_dataarray(new_ds)
+                loaded_datasets[new_ds_id] = new_ds
+
+        for ds_id, ds in loaded_datasets.items():
+            self.scn[ds_id] = ds
+        return loaded_datasets
 
-    def _area_to_sift_attrs(self, area):
+    @staticmethod
+    def _area_to_sift_attrs(area):
         """Area to uwsift keys"""
-        from pyresample.geometry import AreaDefinition
         if not isinstance(area, AreaDefinition):
-            raise NotImplementedError("Only AreaDefinition datasets can "
-                                      "be loaded at this time.")
-
-        half_pixel_x = abs(area.pixel_size_x) / 2.
-        half_pixel_y = abs(area.pixel_size_y) / 2.
+            raise NotImplementedError(
+                "Only AreaDefinition datasets can be loaded at this time. "
+                "If you're trying to read data that needs resampling, like LEO data, remember "
+                "to select a resampling method in the Open File Wizard, and set "
+                "'geometry_definition: SwathDefinition' in your reader config."
+            )
 
         return {
-            Info.PROJ: area.proj4_string,
-            Info.ORIGIN_X: area.area_extent[0] + half_pixel_x,
-            Info.ORIGIN_Y: area.area_extent[3] - half_pixel_y,
-            Info.CELL_HEIGHT: -abs(area.pixel_size_y),
+            Info.PROJ: area.proj_str,
+            Info.ORIGIN_X: area.area_extent[0],  # == lower_(left_)x
+            Info.ORIGIN_Y: area.area_extent[3],  # == upper_(right_)y
             Info.CELL_WIDTH: area.pixel_size_x,
+            Info.CELL_HEIGHT: -area.pixel_size_y,
         }
 
-    def begin_import_products(self, *product_ids) -> Generator[import_progress, None, None]:
-        import dask.array as da
+    def _get_grid_info(self):
+        grid_origin = config.get(f"data_reading.{self.reader}.grid.origin", "NW")
+        grid_first_index_x = config.get(f"data_reading.{self.reader}.grid.first_index_x", 0)
+        grid_first_index_y = config.get(f"data_reading.{self.reader}.grid.first_index_y", 0)
 
-        if product_ids:
-            products = [self._S.query(Product).filter_by(id=anid).one() for anid in product_ids]
-            assert products
+        return {
+            Info.GRID_ORIGIN: grid_origin,
+            Info.GRID_FIRST_INDEX_X: grid_first_index_x,
+            Info.GRID_FIRST_INDEX_Y: grid_first_index_y,
+        }
+
+    def begin_import_products(self, *product_ids) -> Generator[import_progress, None, None]:  # noqa: C901
+        if self.use_inventory_db:
+            products = self._get_products_from_inventory_db(product_ids)
         else:
-            products = list(self._S.query(Resource, Product).filter(
-                Resource.path.in_(self.filenames)).filter(
-                Product.resource_id == Resource.id).all())
-            assert products
+            products = product_ids
+
+        merge_with_existing = self.merge_target is not None
 
         # FIXME: Don't recreate the importer every time we want to load data
-        dataset_ids = [prod.info['_satpy_id'] for prod in products]
-        self.scn.load(dataset_ids)
+        dataset_ids = [prod.info["_satpy_id"] for prod in products]
+        self.scn.load(dataset_ids, pad_data=not merge_with_existing, upper_right_corner="NE")
+
+        # If there are datasets missing, resampling is needed to create them:
+        if self.scn.missing_datasets:
+            # About the next strange line of code: keep a reference to the
+            # original scene to work around an issue in the resampling
+            # implementation for NetCDF data: otherwise the original data
+            # would be garbage collected too early.
+            self.scn_original = self.scn  # noqa - Do not simply remove
+
+            # deactivating reduce_data, see https://github.com/pytroll/satpy/issues/2476
+            self.scn = self.scn.resample(resampler="native", reduce_data=False)
+
+        if self.resampling_info:
+            self._preprocess_products_with_resampling()
+
         num_stages = len(products)
         for idx, (prod, ds_id) in enumerate(zip(products, dataset_ids)):
-            dataset = self.scn[ds_id]
-            shape = dataset.shape
-            num_contents = 1 if prod.info[Info.KIND] == Kind.IMAGE else 2
+            dataset = (
+                self.scn[ds_id] if prod.info[Info.KIND] != Kind.MC_IMAGE else get_enhanced_image(self.scn[ds_id]).data
+            )
+            if prod.info[Info.KIND] == Kind.MC_IMAGE:
+                # The dimension of the dataset is ('bands', 'y', 'x')
+                # but for later the dimension ('y', 'x', 'bands') is needed
+                dataset = dataset.transpose("y", "x", "bands")
+            # Since in the first SatpyImporter loading pass (see
+            # load_all_datasets()) no padding is applied (pad_data=False), the
+            # Info.SHAPE stored at that time may not be the same as it is now if
+            # we load with padding now. In that case we must update the
+            # prod.info[Info.SHAPE] with the actual shape, but let's do this in
+            # any case, it doesn't hurt.
+            prod.info[Info.SHAPE] = dataset.shape
+            kind = prod.info[Info.KIND]
 
             if prod.content:
-                LOG.warning('content was already available, skipping import')
+                LOG.warning("content was already available, skipping import")
                 continue
 
             now = datetime.utcnow()
-            area_info = self._area_to_sift_attrs(dataset.attrs['area'])
-            cell_width = area_info[Info.CELL_WIDTH]
-            cell_height = area_info[Info.CELL_HEIGHT]
-            proj4 = area_info[Info.PROJ]
-            origin_x = area_info[Info.ORIGIN_X]
-            origin_y = area_info[Info.ORIGIN_Y]
-            data = dataset.data
-
-            # Handle building contours for data from 0 to 360 longitude
-            # our map's antimeridian is 180
-            antimeridian = 179.999
-            # find out if there is data beyond 180 in this data
-            if '+proj=latlong' not in proj4:
-                # the x coordinate for the antimeridian in this projection
-                am = Proj(proj4)(antimeridian, 0)[0]
-                if am >= 1e30:
-                    am_index = -1
-                else:
-                    am_index = int(np.ceil((am - origin_x) / cell_width))
+
+            if kind in [Kind.LINES, Kind.POINTS]:
+                if len(dataset.shape) == 1:
+                    LOG.error(f"one dimensional dataset can't be loaded: {ds_id['name']}")
+                    continue
+
+                completion, content, data_memmap = self._create_unstructured_points_dataset_content(dataset, now, prod)
+                yield import_progress(
+                    uuid=prod.uuid,
+                    stages=num_stages,
+                    current_stage=idx,
+                    completion=completion,
+                    stage_desc=f"SatPy {kind.name} data add to workspace",
+                    dataset_info=None,
+                    data=data_memmap,
+                    content=content,
+                )
+                continue
+
+            uuid = prod.uuid
+
+            if merge_with_existing:
+                existing_product = self.merge_target
+                segments = self._extract_segment_number()
+                uuid = existing_product.uuid
+                c = existing_product.content[0]
+                img_data = c.img_data
+                self.merge_data_into_memmap(dataset.data, img_data, segments)
             else:
-                am_index = int(np.ceil((antimeridian - origin_x) / cell_width))
-            # if there is data beyond 180, let's copy it to the beginning of the
-            # array so it shows up in the primary -180/0 portion of the SIFT map
-            if prod.info[Info.KIND] == Kind.CONTOUR and 0 < am_index < shape[1]:
-                # Previous implementation:
-                # Prepend a copy of the last half of the data (180 to 360 -> -180 to 0)
-                # data = da.concatenate((data[:, am_index:], data), axis=1)
-                # adjust X origin to be -180
-                # origin_x -= (shape[1] - am_index) * cell_width
-                # The above no longer works with newer PROJ because +over is deprecated
-                #
-                # New implementation:
-                # Swap the 180 to 360 portion to be -180 to 0
-                # if we have data from 0 to 360 longitude, we want -180 to 360
-                data = da.concatenate((data[:, am_index:], data[:, :am_index]), axis=1)
-                # remove the custom 180 prime meridian in the projection
-                proj4 = proj4.replace("+pm=180 ", "")
-                area_info[Info.PROJ] = proj4
-
-            # shovel that data into the memmap incrementally
-            data_filename = '{}.image'.format(prod.uuid)
-            data_path = os.path.join(self._cwd, data_filename)
-            img_data = np.memmap(data_path, dtype=np.float32, shape=shape, mode='w+')
-            da.store(data, img_data)
-
-            c = Content(
-                lod=0,
-                resolution=int(min(abs(cell_width), abs(cell_height))),
-                atime=now,
-                mtime=now,
+                area_info = self._area_to_sift_attrs(dataset.attrs["area"])
+                grid_info = self._get_grid_info()
+
+                if kind == Kind.MC_IMAGE:
+                    c, img_data = self._create_mc_image_dataset_content(dataset, now, prod, area_info, grid_info)
+                else:
+                    c, img_data = self._create_image_dataset_content(dataset, now, prod, area_info, grid_info)
 
-                # info about the data array memmap
-                path=data_filename,
-                rows=shape[0],
-                cols=shape[1],
-                proj4=proj4,
-                # levels = 0,
-                dtype='float32',
-
-                # info about the coverage array memmap, which in our case just tells what rows are ready
-                # coverage_rows = rows,
-                # coverage_cols = 1,
-                # coverage_path = coverage_filename
-
-                cell_width=cell_width,
-                cell_height=cell_height,
-                origin_x=origin_x,
-                origin_y=origin_y,
+                c.info[Info.KIND] = prod.info[Info.KIND]
+                c.img_data = img_data
+                c.source_files = set()  # TODO(AR): adds member to the ContentImage object 'c'
+                # c.info.update(prod.info) would just make everything leak together so let's not do it
+                prod.content.append(c)
+                self._add_content_to_cache(c)
+
+            required_files = _get_paths_of_required_aux_files(self.scn, self.required_aux_file_types)
+            # Note loaded source files but not required files. This is necessary
+            # because the merger will try to not load already loaded files again
+            # but might need to reload required files.
+            c.source_files |= set(self.filenames) - required_files
+
+            yield import_progress(
+                uuid=uuid,
+                stages=num_stages,
+                current_stage=idx,
+                completion=1.0,
+                stage_desc=f"SatPy {kind.name} data add to workspace",
+                dataset_info=None,
+                data=img_data,
+                content=c,
             )
-            c.info[Info.KIND] = Kind.IMAGE
-            # c.info.update(prod.info) would just make everything leak together so let's not do it
-            self._S.add(c)
-            prod.content.append(c)
-            # prod.touch()
-            self._S.commit()
 
-            yield import_progress(uuid=prod.uuid,
-                                  stages=num_stages,
-                                  current_stage=idx,
-                                  completion=1. / num_contents,
-                                  stage_desc="SatPy image data add to workspace",
-                                  dataset_info=None,
-                                  data=img_data)
+    def _get_products_from_inventory_db(self, product_ids):
+        if product_ids:
+            products = [self._S.query(Product).filter_by(id=anid).one() for anid in product_ids]
+            assert products  # nosec B101
+        else:
+            products = list(
+                self._S.query(Resource, Product)
+                .filter(Resource.path.in_(self.filenames))
+                .filter(Product.resource_id == Resource.id)
+                .all()
+            )
+            assert products  # nosec B101
+        return products
 
-            if num_contents == 1:
-                continue
+    def _create_mc_image_dataset_content(self, dataset, now, prod, area_info, grid_info):
+        data_filename, img_data = self._create_data_memmap_file(dataset.data, dataset.data.dtype, prod)
+        shape = prod.info[Info.SHAPE]
+        c = ContentMultiChannelImage(
+            lod=0,
+            resolution=int(min(abs(area_info[Info.CELL_WIDTH]), abs(area_info[Info.CELL_HEIGHT]))),
+            atime=now,
+            mtime=now,
+            # info about the data array memmap
+            path=data_filename,
+            rows=shape[0],
+            cols=shape[1],
+            bands=shape[2],
+            proj4=area_info[Info.PROJ],
+            dtype=str(dataset.dtype),
+            cell_width=area_info[Info.CELL_WIDTH],
+            cell_height=area_info[Info.CELL_HEIGHT],
+            origin_x=area_info[Info.ORIGIN_X],
+            origin_y=area_info[Info.ORIGIN_Y],
+            grid_origin=grid_info[Info.GRID_ORIGIN],
+            grid_first_index_x=grid_info[Info.GRID_FIRST_INDEX_X],
+            grid_first_index_y=grid_info[Info.GRID_FIRST_INDEX_Y],
+        )
+        return c, img_data
 
-            if find_contours is None:
-                raise RuntimeError("Can't create contours without 'skimage' "
-                                   "package installed.")
-
-            # XXX: Should/could 'lod' be used for different contour level data?
-            levels = [x for y in prod.info['contour_levels'] for x in y]
-            data_filename = '{}.contour'.format(prod.uuid)
-            data_path = os.path.join(self._cwd, data_filename)
-            try:
-                contour_data = self._compute_contours(
-                    img_data, prod.info[Info.VALID_RANGE][0],
-                    prod.info[Info.VALID_RANGE][1], levels)
-            except ValueError:
-                LOG.warning("Could not compute contour levels for '{}'".format(prod.uuid))
-                LOG.debug("Contour error: ", exc_info=True)
-                continue
+    def _create_image_dataset_content(self, dataset, now, prod, area_info, grid_info):
+        # For kind IMAGE the dtype must be float32 seemingly, see class
+        # Column, comment for 'dtype' and the construction of c = Content
+        # just below.
+        # FIXME: It is dubious to enforce that type conversion to happen in
+        #  _create_data_memmap_file, but otherwise IMAGES of pixel counts
+        #  data (dtype = np.uint16) crash.
+        data_filename, img_data = self._create_data_memmap_file(dataset.data, np.float32, prod)
+        shape = prod.info[Info.SHAPE]
+        c = ContentImage(
+            lod=0,
+            resolution=int(min(abs(area_info[Info.CELL_WIDTH]), abs(area_info[Info.CELL_HEIGHT]))),
+            atime=now,
+            mtime=now,
+            # info about the data array memmap
+            path=data_filename,
+            rows=shape[0],
+            cols=shape[1],
+            proj4=area_info[Info.PROJ],
+            dtype="float32",
+            cell_width=area_info[Info.CELL_WIDTH],
+            cell_height=area_info[Info.CELL_HEIGHT],
+            origin_x=area_info[Info.ORIGIN_X],
+            origin_y=area_info[Info.ORIGIN_Y],
+            grid_origin=grid_info[Info.GRID_ORIGIN],
+            grid_first_index_x=grid_info[Info.GRID_FIRST_INDEX_X],
+            grid_first_index_y=grid_info[Info.GRID_FIRST_INDEX_Y],
+        )
+        return c, img_data
 
-            contour_data[:, 0] *= cell_width
-            contour_data[:, 0] += origin_x
-            contour_data[:, 1] *= cell_height
-            contour_data[:, 1] += origin_y
-            contour_data.tofile(data_path)
-            c = Content(
-                lod=0,
-                resolution=int(min(abs(cell_width), abs(cell_height))),
-                atime=now,
-                mtime=now,
+    def _create_unstructured_points_dataset_content(self, dataset, now, prod):
+        data_filename, data_memmap = self._create_data_memmap_file(dataset.data, dataset.dtype, prod)
+        shape = prod.info[Info.SHAPE]
+        content = ContentUnstructuredPoints(
+            atime=now,
+            mtime=now,
+            # info about the data array memmap
+            path=data_filename,
+            n_points=shape[0],
+            n_dimensions=shape[1],
+            dtype=dataset.dtype,
+        )
+        content.info[Info.KIND] = prod.info[Info.KIND]
+        prod.content.append(content)
+        self._add_content_to_cache(content)
+        completion = 2.0
+        return completion, content, data_memmap
+
+    def _preprocess_products_with_resampling(self) -> None:
+        resampler: str = self.resampling_info["resampler"]
+        max_area = self.scn.finest_area()
+        if isinstance(max_area, AreaDefinition) and max_area.area_id == self.resampling_info["area_id"]:
+            LOG.info(
+                f"Source and target area ID are identical:"
+                f" '{self.resampling_info['area_id']}'."
+                f" Skipping resampling."
+            )
+        else:
+            area_name = max_area.area_id if hasattr(max_area, "area_id") else max_area.name
+            LOG.info(
+                f"Resampling from area ID/name '{area_name}'"
+                f" to area ID '{self.resampling_info['area_id']}'"
+                f" with method '{resampler}'"
+            )
+            target_area_def = AreaDefinitionsManager.area_def_by_id(self.resampling_info["area_id"])
 
-                # info about the data array memmap
-                path=data_filename,
-                rows=contour_data.shape[0],  # number of vertices
-                cols=contour_data.shape[1],  # col (x), row (y), "connect", num_points_for_level
-                proj4=proj4,
-                # levels = 0,
-                dtype='float32',
-
-                cell_width=cell_width,
-                cell_height=cell_height,
-                origin_x=origin_x,
-                origin_y=origin_y,
+            # About the next strange line of code: keep a reference to the
+            # original scene to work around an issue in the resampling
+            # implementation for NetCDF data: otherwise the original data
+            # would be garbage collected too early.
+            if not self.scn_original:
+                self.scn_original = self.scn  # noqa - Do not simply remove
+
+            # deactivating reduce_data, see https://github.com/pytroll/satpy/issues/2476
+            reduce_data = False if resampler == "native" else True
+
+            self.scn = self.scn.resample(
+                target_area_def,
+                resampler=resampler,
+                radius_of_influence=self.resampling_info["radius_of_influence"],
+                reduce_data=reduce_data,
             )
-            c.info[Info.KIND] = Kind.CONTOUR
-            # c.info["level_index"] = level_indexes
+
+    def _get_fci_segment_height(self, segment_number: int, segment_width: int) -> int:
+        try:
+            seg_heights = self.scn._readers[self.reader].segment_heights
+            # we can assume that we're reading only from one filetype (either FDHSI or HRFI)
+            file_type = list(self.scn._readers[self.reader].file_handlers.keys())[0]
+            return seg_heights(file_type, segment_width)[segment_number - 1]
+        except AttributeError:
+            LOG.exception(
+                "You're probably using an old version of Satpy. Please update Satpy to be >=0.39.0 "
+                "to use merge_with_existing=True with the FCI reader."
+            )
+            raise
+
+        return -1
+
+    def _calc_segment_heights(self, segments_data, segments_indices):
+        def get_segment_height_calculator() -> Callable:
+            if self.reader in ["fci_l1c_nc", "fci_l1c_fdhsi"]:
+                return self._get_fci_segment_height
+            else:
+                return lambda x, y: segments_data.shape[0] // len(segments_indices)
+
+        calc_segment_height = get_segment_height_calculator()
+        expected_num_segments = self._extract_expected_segments()
+        segment_heights = np.zeros((expected_num_segments,))
+        for i in range(1, expected_num_segments + 1):
+            segment_heights[i - 1] = calc_segment_height(i, segments_data.shape[1])
+        return segment_heights.astype(int)
+
+    def _determine_segments_to_image_mapping(self, segments_data, segments_indices) -> Tuple[List, List]:
+        segment_heights = self._calc_segment_heights(segments_data, segments_indices)
+
+        segment_starts_stops = []
+        image_starts_stops = []
+        visited_segments: List[int] = []
+        for segment_num in segments_indices:
+            curr_segment_height_idx = segment_num - 1
+            curr_segment_height = segment_heights[curr_segment_height_idx]
+
+            image_stop_pos = segment_heights[curr_segment_height_idx:].sum()
+            image_start_pos = image_stop_pos - curr_segment_height
+
+            curr_segment_stop = segments_data.shape[0]
+            if visited_segments:
+                curr_segment_stop -= segment_heights[visited_segments].sum()
+            curr_segment_start = curr_segment_stop - curr_segment_height
+
+            visited_segments.append(curr_segment_height_idx)
+            segment_starts_stops.append((curr_segment_start, curr_segment_stop))
+            image_starts_stops.append((image_start_pos, image_stop_pos))
+        return segment_starts_stops, image_starts_stops
+
+    def merge_data_into_memmap(self, segments_data, image_data, segments_indices):
+        """
+        Merge new segments from data into img_data. The data is expected to
+        contain the data for all segments with the last segment (that has the
+        largest segment number) as first in the data array.
+        The segments list defines which chunk of data belongs to which segment.
+        The list must be sorted ascending.
+        Those requirements match the current Satpy behavior which loads segments
+        in ascending order and produce a data array with the last segments data
+        first.
+        :param segments_data: the segments data as provided by Satpy importer
+        :param image_data: the dataset data where the segments are merged into
+        :param segments_indices: list of segments whose data is to be merged
+        Note: this is not the highest segment number in the current segments
+        list but the highest segment number which can appear for the product.
+        """
+
+        segment_starts_stops, image_starts_stops = self._determine_segments_to_image_mapping(
+            segments_data, segments_indices
+        )
+
+        for i in range(len(segment_starts_stops)):
+            segment_start = segment_starts_stops[i][0]
+            segment_stop = segment_starts_stops[i][1]
+            image_start = image_starts_stops[i][0]
+            image_stop = image_starts_stops[i][1]
+            image_data[image_start:image_stop, :] = segments_data[segment_start:segment_stop, :]
+
+    def _add_content_to_cache(self, c: Content) -> None:
+        if self.use_inventory_db:
             self._S.add(c)
-            prod.content.append(c)
             self._S.commit()
 
-            completion = 2. / num_contents
-            yield import_progress(uuid=prod.uuid,
-                                  stages=num_stages,
-                                  current_stage=idx,
-                                  completion=completion,
-                                  stage_desc="SatPy contour data add to workspace",
-                                  dataset_info=None,
-                                  data=img_data)
-
-    def _get_verts_and_connect(self, paths):
-        """ retrieve vertices and connects from given paths-list
-        """
-        # THIS METHOD WAS COPIED FROM VISPY
-        verts = np.vstack(paths)
-        gaps = np.add.accumulate(np.array([len(x) for x in paths])) - 1
-        connect = np.ones(gaps[-1], dtype=bool)
-        connect[gaps[:-1]] = False
-        return verts, connect
-
-    def _compute_contours(self, img_data: np.ndarray, vmin: float, vmax: float, levels: list) -> np.ndarray:
-        all_levels = []
-        empty_level = np.array([[0, 0, 0, 0]], dtype=np.float32)
-        for level in levels:
-            if level < vmin or level > vmax:
-                all_levels.append(empty_level)
-                continue
+    def _create_data_memmap_file(self, data: da.array, dtype, prod: Product) -> Tuple[str, Optional[np.memmap]]:
+        """
+        Create *binary* file in the current working directory to cache `data`
+        as numpy memmap. The filename extension of the file is derived from the
+        kind of the given `prod`.
 
-            contours = find_contours(img_data, level, positive_orientation='high')
-            if not contours:
-                LOG.debug("No contours found for level: {}".format(level))
-                all_levels.append(empty_level)
-                continue
-            v, c = self._get_verts_and_connect(contours)
-            # swap row, column to column, row (x, y)
-            v[:, [0, 1]] = v[:, [1, 0]]
-            v += np.array([0.5, 0.5])
-            v[:, 0] = np.where(np.isnan(v[:, 0]), 0, v[:, 0])
-
-            # HACK: Store float vertices, boolean, and index arrays together in one float array
-            this_level = np.empty((v.shape[0],), np.float32)
-            this_level[:] = np.nan
-            this_level[-1] = v.shape[0]
-            c = np.concatenate((c, [False])).astype(np.float32)
-            # level_data = np.concatenate((v, c[:, None]), axis=1)
-            level_data = np.concatenate((v, c[:, None], this_level[:, None]), axis=1)
-            all_levels.append(level_data)
-
-        if not all_levels:
-            raise ValueError("No valid contour levels")
-        return np.concatenate(all_levels).astype(np.float32)
+        'dtype' must be given explicitly because it is not necessarily
+        ``dtype == data.dtype`` (caller decides).
+        """
+        # shovel that data into the memmap incrementally
+
+        kind = prod.info[Info.KIND]
+        data_filename = "{}.{}".format(prod.uuid, kind.name.lower())
+        data_path = os.path.join(self._cwd, data_filename)
+        if data is None or data.size == 0:
+            # For empty data memmap is not possible
+            return data_filename, None
+
+        data_memmap = np.memmap(data_path, dtype=dtype, shape=data.shape, mode="w+")
+        da.store(data, data_memmap)
+
+        return data_filename, data_memmap
```

### Comparing `uwsift-1.2.3/uwsift/workspace/metadatabase.py` & `uwsift-2.0.0b0/uwsift/workspace/metadatabase.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
 """
 metadatabase.py
 ===============
 
 PURPOSE
-SQLAlchemy database tables of metadata used by Workspace to manage its local cache.
+SQLAlchemy database tables of metadata used by CachingWorkspace to manage its local cache.
 
 
 OVERVIEW
 
 ::
 
     Resource : a file containing products, somewhere in the filesystem,
@@ -28,125 +28,108 @@
 REQUIRES
 SQLAlchemy with SQLite
 
 :author: R.K.Garcia <rayg@ssec.wisc.edu>
 :copyright: 2016 by University of Wisconsin Regents, see AUTHORS for more details
 :license: GPLv3, see LICENSE for more details
 """
-__author__ = 'rayg'
-__docformat__ = 'reStructuredText'
 
-import argparse
 import logging
 import os
-import sys
-import unittest
 from collections import defaultdict
 from collections.abc import MutableMapping
 from datetime import datetime
-from functools import reduce
+from typing import Optional
 from uuid import UUID
 
-from sqlalchemy import Table, Column, Integer, String, Unicode, ForeignKey, DateTime, Interval, PickleType, \
-    Float, create_engine
+from sqlalchemy import (
+    Column,
+    DateTime,
+    Float,
+    ForeignKey,
+    Integer,
+    Interval,
+    PickleType,
+    String,
+    Table,
+    Unicode,
+    create_engine,
+)
 from sqlalchemy.ext.associationproxy import association_proxy
-from sqlalchemy.ext.declarative import declarative_base
-from sqlalchemy.orm import Session, relationship, sessionmaker, backref, scoped_session
+from sqlalchemy.orm import Session, backref, relationship, scoped_session, sessionmaker
 from sqlalchemy.orm.collections import attribute_mapped_collection
 
-from uwsift.common import Info, FCS_SEP
+try:
+    from sqlalchemy.orm import declarative_base, declared_attr
+except ImportError:
+    # sqlalchemy <2.0
+    from sqlalchemy.ext import declarative_base, declared_attr
 
-LOG = logging.getLogger(__name__)
-
-#
-# ref   http://docs.sqlalchemy.org/en/latest/_modules/examples/vertical/dictlike.html
-#
-
-# class ProxiedDictMixin(object):
-#     """Adds obj[key] access to a mapped class.
-#
-#     This class basically proxies dictionary access to an attribute
-#     called ``_proxied``.  The class which inherits this class
-#     should have an attribute called ``_proxied`` which points to a dictionary.
-#
-#     """
-#
-#     def __len__(self):
-#         return len(self._proxied)
-#
-#     def __iter__(self):
-#         return iter(self._proxied)
-#
-#     def __getitem__(self, key):
-#         # if key in Info:
-#         #     v = getattr(self, str(key), Info)
-#         #     if v is not Info:
-#         #         return v
-#         return self._proxied[key]
-#
-#     def __contains__(self, key):
-#         return key in self._proxied
-#
-#     def __setitem__(self, key, value):
-#         self._proxied[key] = value
-#
-#     def __delitem__(self, key):
-#         del self._proxied[key]
+from uwsift.common import FCS_SEP, Info
 
+LOG = logging.getLogger(__name__)
 
 # =================
 # Database Entities
 
 Base = declarative_base()
 
 # resources can have multiple products in them
-# products may require multiple resourcse (e.g. separate GEO; tiled imagery)
-PRODUCTS_FROM_RESOURCES_TABLE_NAME = 'product_resource_assoc_v1'
-ProductsFromResources = Table(PRODUCTS_FROM_RESOURCES_TABLE_NAME, Base.metadata,
-                              Column('product_id', Integer, ForeignKey('products_v1.id')),
-                              Column('resource_id', Integer, ForeignKey('resources_v1.id')))
+# products may require multiple resources (e.g. separate GEO; tiled imagery)
+PRODUCTS_FROM_RESOURCES_TABLE_NAME = "product_resource_assoc_v1"
+ProductsFromResources = Table(
+    PRODUCTS_FROM_RESOURCES_TABLE_NAME,
+    Base.metadata,
+    Column("product_id", Integer, ForeignKey("products_v1.id")),
+    Column("resource_id", Integer, ForeignKey("resources_v1.id")),
+)
 
 
-class Resource(Base):
+class Resource(Base):  # type: ignore
     """
     held metadata regarding a file that we can access and import data into the workspace from
     resources are external to the workspace, but the workspace can keep track of them in its database
     """
-    __tablename__ = 'resources_v1'
+
+    __tablename__ = "resources_v1"
     # identity information
     id = Column(Integer, primary_key=True)
 
     # primary handler
     format = Column(PickleType)  # classname, class or callable which can pull this data into workspace from storage
 
     # {scheme}://{path}/{name}?{query}, default is just an absolute path in filesystem
-    scheme = Column(Unicode,
-                    nullable=True)  # uri scheme for the content (the part left of ://), assume file:// by default
+    scheme = Column(
+        Unicode, nullable=True
+    )  # uri scheme for the content (the part left of ://), assume file:// by default
     path = Column(Unicode)  # '/' separated real path
     query = Column(Unicode, nullable=True)  # query portion of a URI or URL, e.g. 'interval=1m&stride=2'
 
     mtime = Column(DateTime)  # last observed mtime of the file, for change checking
     atime = Column(DateTime)  # last time this file was accessed by application
 
     product = relationship("Product", secondary=ProductsFromResources, backref="resource")
 
     @property
     def uri(self):
-        return self.path if (not self.scheme or self.scheme == 'file') else "{}://{}/{}{}".format(
-            self.scheme, self.path, self.name, '' if not self.query else '?' + self.query)
+        return (
+            self.path
+            if (not self.scheme or self.scheme == "file")
+            else "{}://{}/{}{}".format(self.scheme, self.path, self.name, "" if not self.query else "?" + self.query)
+        )
 
     def touch(self, when=None):
         self.atime = datetime.utcnow() if not when else when
 
     def exists(self):
-        if self.scheme not in {None, 'file'}:
+        if self.scheme not in {None, "file"}:
             return True  # FUTURE: alternate tests for still-exists-ness
         if os.path.exists(self.path):
             return True
-        LOG.info('path {} no longer exists'.format(self.path))
+        LOG.info("path {} no longer exists".format(self.path))
         return False
 
 
 class ChainRecordWithDict(MutableMapping):
     """
     allow Product database entries and key-value table to act as a coherent dictionary
     """
@@ -161,84 +144,77 @@
         for k in self.keys():
             yield k, self[k]
 
     def values(self):
         for k in self.keys():
             yield self[k]
 
-    # def update(self, *args, **kwds):
-    #     for arg in args:
-    #         for k, v in arg.items():  # (arg if isinstance(arg, Iterable) else arg.items()):
-    #             self[k] = v
-    #     for k,v in kwds.items():
-    #         self[k] = v
-
     def __len__(self):
         return len(self.keys())
 
     def __iter__(self):
         yield from self.keys()
 
     def __contains__(self, item):
         return item in self.keys()
 
     def __getitem__(self, key):
         fieldname = self._field_keys.get(key)
         if fieldname is not None:
-            assert (isinstance(fieldname, str))
+            assert isinstance(fieldname, str)  # nosec B101
             return getattr(self._obj, fieldname)
         return self._more[key]
 
     def __repr__(self):
-        return '<ChainRecordWithDict {}>'.format(repr(tuple(self.keys())))
+        return "<ChainRecordWithDict {}>".format(repr(tuple(self.keys())))
 
     def __setitem__(self, key, value):
         fieldname = self._field_keys.get(key)  # Info -> fieldname
         if fieldname is not None:
-            LOG.debug('assigning database field {}'.format(fieldname))
-            # self._obj.__dict__[fieldname] = value
+            LOG.debug("assigning database field {}".format(fieldname))
             setattr(self._obj, fieldname, value)
         else:
             self._more[key] = value
 
     def __delitem__(self, key):
         if key in self._field_keys:
-            raise KeyError('cannot remove key {}'.format(key))
+            raise KeyError("cannot remove key {}".format(key))
         del self._more[key]
 
 
-class Product(Base):
+class Product(Base):  # type: ignore
     """
     Primary entity being tracked in metadatabase
     One or more StoredProduct are held in a single File
     A StoredProduct has zero or more Content representations, potentially at different projections
     A StoredProduct has zero or more ProductKeyValue pairs with additional metadata
     A File's format allows data to be imported to the workspace
     A StoredProduct's kind determines how its cached data is transformed to different representations for display
     additional information is stored in a key-value table addressable as product[key:str]
     """
-    __tablename__ = 'products_v1'
+
+    __tablename__ = "products_v1"
 
     # identity information
     id = Column(Integer, primary_key=True)
     resource_id = Column(Integer, ForeignKey(Resource.id))
     # relationship: .resource
-    uuid_str = Column(String, nullable=False,
-                      unique=True)  # UUID representing this data in SIFT, or None if not in cache
+    uuid_str = Column(
+        String, nullable=False, unique=True
+    )  # UUID representing this data in SIFT, or None if not in cache
 
     @property
     def uuid(self):
         return UUID(self.uuid_str)
 
     @uuid.setter
     def uuid(self, uu):
         self.uuid_str = str(uu)
 
     # primary handler
-    # kind = Column(PickleType)  # class or callable which can perform transformations on this data in workspace
     atime = Column(DateTime, nullable=False)  # last time this file was accessed by application
 
     # cached metadata provided by the file format handler
     # product identifier eg "B01", "B02"  # resource + shortname should be sufficient to identify the data
     name = Column(String, nullable=False)
 
     # presentation is consistent within a family
@@ -268,42 +244,32 @@
         return self.family + FCS_SEP + self.category + FCS_SEP + self.serial
 
     @ident.setter
     def ident(self, new_ident: str):
         fam, ctg, ser = new_ident.split("::")
         self.family, self.category, self.serial = fam, ctg, ser
 
-    # platform = Column(String)  # platform or satellite name e.g. "GOES-16", "Himawari-8"; should match Platform enum
-    # standard_name = Column(String, nullable=True)
-    #
     # times
-    # display_time = Column(DateTime)  # normalized instantaneous scheduled observation time e.g. 20170122T2310
     obs_time = Column(DateTime, nullable=False)  # actual observation time start
     obs_duration = Column(Interval, nullable=False)  # duration of the observation
 
-    # native resolution information - see Content for projection details at different LODs
-    # resolution = Column(Integer, nullable=True)  # meters max resolution, e.g. 500, 1000, 2000, 4000
-
-    # descriptive - move these to Info keys
-    # units = Column(Unicode, nullable=True)  # udunits compliant units, e.g. 'K'
-    # label = Column(Unicode, nullable=True)  # "AHI Refl B11"
-    # description = Column(UnicodeText, nullable=True)
-
-    # link to workspace cache files representing this data, not lod=0 is overview
-    content = relationship("Content", backref=backref("product"), cascade="all", order_by=lambda: Content.lod)
+    # link to workspace cache files representing this data
+    content = relationship("Content", backref=backref("product"), cascade="all")
 
     # link to key-value further information
     # this provides dictionary style access to key-value pairs
-    _key_values = relationship("ProductKeyValue", collection_class=attribute_mapped_collection('key'),
-                               cascade="all, delete-orphan")
-    _kwinfo = association_proxy("_key_values", "value",
-                                creator=lambda key, value: ProductKeyValue(key=key, value=value))
+    _key_values = relationship(
+        "ProductKeyValue", collection_class=attribute_mapped_collection("key"), cascade="all, delete-orphan"
+    )
+    _kwinfo = association_proxy(
+        "_key_values", "value", creator=lambda key, value: ProductKeyValue(key=key, value=value)
+    )
 
-    # derived / algebraic layers have a symbol table and an expression
-    # typically Content objects for algebraic layers cache calculation output
+    # derived / algebraic datasets have a symbol table and an expression
+    # typically Content objects for algebraic datasets cache calculation output
     symbol = relationship("SymbolKeyValue", backref=backref("product"), cascade="all, delete-orphan")
     expression = Column(Unicode, nullable=True)
 
     _info = None  # database fields and key-value dictionary merged as one transparent mapping
 
     def __init__(self, *args, **kwargs):
         super(Product, self).__init__(*args, **kwargs)
@@ -315,16 +281,18 @@
         valset = set(cls.INFO_TO_FIELD.values())
         columns = set(cls.__table__.columns.keys())
         for k, v in mapping.items():
             f = cls.INFO_TO_FIELD.get(k)
             if f is not None:
                 fields[f] = v
             elif k in valset:
-                LOG.warning("key {} corresponds to a database field when standard key is available; "
-                            "this code may not be intended".format(k))
+                LOG.warning(
+                    "key {} corresponds to a database field when standard key is available; "
+                    "this code may not be intended".format(k)
+                )
                 fields[k] = v
             elif k in columns:
                 fields[k] = v
             else:
                 keyvalues[k] = v
         return fields, keyvalues
 
@@ -333,15 +301,15 @@
         """
         create a Product using info Info dictionary items and arbitrary key-values
         :param mapping: dictionary of product metadata
         :return: Product object
         """
         fields, keyvalues = cls._separate_fields_and_keys(mapping)
         LOG.debug("fields to import: {}".format(repr(fields)))
-        LOG.debug("key-value pairs to {}: {}".format('IGNORE' if only_fields else 'import', repr(keyvalues)))
+        LOG.debug("key-value pairs to {}: {}".format("IGNORE" if only_fields else "import", repr(keyvalues)))
         try:
             p = cls(**fields)
         except AttributeError:
             LOG.error("unable to initialize Product from info: {}".format(repr(fields)))
             raise
         if not only_fields:
             p.info.update(keyvalues)
@@ -350,16 +318,17 @@
                 p.symbol.append(SymbolKeyValue(key=k, value=v))
         if codeblock:
             p.expression = codeblock
 
         return p
 
     def __repr__(self):
-        return "<Product '{}' @ {}~{} / {} keys>".format(self.name, self.obs_time, self.obs_time + self.obs_duration,
-                                                         len(self.info.keys()))
+        return "<Product '{}' @ {}~{} / {} keys>".format(
+            self.name, self.obs_time, self.obs_time + self.obs_duration, len(self.info.keys())
+        )
 
     @property
     def info(self):
         """
         :return: mapping merging Info-compatible database fields with key-value dictionary access pattern
         """
         if self._info is None:
@@ -385,185 +354,220 @@
     @property
     def proj4(self):
         nat = self.content[-1] if len(self.content) else None
         return nat.proj4 if nat else None
 
     @proj4.setter
     def proj4(self, value):
-        LOG.debug('DEPRECATED: setting proj4 on resource')
+        LOG.debug("DEPRECATED: setting proj4 on resource")
 
     @property
     def cell_height(self):
-        nat = self.content[-1] if len(self.content) else None
-        return nat.cell_height if nat else None
+        if not self.content:
+            return None
+        nat = self.content[-1]
+        if isinstance(nat, ContentImage):
+            return nat.cell_height
+        return None
 
     @cell_height.setter
     def cell_height(self, value):
-        LOG.debug('DEPRECATED: setting cell_height on resource')
+        LOG.debug("DEPRECATED: setting cell_height on resource")
 
     @property
     def cell_width(self):
-        nat = self.content[-1] if len(self.content) else None
-        return nat.cell_width if nat else None
+        if not self.content:
+            return None
+        nat = self.content[-1]
+        if isinstance(nat, ContentImage):
+            return nat.cell_width
+        return None
 
     @cell_width.setter
     def cell_width(self, value):
-        LOG.debug('DEPRECATED: setting cell_width on resource')
+        LOG.debug("DEPRECATED: setting cell_width on resource")
 
     @property
     def origin_x(self):
-        nat = self.content[-1] if len(self.content) else None
-        return nat.origin_x if nat else None
+        if not self.content:
+            return None
+        nat = self.content[-1]
+        if isinstance(nat, ContentImage):
+            return nat.origin_x
+        return None
 
     @origin_x.setter
     def origin_x(self, value):
-        LOG.debug('DEPRECATED: setting origin_x on resource')
+        LOG.debug("DEPRECATED: setting origin_x on resource")
 
     @property
     def origin_y(self):
-        nat = self.content[-1] if len(self.content) else None
-        return nat.origin_y if nat else None
+        if not self.content:
+            return None
+        nat = self.content[-1]
+        if isinstance(nat, ContentImage):
+            return nat.origin_y
+        return None
 
     @origin_y.setter
     def origin_y(self, value):
-        LOG.debug('DEPRECATED: setting origin_y on resource')
+        LOG.debug("DEPRECATED: setting origin_y on resource")
 
-    def can_be_activated_without_importing(self):
-        return len(self.content) > 0
+    @property
+    def grid_origin(self):
+        if not self.content:
+            return None
+        nat = self.content[-1]
+        if isinstance(nat, ContentImage):
+            return nat.grid_origin
+        return None
+
+    @grid_origin.setter
+    def grid_origin(self, value):
+        LOG.debug("DEPRECATED: setting grid_origin on resource")
+
+    @property
+    def grid_first_index_x(self):
+        if not self.content:
+            return None
+        nat = self.content[-1]
+        if isinstance(nat, ContentImage):
+            return nat.grid_first_index_x
+        return None
+
+    @grid_first_index_x.setter
+    def grid_first_index_x(self, value):
+        LOG.debug("DEPRECATED: setting grid_first_index_x on resource")
+
+    @property
+    def grid_first_index_y(self):
+        if not self.content:
+            return None
+        nat = self.content[-1]
+        if isinstance(nat, ContentImage):
+            return nat.grid_first_index_y
+        return None
+
+    @grid_first_index_y.setter
+    def grid_first_index_y(self, value):
+        LOG.debug("DEPRECATED: setting grid_first_index_y on resource")
 
     INFO_TO_FIELD = {
-        Info.SHORT_NAME: 'name',
-        Info.UUID: 'uuid',
-        Info.PROJ: 'proj4',
-        Info.OBS_TIME: 'obs_time',
-        Info.OBS_DURATION: 'obs_duration',
-        Info.CELL_WIDTH: 'cell_width',
-        Info.CELL_HEIGHT: 'cell_height',
-        Info.ORIGIN_X: 'origin_x',
-        Info.ORIGIN_Y: 'origin_y',
-        Info.FAMILY: 'family',
-        Info.CATEGORY: 'category',
-        Info.SERIAL: 'serial'
+        Info.SHORT_NAME: "name",
+        Info.UUID: "uuid",
+        Info.PROJ: "proj4",
+        Info.OBS_TIME: "obs_time",
+        Info.OBS_DURATION: "obs_duration",
+        Info.CELL_WIDTH: "cell_width",
+        Info.CELL_HEIGHT: "cell_height",
+        Info.ORIGIN_X: "origin_x",
+        Info.ORIGIN_Y: "origin_y",
+        Info.GRID_ORIGIN: "grid_origin",
+        Info.GRID_FIRST_INDEX_X: "grid_first_index_x",
+        Info.GRID_FIRST_INDEX_Y: "grid_first_index_y",
+        Info.FAMILY: "family",
+        Info.CATEGORY: "category",
+        Info.SERIAL: "serial",
     }
 
     def touch(self, when=None):
         self.atime = when = when or datetime.utcnow()
         [x.touch(when) for x in self.resource]
 
 
-class ProductKeyValue(Base):
+class ProductKeyValue(Base):  # type: ignore
     """
     key-value pairs associated with a product
     """
-    __tablename__ = 'product_key_values_v1'
+
+    __tablename__ = "product_key_values_v1"
     product_id = Column(ForeignKey(Product.id), primary_key=True)
-    key = Column(PickleType,
-                 primary_key=True)  # FUTURE: can this be a string? for now need pickling of Info/Platform Enum
+    key = Column(
+        PickleType, primary_key=True
+    )  # FUTURE: can this be a string? for now need pickling of Info/Platform Enum
     # relationship: .product
     value = Column(PickleType)
 
 
-class SymbolKeyValue(Base):
+class SymbolKeyValue(Base):  # type: ignore
     """
-    derived layers have a symbol table which becomes namespace used by expression
+    datasets of derived layers have a symbol table which becomes namespace used by expression
     """
-    __tablename__ = 'algebraic_symbol_key_values_v1'
+
+    __tablename__ = "algebraic_symbol_key_values_v1"
     product_id = Column(ForeignKey(Product.id), primary_key=True)
     key = Column(Unicode, primary_key=True)
     # relationship: .product
     value = Column(PickleType, nullable=True)  # UUID object typically
 
 
-class Content(Base):
+class Content(Base):  # type: ignore
     """
     represent flattened product data files in cache (i.e. cache content)
     typically memory-map ready data (np.memmap)
     basic correspondence to projection/geolocation information may accompany
     images will typically have rows>0 cols>0 levels=None (implied levels=1)
     profiles may have rows>0 cols=None (implied cols=1) levels>0
     a given product may have several Content for different projections
     additional information is stored in a key-value table addressable as content[key:str]
     """
-    # _array = None  # when attached, this is a np.memmap
 
-    __tablename__ = 'contents_v1'
+    __tablename__ = "content_v1"
     id = Column(Integer, primary_key=True)
-    product_id = Column(Integer, ForeignKey(Product.id))
-
-    # handle overview versus detailed data
-    lod = Column(Integer)  # power of 2 level of detail; 0 for coarse-resolution overview
-    LOD_OVERVIEW = 0
+    type = Column(String)
+    __mapper_args__ = {
+        "polymorphic_identity": "base",
+        "polymorphic_on": type,
+    }
 
-    resolution = Column(Integer)  # maximum resolution in meters for this representation of the dataset
+    product_id = Column(Integer, ForeignKey(Product.id))
 
     # time accounting, used to check if data needs to be re-imported to workspace,
     # or whether data is LRU and can be removed from a crowded workspace
     mtime = Column(DateTime)  # last observed mtime of the original source of this data, for change checking
     atime = Column(DateTime)  # last time this product was accessed by application
 
     # actual data content
     # NaNs are used to signify missing data; NaNs can include integer category fields in significand;
     # please ref IEEE 754
     path = Column(String, unique=True)  # relative to workspace, binary array of data
-    rows, cols, levels = Column(Integer), Column(Integer, nullable=True), Column(Integer, nullable=True)
+
+    # TODO Number of attributes per point: n_attributes (e.g. lightning peak
+    #  current) but then, what are the datatypes of the attributes?
+    n_attributes = Column(Integer)  # always 1, reserverd for future extensions
+
+    # TODO Currently all attributes must have the same datatype as the points,
+    #  because there is no way yet to define them differently
+    #  therefore only float32 is possible.
+    #  lat, lon (from SwathDefinition) are always float, thus dtype must be
+    #  float for now, only attributes could have a different dtype in the future
     # default float32; can be int16 in the future for scaled integer images for instance; should be a numpy type name
     dtype = Column(String, nullable=True)
+
     # json for numpy array with polynomial coefficients for transforming native data to natural units
     # (e.g. for scaled integers), c[0] + c[1]*x + c[2]*x**2 ...
-    # coeffs = Column(String, nullable=True)
+
     # json for optional dict {int:string} lookup table for NaN flag fields
     # (when dtype is float32 or float64) or integer values (when dtype is an int8/16/32/64)
-    # values = Column(String, nullable=True)
 
     # projection information for this representation of the data
     # proj4 projection string for the data in this array, if one exists; else assume y=lat/x=lon
-    proj4 = Column(String, nullable=True)
-    cell_width = Column(Float, nullable=True)
-    cell_height = Column(Float, nullable=True)
-    origin_x = Column(Float, nullable=True)
-    origin_y = Column(Float, nullable=True)
-
-    # sparsity and coverage, int8 arrays if needed to show incremental availability of the data
-    # dimensionality is always a reduction factor of rows/cols/levels
-    # coverage is stretched across the data array
-    #   e.g. for loading data sectioned or blocked across multiple files
-    # sparsity is broadcast over the data array
-    #   e.g. for incrementally loading sparse data into a dense array
-    # a zero value indicates data is not available, nonzero signifies availability
-    coverage_rows = Column(Integer, nullable=True)
-    coverage_cols = Column(Integer, nullable=True)
-    coverage_levels = Column(Integer, nullable=True)
-    coverage_path = Column(String, nullable=True)
-    sparsity_rows = Column(Integer, nullable=True)
-    sparsity_cols = Column(Integer, nullable=True)
-    sparsity_levels = Column(Integer, nullable=True)
-    sparsity_path = Column(String, nullable=True)
-
-    # navigation information, if required
-    xyz_dtype = Column(String, nullable=True)  # dtype of x,y,z arrays, default float32
-    y_path = Column(String, nullable=True)  # if needed, y location cache path relative to workspace
-    x_path = Column(String, nullable=True)  # if needed, x location cache path relative to workspace
-    z_path = Column(String, nullable=True)  # if needed, z location cache path relative to workspace
+    proj4 = Column(String, nullable=True)  # TODO maybe basis
 
     # link to key-value further information; primarily a hedge in case specific information
     # has to be squirreled away for later consideration for main content table
     # this provides dictionary style access to key-value pairs
-    _key_values = relationship("ContentKeyValue", collection_class=attribute_mapped_collection('key'),
-                               cascade="all, delete-orphan")
-    _kwinfo = association_proxy("_key_values", "value",
-                                creator=lambda key, value: ContentKeyValue(key=key, value=value))
+    _key_values = relationship(
+        "ContentKeyValue", collection_class=attribute_mapped_collection("key"), cascade="all, delete-orphan"
+    )
+    _kwinfo = association_proxy(
+        "_key_values", "value", creator=lambda key, value: ContentKeyValue(key=key, value=value)
+    )
 
-    INFO_TO_FIELD = {
-        Info.CELL_HEIGHT: 'cell_height',
-        Info.CELL_WIDTH: 'cell_width',
-        Info.ORIGIN_X: 'origin_x',
-        Info.ORIGIN_Y: 'origin_y',
-        Info.PROJ: 'proj4',
-        Info.PATHNAME: 'path'
-    }
+    INFO_TO_FIELD = {Info.PROJ: "proj4", Info.PATHNAME: "path"}
 
     _info = None  # database fields and key-value dictionary merged as one transparent mapping
 
     def __init__(self, *args, **kwargs):
         super(Content, self).__init__(*args, **kwargs)
         self._info = ChainRecordWithDict(self, self.INFO_TO_FIELD, self._kwinfo)
 
@@ -574,46 +578,35 @@
         valset = set(cls.INFO_TO_FIELD.values())
         columns = set(cls.__table__.columns.keys())
         for k, v in mapping.items():
             f = cls.INFO_TO_FIELD.get(k)
             if f is not None:
                 fields[f] = v
             elif k in valset:
-                LOG.warning("key {} corresponds to a database field when standard key is available; "
-                            "this code may not be intended".format(k))
+                LOG.warning(
+                    "key {} corresponds to a database field when standard key is available; "
+                    "this code may not be intended".format(k)
+                )
                 fields[k] = v
             elif k in columns:
                 fields[k] = v
             else:
                 keyvalues[k] = v
         return fields, keyvalues
 
-    # @classmethod
-    # def _patch_info_fields(cls, d):
-    #     if 'lod' not in d:
-    #         d['lod'] = Content.LOD_OVERVIEW
-    #     if ('resolution' not in d) and ('cell_width' in d and 'cell_height' in d):
-    #         d['resolution'] = min(d['cell_width'], d['cell_height'])
-    #     now = datetime.utcnow()
-    #     if 'atime' not in d:
-    #         d['atime'] = now
-    #     if 'mtime' not in d:
-    #         d['mtime'] = now
-
     @classmethod
     def from_info(cls, mapping, only_fields=False):
         """
         create a Product using info Info dictionary items and arbitrary key-values
         :param mapping: dictionary of product metadata
         :return: Product object
         """
         fields, keyvalues = cls._separate_fields_and_keys(mapping)
-        # cls._patch_info_fields(fields)
         LOG.debug("fields to import: {}".format(repr(fields)))
-        LOG.debug("key-value pairs to {}: {}".format('IGNORE' if only_fields else 'import', repr(keyvalues)))
+        LOG.debug("key-value pairs to {}: {}".format("IGNORE" if only_fields else "import", repr(keyvalues)))
         try:
             p = cls(**fields)
         except AttributeError:
             LOG.error("unable to initialize Content from info: {}".format(repr(fields)))
             raise
         if not only_fields:
             p.info.update(keyvalues)
@@ -648,282 +641,240 @@
     def name(self):
         return self.product.name
 
     @property
     def uuid(self):
         return self.product.uuid
 
-    @property
-    def is_overview(self):
-        return self.lod == self.LOD_OVERVIEW
-
     def __str__(self):
-        product = "%s:%s.%s" % (self.product.source.name or '?',
-                                self.product.platform or '?',
-                                self.product.identifier or '?')
-        isoverview = ' overview' if self.is_overview else ''
-        dtype = self.dtype or 'float32'
-        xyzcs = ' '.join(
-            q for (q, p) in
-            zip('XYZCS', (self.x_path, self.y_path, self.z_path, self.coverage_path, self.sparsity_path)) if p
+        product = "%s:%s.%s" % (
+            self.product.source.name or "?",
+            self.product.platform or "?",
+            self.product.identifier or "?",
+        )
+        dtype = self.dtype or "float32"
+        xyzcs = " ".join(
+            q
+            for (q, p) in zip("XYZCS", (self.x_path, self.y_path, self.z_path, self.coverage_path, self.sparsity_path))
+            if p
+        )
+        return "<{uuid} product {product} content with path={path} dtype={dtype} {xyzcs}>".format(
+            uuid=self.uuid, product=product, path=self.path, dtype=dtype, xyzcs=xyzcs
         )
-        return "<{uuid} product {product} content{isoverview} with path={path} dtype={dtype} {xyzcs}>".format(
-            uuid=self.uuid, product=product, isoverview=isoverview, path=self.path, dtype=dtype, xyzcs=xyzcs)
 
-    def touch(self, when=None):
+    def touch(self, when: Optional[datetime] = None) -> None:
         self.atime = when = when or datetime.utcnow()
         self.product.touch(when)
 
+
+class ContentImage(Content):
+    __mapper_args__ = {"polymorphic_identity": "image"}
+
+    # handle overview versus detailed data
+    lod = Column(Integer)  # power of 2 level of detail; 0 for coarse-resolution overview
+    LOD_OVERVIEW = 0
+
+    resolution = Column(Integer)  # maximum resolution in meters for this representation of the dataset
+
+    rows = Column(Integer)
+    cols = Column(Integer, nullable=True)
+    levels = Column(Integer, nullable=True)
+
+    cell_width = Column(Float, nullable=True)
+    cell_height = Column(Float, nullable=True)
+    origin_x = Column(Float, nullable=True)
+    origin_y = Column(Float, nullable=True)
+
+    # original grid layout information
+    grid_origin = Column(String, nullable=True)
+    grid_first_index_x = Column(Integer, nullable=True)
+    grid_first_index_y = Column(Integer, nullable=True)
+
+    # sparsity and coverage, int8 arrays if needed to show incremental availability of the data
+    # dimensionality is always a reduction factor of rows/cols/levels
+    # coverage is stretched across the data array
+    #   e.g. for loading data sectioned or blocked across multiple files
+    # sparsity is broadcast over the data array
+    #   e.g. for incrementally loading sparse data into a dense array
+    # a zero value indicates data is not available, nonzero signifies availability
+    coverage_rows = Column(Integer, nullable=True)
+    coverage_cols = Column(Integer, nullable=True)
+    coverage_levels = Column(Integer, nullable=True)
+    coverage_path = Column(String, nullable=True)
+    sparsity_rows = Column(Integer, nullable=True)
+    sparsity_cols = Column(Integer, nullable=True)
+    sparsity_levels = Column(Integer, nullable=True)
+    sparsity_path = Column(String, nullable=True)
+
+    # navigation information, if required
+    xyz_dtype = Column(String, nullable=True)  # dtype of x,y,z arrays, default float32
+    y_path = Column(String, nullable=True)  # if needed, y location cache path relative to workspace
+    x_path = Column(String, nullable=True)  # if needed, x location cache path relative to workspace
+    z_path = Column(String, nullable=True)  # if needed, z location cache path relative to workspace
+
+    INFO_TO_FIELD = {
+        Info.PROJ: "proj4",
+        Info.PATHNAME: "path",
+        Info.CELL_HEIGHT: "cell_height",
+        Info.CELL_WIDTH: "cell_width",
+        Info.ORIGIN_X: "origin_x",
+        Info.ORIGIN_Y: "origin_y",
+        Info.GRID_ORIGIN: "grid_origin",
+        Info.GRID_FIRST_INDEX_X: "grid_first_index_x",
+        Info.GRID_FIRST_INDEX_Y: "grid_first_index_y",
+    }
+
+    def __init__(self, *args, **kwargs):
+        super(ContentImage, self).__init__(*args, **kwargs)
+
+    @property
+    def is_overview(self):
+        return self.lod == self.LOD_OVERVIEW
+
+    @property
+    def shape(self):
+        return tuple(filter(lambda x: x, [self.rows, self.cols, self.levels]))
+
+
+class ContentMultiChannelImage(ContentImage):
+    __mapper_args__ = {"polymorphic_identity": "mc_image"}
+
+    bands = Column(Integer, nullable=True)
+    coverage_bands = Column(Integer, nullable=True)
+
+    def __int__(self, *args, **kwargs):
+        super(ContentMultiChannelImage, self).__int__(*args, **kwargs)
+
     @property
     def shape(self):
-        rcl = reduce(lambda a, b: a + [b] if b else a, [self.rows, self.cols, self.levels], [])
-        return tuple(rcl)
+        return tuple(filter(lambda x: x, [self.rows, self.cols, self.bands]))
 
-    # this doesn't belong here, database routines only plz
-    # @property
-    # def data(self):
-    #     """
-    #     numpy array with the content
-    #     :return:
-    #     """
-    #     self.touch()
-    #     if self._array is not None:
-    #         return self._array
-    #     self._array = zult = np.memmap(self.path, mode='r', shape=self.shape, dtype=self.dtype or 'float32')
-    #     return zult
-
-    # def close(self):
-    #     if self._array is not None:
-    #         self._array = None
 
+class ContentUnstructuredPoints(Content):
+    __mapper_args__ = {"polymorphic_identity": "unstructured_points"}
 
-class ContentKeyValue(Base):
+    n_points = Column(Integer)
+
+    @declared_attr
+    def n_dimensions(cls):
+        # Points may have 2 or 3 spatial dimensions
+        return Content.__table__.c.get("n_dimensions", Column(Integer))
+
+
+class ContentLines(Content):
+    __mapper_args__ = {"polymorphic_identity": "lines"}
+
+    n_lines = Column(Integer)
+
+    @declared_attr
+    def n_dimensions(cls):
+        # Points have 4 spatial dimensions
+        return Content.__table__.c.get("n_dimensions", Column(Integer))
+
+
+class ContentKeyValue(Base):  # type: ignore
     """
     key-value pairs associated with a product
     """
-    __tablename__ = 'content_key_values_v1'
+
+    __tablename__ = "content_key_values_v1"
     product_id = Column(ForeignKey(Content.id), primary_key=True)
     key = Column(PickleType, primary_key=True)  # FUTURE: can this be a string?
     value = Column(PickleType)
 
 
 # singleton instance
 _MDB = None
 
 
 class Metadatabase(object):
     """
     singleton interface to application metadatabase
     """
+
     engine = None
     connection = None
     session_factory = None
     session_nesting = None
 
     def __init__(self, uri=None, **kwargs):
         self.session_nesting = defaultdict(int)
         global _MDB
         if _MDB is not None:
-            raise AssertionError('Metadatabase is a singleton and already exists')
+            raise AssertionError("Metadatabase is a singleton and already exists")
         self._MDB = self
         if uri:
-            self.connect(uri, **kwargs)
+            self._connect(uri, **kwargs)
 
     @staticmethod
     def instance(*args, **kwargs):
         global _MDB
         if _MDB is None:
             _MDB = Metadatabase(*args, **kwargs)
         return _MDB
 
     def _all_tables_present(self):
         from sqlalchemy.engine.reflection import Inspector
+
         inspector = Inspector.from_engine(self.engine)
         all_tables = set(inspector.get_table_names())
         zult = True
-        for table_name in (Resource.__tablename__, Product.__tablename__,
-                           ProductKeyValue.__tablename__, SymbolKeyValue.__tablename__,
-                           Content.__tablename__, ContentKeyValue.__tablename__, PRODUCTS_FROM_RESOURCES_TABLE_NAME):
+        for table_name in (
+            Resource.__tablename__,
+            Product.__tablename__,
+            ProductKeyValue.__tablename__,
+            SymbolKeyValue.__tablename__,
+            Content.__tablename__,
+            ContentKeyValue.__tablename__,
+            PRODUCTS_FROM_RESOURCES_TABLE_NAME,
+        ):
             present = table_name in all_tables
             LOG.debug("table {} {} present in database".format(table_name, "is" if present else "is not"))
             zult = False if not present else zult
         return zult
 
-    def connect(self, uri, create_tables=False, **kwargs):
-        assert (self.engine is None)
-        assert (self.connection is None)
+    def _connect(self, uri, create_tables=False, **kwargs):
+        assert self.engine is None  # nosec B101
+        assert self.connection is None  # nosec B101
         self.engine = create_engine(uri, **kwargs)
-        LOG.info('attaching database at {}'.format(uri))
+        LOG.info("attaching database at {}".format(uri))
         if create_tables or not self._all_tables_present():
             LOG.info("creating database tables")
             Base.metadata.create_all(self.engine)
         self.connection = self.engine.connect()
         # http://docs.sqlalchemy.org/en/latest/orm/contextual.html
         self.session_factory = sessionmaker(bind=self.engine)
         self.SessionRegistry = scoped_session(self.session_factory)  # thread-local session registry
 
     def session(self):
         return self.session_factory()
 
     def __enter__(self) -> Session:
         ses = self.SessionRegistry()
+        assert self.session_nesting is not None  # nosec B101 # suppress mypy [index]
         self.session_nesting[id(ses)] += 1
         return ses
 
     def __exit__(self, exc_type, exc_val, exc_tb):
         # fetch the active session, typically zero or one active per thread
         s = self.SessionRegistry()
         self.session_nesting[id(s)] -= 1
-        # LOG.debug("database session nesting now at {}".format(self.session_nesting[id(s)]))
         if self.session_nesting[id(s)] <= 0:
             if exc_val is not None:
                 LOG.warning("an exception occurred, rolling back any metadatabase changes")
                 s.rollback()
             else:
                 if bool(s.dirty) or bool(s.new) or bool(s.deleted):
                     LOG.debug("committing metadatabase changes")
                     s.commit()
                 else:
                     LOG.debug("closing clean database session without commit")
-                    # LOG.debug("session is clean but committing before close anyway")
-                    # s.commit()
             s.close()
             del self.session_nesting[id(s)]
         else:  # we're in a nested context for this session
             if exc_val is not None:  # propagate the exception to the outermost session context
-                LOG.warning('propagating database exception to outermost context')
+                LOG.warning("propagating database exception to outermost context")
                 raise exc_val
 
     #
     # high-level functions
     #
-
-
-# # ============================
-# # mapping wrappers
-#
-# class ProductInfoAsWritableMappingAdapter(MutableMapping):
-#     """
-#     database Product.info dictionary adapter
-#     """
-#     def __init__(self, session, product, warn_on_write=True):
-#         self.S = session
-#         self.prod = product
-#         self.wow = warn_on_write
-#
-#     def __contains__(self, item):
-#         items = self.S.query(ProductKeyValue).filter_by(product_id=self.prod.id, key=item).all()
-#         return len(items)>0
-#
-#     def __getitem__(self, item:str):
-#         kvs = self.S.query(ProductKeyValue).filter_by(product_id=self.prod.id, key=item).all()
-#         if not kvs:
-#             raise KeyError("product does not have value for key {}".format(item))
-#         if len(kvs)>1:
-#             raise AssertionError('more than one value for %s' % item)
-#
-#     def __setitem__(self, key, value):
-#         if self.wow:
-#             LOG.warning('attempting to write to Product info dictionary in workspace??')
-#         kvs = self.S.query(ProductKeyValue).filter_by(product_id=self.prod.id, key=key).all()
-#         if not kvs:
-#             kv = ProductKeyValue(key=key, value=value)
-#             self.S.add(kv)
-#             self.product.info.append(kv)
-#             self.S.commit()
-#         if len(kvs)>1:
-#             raise AssertionError('more than one value for {}'.format(key))
-#         kvs[0].value = value
-#         self.S.commit()
-
-
-# ============================
-# support and testing routines
-
-class tests(unittest.TestCase):
-    # data_file = os.environ.get('TEST_DATA', os.path.expanduser("~/Data/test_files/thing.dat"))
-    mdb = None
-
-    def setUp(self):
-        pass
-
-    def test_insert(self):
-        from datetime import datetime, timedelta
-        mdb = Metadatabase('sqlite://', create_tables=True)
-        # mdb.create_tables()
-        s = mdb.session()
-        from uuid import uuid1
-        uu = uuid1()
-        when = datetime.utcnow()
-        nextwhen = when + timedelta(minutes=5)
-        f = Resource(path='/path/to/foo.bar', mtime=when, atime=when, format=None)
-        p = Product(uuid_str=str(uu), atime=when, name='B00 Refl', obs_time=when, obs_duration=timedelta(minutes=5))
-        f.product.append(p)
-        p.info['test_key'] = u'test_value'
-        p.info['turkey'] = u'cobbler'
-        s.add(f)
-        s.add(p)
-        s.commit()
-        p.info.update({'key': 'value'})
-        p.info.update({Info.OBS_TIME: datetime.utcnow()})
-        p.info.update({Info.OBS_TIME: nextwhen, Info.OBS_DURATION: timedelta(seconds=15)})
-        # p.info.update({'key': 'value', Info.OBS_TIME: nextwhen, Info.OBS_DURATION: nextwhen + timedelta(seconds=15)})
-        # p.info[Info.OBS_TIME] = nextwhen
-        # p.info['key'] = 'value'
-        # p.obs_time = nextwhen
-        s.commit()
-        self.assertIs(p.resource[0], f)
-        self.assertEqual(p.uuid, uu)
-        self.assertEqual(p.obs_time, nextwhen)
-        q = f.product[0]
-        # q = s.query(Product).filter_by(resource=f).first()
-        self.assertEqual(q.info['test_key'], u'test_value')
-        # self.assertEquals(q[Info.UUID], q.uuid)
-        self.assertEqual(q.info['turkey'], p.info['turkey'])
-        self.assertEqual(q.info['key'], p.info['key'])
-        # self.assertEqual(q.obs_time, nextwhen)
-
-
-def _debug(type, value, tb):
-    "enable with sys.excepthook = debug"
-    if not sys.stdin.isatty():
-        sys.__excepthook__(type, value, tb)
-    else:
-        import traceback
-        import pdb  # noqa
-        traceback.print_exception(type, value, tb)
-        # then start the debugger in post-mortem mode.
-        pdb.post_mortem(tb)  # more modern
-
-
-def main():
-    parser = argparse.ArgumentParser(
-        description="PURPOSE",
-        epilog="",
-        fromfile_prefix_chars='@')
-    parser.add_argument('-v', '--verbose', dest='verbosity', action="count", default=0,
-                        help='each occurrence increases verbosity 1 level through ERROR-WARNING-Info-DEBUG')
-    parser.add_argument('-d', '--debug', dest='debug', action='store_true',
-                        help="enable interactive PDB debugger on exception")
-    # http://docs.python.org/2.7/library/argparse.html#nargs
-    # parser.add_argument('--stuff', nargs='5', dest='my_stuff',
-    #                    help="one or more random things")
-    parser.add_argument('inputs', nargs='*',
-                        help="input files to process")
-    args = parser.parse_args()
-
-    if args.debug:
-        sys.excepthook = _debug
-
-    if not args.inputs:
-        logging.basicConfig(level=logging.DEBUG)
-        unittest.main()
-        return 0
-
-    levels = [logging.ERROR, logging.WARN, logging.INFO, logging.DEBUG]
-    logging.basicConfig(level=levels[min(3, args.verbosity)])
-
-    return 0
-
-
-if __name__ == '__main__':
-    sys.exit(main())
```

### Comparing `uwsift-1.2.3/uwsift/workspace/workspace.py` & `uwsift-2.0.0b0/uwsift/workspace/caching_workspace.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,240 +1,40 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-"""Implement Workspace, a singleton object which manages large amounts of data and caches local content.
-
-Workspace of Products
-
-- retrieved from Resources and
-- represented by multidimensional Content, each of which has data,
-  coverage, and sparsity arrays in separate workspace flat files
-
-Workspace responsibilities include:
-
-- understanding projections and y, x, z coordinate systems
-- subsecting data within slicing or geospatial boundaries
-- caching useful arrays as secondary content
-- performing minimized on-demand calculations, e.g. algebraic layers, in the background
-- use Importers to bring content arrays into the workspace from external resources, also in the background
-- maintain a metadatabase of what products have in-workspace content, and what products are available
-  from external resources
-- compose Collector, which keeps track of Products within Resources outside the workspace
-
-FUTURE import sequence:
-
-- trigger: user requests skim (metadata only) or import (metadata plus bring into document)
-      of a file or directory system for each file selected
-- phase 1: regex for file patterns identifies which importers are worth trying
-- phase 2: background: importers open files, form metadatabase insert transaction,
-      first importer to succeed wins (priority order). stop after this if just skimming
-- phase 3: background: load of overview (lod=0), adding flat files to workspace and Content entry to metadatabase
-- phase 3a: document and scenegraph show overview up on screen
-- phase 4: background: load of one or more levels of detail, with max LOD currently being considered native
-- phase 4a: document updates to show most useful LOD+stride content
-
-:author: R.K.Garcia <rayg@ssec.wisc.edu>
-:copyright: 2014-2017 by University of Wisconsin Regents, see AUTHORS for more details
-:license: GPLv3, see LICENSE for more details
-
-"""
-
 import logging
 import os
 import shutil
-from collections import defaultdict, OrderedDict
-from collections.abc import Mapping as ReadOnlyMapping
-from datetime import datetime, timedelta
-from typing import Mapping, Generator, Tuple, Dict
-from uuid import UUID, uuid1 as uuidgen
+from collections import OrderedDict
+from datetime import datetime
+from typing import Dict, Generator, Mapping, Optional, Tuple
+from uuid import UUID
 
-import numba as nb
 import numpy as np
-from PyQt5.QtCore import QObject, pyqtSignal
-from pyproj import Proj
-from rasterio import Affine
-from shapely.geometry.polygon import LinearRing
 from sqlalchemy.orm.exc import NoResultFound
 
-from uwsift.common import Info, Kind, Flags, State
-from uwsift.model.shapes import content_within_shape
-from uwsift.queue import TASK_PROGRESS, TASK_DOING
-from .importer import aImporter, SatpyImporter, generate_guidebook_metadata
-from .metadatabase import Metadatabase, Content, Product, Resource
+from uwsift.common import Info, Kind, State
+from uwsift.queue import TASK_DOING, TASK_PROGRESS
+
+from .importer import SatpyImporter, aImporter
+from .metadatabase import Content, ContentImage, Metadatabase, Product, Resource
+from .workspace import ActiveContent, BaseWorkspace, frozendict
 
 LOG = logging.getLogger(__name__)
 
 DEFAULT_WORKSPACE_SIZE = 256
 MIN_WORKSPACE_SIZE = 8
 
 IMPORT_CLASSES = [SatpyImporter]
 
 # first instance is main singleton instance; don't preclude the possibility of importing from another workspace later on
 TheWorkspace = None
 
 
-class frozendict(ReadOnlyMapping):
-    def __init__(self, source=None):
-        self._D = dict(source) if source else {}
-
-    def __getitem__(self, key):
-        return self._D[key]
-
-    def __iter__(self):
-        for k in self._D.keys():
-            yield k
-
-    def __len__(self):
-        return len(self._D)
-
-    def __repr__(self):
-        return "frozendict({" + ", ".join("{}: {}".format(repr(k), repr(v)) for (k, v) in self.items()) + "})"
-
-
-@nb.jit(nogil=True)
-def mask_from_coverage_sparsity_2d(mask: np.ndarray, coverage: np.ndarray, sparsity: np.ndarray):
-    """
-    update a numpy.ma style mask from coverage and sparsity arrays
-    Args:
-        mask (np.array, dtype=bool): mutable array to be updated as a numpy.masked_array mask
-        coverage (np.array) : coverage array to stretch across the mask
-        sparsity (np.array) : sparsity array to repeat across the mask
-    """
-    # FIXME: we should not be using this without slicing to the part of interest; else we can eat a whole lot of memory
-    h, w = mask.shape
-    cov_rpt_y, cov_rpt_x = h // coverage.shape[0], w // coverage.shape[1]
-    spr_h, spr_w = sparsity.shape
-    for y in range(h):
-        for x in range(w):
-            mask[y, x] |= bool(0 == coverage[y // cov_rpt_y, x // cov_rpt_x] * sparsity[y % spr_h, x % spr_w])
-
-
-class ActiveContent(QObject):
-    """
-    ActiveContent composes numpy.memmap arrays with their corresponding Content metadata, and is owned by Workspace
-    Purpose: consolidate common operations on content, while factoring in things like sparsity, coverage, y, x, z arrays
-    Workspace instantiates ActiveContent from metadatabase Content entries
-    """
-    _cid = None  # Content.id database entry I belong to
-    _wsd = None  # full path of workspace
-    _rcl = None
-    _y = None
-    _x = None
-    _z = None
-    _data = None
-    _mask = None
-    _coverage = None
-    _sparsity = None
-
-    def __init__(self, workspace_cwd: str, C: Content):
-        super(ActiveContent, self).__init__()
-        self._cid = C.id
-        self._wsd = workspace_cwd
-        if workspace_cwd is None and C is None:
-            LOG.warning('test initialization of ActiveContent')
-            self._test_init()
-        else:
-            self._attach(C)
-
-    def _test_init(self):
-        data = np.ones((4, 12), dtype=np.float32)
-        data = np.cumsum(data, axis=0)
-        data = np.cumsum(data, axis=1)
-        self._data = data
-        self._sparsity = sp = np.zeros((2, 2), dtype=np.int8)
-        sp[1, 1] = 1  # only 1/4 of dataset loaded
-        self._coverage = co = np.zeros((4, 1), dtype=np.int8)
-        co[2:4] = 1  # and of that, only the bottom half of the image
-
-    @staticmethod
-    def _rcls(rows: int, columns: int, levels: int):
-        """
-        :param rows: rows or None
-        :param columns: columns or None
-        :param levels: levels or None
-        :return: condensed tuple(string with 'rcl', 'rc', 'rl', dimension tuple corresponding to string)
-        """
-        rcl_shape = tuple(
-            (name, dimension) for (name, dimension) in zip('rcl', (rows, columns, levels)) if dimension)
-        rcl = tuple(x[0] for x in rcl_shape)
-        shape = tuple(x[1] for x in rcl_shape)
-        return rcl, shape
-
-    @classmethod
-    def can_attach(cls, wsd: str, c: Content):
-        """
-        Is this content available in the workspace?
-        Args:
-            wsd: workspace realpath
-            c: Content metadatabase entry
-
-        Returns:
-            bool
-        """
-        path = os.path.join(wsd, c.path)
-        return os.access(path, os.R_OK) and (os.stat(path).st_size > 0)
-
-    @property
-    def data(self):
-        """
-        Returns: content data (np.ndarray)
-        """
-        # FIXME: apply sparsity, coverage, and missing value masks
-        return self._data
-
-    def _update_mask(self):
-        """
-        merge sparsity and coverage mask to a standard maskedarray mask
-        :return:
-        """
-        # FIXME: beware the race conditions with this
-        # FIXME: it would be better to lazy-eval the mask, assuming coverage and sparsity << data
-        if self._mask is None:
-            # self._mask = mask = np.zeros_like(self._data, dtype=bool)
-            self._mask = mask = ~np.isfinite(self._data)
-        else:
-            mask = self._mask
-            mask[:] = False
-        # LOG.warning('mask_from_coverage_sparsity needs inclusion')
-        # present = np.array([[1]], dtype=np.int8)
-        # mask_from_coverage_sparsity_2d(mask, self._coverage or present, self._sparsity or present)
-
-    def _attach(self, c: Content, mode='c'):
-        """
-        attach content arrays, for holding by workspace in _available
-        :param c: Content entity from database
-        :return: workspace_data_arrays instance
-        """
-        self._rcl, self._shape = rcl, shape = self._rcls(c.rows, c.cols, c.levels)
-
-        def mm(path, *args, **kwargs):
-            full_path = os.path.join(self._wsd, path)
-            if not os.access(full_path, os.R_OK):
-                LOG.warning("unable to find {}".format(full_path))
-                return None
-            return np.memmap(full_path, *args, **kwargs)
-
-        self._data = mm(c.path, dtype=c.dtype or np.float32, mode=mode, shape=shape)  # potentially very very large
-        self._y = mm(c.y_path, dtype=c.dtype or np.float32, mode=mode, shape=shape) if c.y_path else None
-        self._x = mm(c.x_path, dtype=c.dtype or np.float32, mode=mode, shape=shape) if c.x_path else None
-        self._z = mm(c.z_path, dtype=c.dtype or np.float32, mode=mode, shape=shape) if c.z_path else None
-
-        _, cshape = self._rcls(c.coverage_cols, c.coverage_cols, c.coverage_levels)
-        self._coverage = mm(c.coverage_path, dtype=np.int8, mode=mode, shape=cshape) if c.coverage_path else np.array(
-            [1])
-        _, sshape = self._rcls(c.coverage_cols, c.coverage_cols, c.coverage_levels)
-        self._sparsity = mm(c.sparsity_path, dtype=np.int8, mode=mode, shape=sshape) if c.sparsity_path else np.array(
-            [1])
-
-        self._update_mask()
-
-
-class Workspace(QObject):
+class CachingWorkspace(BaseWorkspace):
     """Data management and cache object.
 
-    Workspace is a singleton object which works with Datasets shall:
+    CachingWorkspace is a singleton object which works with Datasets shall:
 
     - own a working directory full of recently used datasets
     - provide DatasetInfo dictionaries for shorthand use between application subsystems
 
         - datasetinfo dictionaries are ordinary python dictionaries containing [Info.UUID],
           projection metadata, LOD info
 
@@ -245,142 +45,77 @@
     - incrementally cache often-used subsections and strides ("image pyramid") using appropriate tools like gdal
     - notify subscribers of changes to datasets (Qt signal/slot pub-sub)
     - during idle, clean out unused/idle data content, given DatasetInfo contents provides enough metadata to recreate
     - interface to external data processing or loading plug-ins and notify application of new-dataset-in-workspace
 
     """
 
-    cwd = None  # directory we work in
-    _own_cwd = None  # whether or not we created the cwd - which is also whether or not we're allowed to destroy it
-    _pool = None  # process pool that importers can use for background activities, if any
-    # _importers = None  # list of importers to consult when asked to start an import
-    _available: Mapping[int, ActiveContent] = None  # dictionary of {Content.id : ActiveContent object}
-    _inventory: Metadatabase = None  # metadatabase instance, sqlalchemy
-    _inventory_path = None  # filename to store and load inventory information (simple cache)
-    _tempdir = None  # TemporaryDirectory, if it's needed (i.e. a directory name was not given)
-    _max_size_gb = None  # maximum size in gigabytes of flat files we cache in the workspace
-    _queue = None
-
-    # signals
-    # a dataset started importing; generated after overview level of detail is available
-    # didStartImport = pyqtSignal(dict)
-    # didMakeImportProgress = pyqtSignal(dict)
-    didUpdateProductsMetadata = pyqtSignal(set)  # set of UUIDs with changes to their metadata
-    # didFinishImport = pyqtSignal(dict)  # all loading activities for a dataset have completed
-    # didDiscoverExternalDataset = pyqtSignal(dict)  # a new dataset was added to the workspace from an external agent
-    didChangeProductState = pyqtSignal(UUID, Flags)  # a product changed state, e.g. an importer started working on it
-
-    _state: Mapping[UUID, Flags] = None
-
-    def set_product_state_flag(self, uuid: UUID, flag):
-        """primarily used by Importers to signal work in progress
-        """
-        state = self._state[uuid]
-        state.add(flag)
-        self.didChangeProductState.emit(uuid, state)
-
-    def clear_product_state_flag(self, uuid: UUID, flag):
-        state = self._state[uuid]
-        state.remove(flag)
-        self.didChangeProductState.emit(uuid, state)
-
-    def product_state(self, uuid: UUID) -> Flags:
-        state = Flags(self._state[uuid])
-        # add any derived information
-        if uuid in self._available:
-            state.add(State.ATTACHED)
-        with self._inventory as s:
-            ncontent = s.query(Content).filter_by(uuid=uuid).count()
-        if ncontent > 0:
-            state.add(State.CACHED)
-        return state
-
     @property
     def _S(self):
         """
         use scoped_session registry of metadatabase to provide thread-local session object.
         ref http://docs.sqlalchemy.org/en/latest/orm/contextual.html
         Returns:
         """
         return self._inventory.SessionRegistry
 
     @property
     def metadatabase(self) -> Metadatabase:
         return self._inventory
 
-    @staticmethod
-    def defaultWorkspace():
-        """
-        return the default (global) workspace
-        Currently no plans to have more than one workspace, but secondaries may eventually have some advantage.
-        :return: Workspace instance
-        """
-        return TheWorkspace
-
-    def __init__(self, directory_path=None, process_pool=None, max_size_gb=None, queue=None, initial_clear=False):
-        """
-        Initialize a new or attach an existing workspace, creating any necessary bookkeeping.
-        """
-        super(Workspace, self).__init__()
-        self._queue = queue
-        self._max_size_gb = max_size_gb if max_size_gb is not None else DEFAULT_WORKSPACE_SIZE
+    def __init__(
+        self,
+        directory_path: str,
+        process_pool=None,
+        max_size_gb=DEFAULT_WORKSPACE_SIZE,
+        queue=None,
+        initial_clear=False,
+    ):
+        super(
+            CachingWorkspace,
+            self,
+        ).__init__(directory_path, queue=queue)
+        self._max_size_gb = max_size_gb  # maximum size in gigabytes of flat files we cache in the workspace
         if self._max_size_gb < MIN_WORKSPACE_SIZE:
             self._max_size_gb = MIN_WORKSPACE_SIZE
-            LOG.warning('setting workspace size to %dGB' % self._max_size_gb)
+            LOG.warning("setting workspace size to %dGB" % self._max_size_gb)
         if directory_path is None:
+            # a directory name was not given, we need a temporary directory
             import tempfile
+
             self._tempdir = tempfile.TemporaryDirectory()
             directory_path = str(self._tempdir)
-            LOG.info('using temporary directory {}'.format(directory_path))
+            LOG.info("using temporary directory {}".format(directory_path))
 
-        # HACK: handle old workspace command line flag
-        if isinstance(directory_path, (list, tuple)):
-            self.cache_dir = cache_path = os.path.abspath(directory_path[1])
-            self.cwd = directory_path = os.path.abspath(directory_path[0])
-        else:
-            self.cwd = directory_path = os.path.abspath(directory_path)
-            self.cache_dir = cache_path = os.path.join(self.cwd, 'data_cache')
-        self._inventory_path = os.path.join(self.cwd, '_inventory.db')
+        # filename to store and load inventory information (simple cache)
+        self._inventory_path = os.path.join(self.cwd, "_inventory.db")
         if initial_clear:
             self.clear_workspace_content()
-        if not os.path.isdir(cache_path):
-            LOG.info("creating new workspace cache at {}".format(cache_path))
-            os.makedirs(cache_path)
-        if not os.path.isdir(directory_path):
-            LOG.info("creating new workspace at {}".format(directory_path))
-            os.makedirs(directory_path)
-            self._own_cwd = True
-            self._init_create_workspace()
+
         else:
             LOG.info("attaching pre-existing workspace at {}".format(directory_path))
             self._own_cwd = False
             self._init_inventory_existing_datasets()
 
-        self._available = {}
-        self._importers = IMPORT_CLASSES.copy()
-        self._state = defaultdict(Flags)
-        global TheWorkspace  # singleton
-        if TheWorkspace is None:
-            TheWorkspace = self
-
     def _init_create_workspace(self):
         """
         initialize a previously empty workspace
         :return:
         """
         should_init = not os.path.exists(self._inventory_path)
         dn, fn = os.path.split(self._inventory_path)
         if not os.path.isdir(dn):
             raise EnvironmentError("workspace directory {} does not exist".format(dn))
-        LOG.info('{} database at {}'.format('initializing' if should_init else 'attaching', self._inventory_path))
-        self._inventory = Metadatabase('sqlite:///' + self._inventory_path, create_tables=should_init)
+        LOG.info("{} database at {}".format("initializing" if should_init else "attaching", self._inventory_path))
+        # metadatabase instance, sqlalchemy:
+        self._inventory = Metadatabase("sqlite:///" + self._inventory_path, create_tables=should_init)
         if should_init:
             with self._inventory as s:
-                assert (0 == s.query(Content).count())
-        LOG.info('done with init')
+                assert 0 == s.query(Content).count()  # nosec B101
+        LOG.info("done with init")
 
     def clear_workspace_content(self):
         """Remove binary files from workspace and workspace database."""
         LOG.info("Clearing workspace contents...")
         try:
             os.remove(self._inventory_path)
         except FileNotFoundError:
@@ -399,15 +134,16 @@
         to_purge = []
         with self._inventory as s:
             for c in s.query(Content).all():
                 if not ActiveContent.can_attach(self.cache_dir, c):
                     LOG.warning("purging missing content {}".format(c.path))
                     to_purge.append(c)
             LOG.debug(
-                "{} content entities no longer present in cache - will remove from database".format(len(to_purge)))
+                "{} content entities no longer present in cache - will remove from database".format(len(to_purge))
+            )
             for c in to_purge:
                 try:
                     c.product.content.remove(c)
                 except AttributeError:
                     # no_product
                     LOG.warning("orphaned content {}??, removing".format(c.path))
                 s.delete(c)
@@ -419,40 +155,37 @@
         LOG.debug("purging any resources that are no longer accessible")
         with self._inventory as s:
             resall = list(s.query(Resource).all())
             n_purged = 0
             for r in resall:
                 if not r.exists():
                     LOG.info("resource {} no longer exists, purging from database")
-                    # for p in r.product:
-                    #     p.resource.remove(r)
                     n_purged += 1
                     s.delete(r)
         LOG.info("discarded metadata for {} orphaned resources".format(n_purged))
 
     def _purge_orphan_products(self):
         """
         remove products from database that have no cached Content, and no Resource we can re-import from
         """
-        LOG.debug("purging Products no longer recoverable by re-importing from Resources, "
-                  "and having no Content representation in cache")
+        LOG.debug(
+            "purging Products no longer recoverable by re-importing from Resources, "
+            "and having no Content representation in cache"
+        )
         with self._inventory as s:
             n_purged = 0
             prodall = list(s.query(Product).all())  # SIFT/sift#180, avoid locking database too long
             for p in prodall:
                 if len(p.content) == 0 and len(p.resource) == 0:
                     n_purged += 1
                     s.delete(p)
         LOG.info("discarded metadata for {} orphaned products".format(n_purged))
 
     def _migrate_metadata(self):
         """Replace legacy metadata uses with new uses."""
-        # with self._inventory as s:
-        #     for p in s.query(Product).all():
-        #         pass
 
     def _bgnd_startup_purge(self):
         ntot = 5
         n = 1
         yield {TASK_DOING: "DB pruning cache entries", TASK_PROGRESS: float(n) / float(ntot)}
         self._purge_missing_content()
         n += 1
@@ -463,266 +196,217 @@
         self._purge_orphan_products()
         n += 1
         yield {TASK_DOING: "DB migrating metadata", TASK_PROGRESS: float(n) / float(ntot)}
         self._migrate_metadata()
         n += 1
         yield {TASK_DOING: "DB ready", TASK_PROGRESS: float(n) / float(ntot)}
 
-    def _then_refresh_mdb_customers(self, *args, **kwargs):
-        self.didUpdateProductsMetadata.emit(set())
-
     def _init_inventory_existing_datasets(self):
         """
         Do an inventory of an pre-existing workspace
         FIXME: go through and check that everything in the workspace makes sense
         FIXME: check workspace subdirectories for helper sockets and mmaps
         :return:
         """
         # attach the database, creating it if needed
         self._init_create_workspace()
         for _ in self._bgnd_startup_purge():
             # SIFT/sift#180 -- background thread of lengthy database operations can cause lock failure in pysqlite
             pass
-        # self._queue.add("database cleanup", self._bgnd_startup_purge(), "database cleanup",
-        #           interactive=False, and_then=self._then_refresh_mdb_customers)
-
-    def _store_inventory(self):
-        """
-        write inventory dictionary to an inventory.pkl file in the cwd
-        :return:
-        """
-        self._S.commit()
 
     #
     #  data array handling
     #
 
     def _remove_content_files_from_workspace(self, c: Content):
         total = 0
         for filename in [c.path, c.coverage_path, c.sparsity_path]:
             if not filename:
                 continue
             pn = os.path.join(self.cache_dir, filename)
             if os.path.exists(pn):
-                LOG.debug('removing {}'.format(pn))
+                LOG.debug("removing {}".format(pn))
                 total += os.stat(pn).st_size
                 try:
                     os.remove(pn)
                 except FileNotFoundError:
                     LOG.warning("could not remove {} - file not found; continuing".format(pn))
 
         return total
 
     def _activate_content(self, c: Content) -> ActiveContent:
-        self._available[c.id] = zult = ActiveContent(self.cache_dir, c)
+        self._available[c.id] = zult = ActiveContent(self.cache_dir, c, self.get_info(c.uuid))
         c.touch()
         c.product.touch()
         return zult
 
     def _cached_arrays_for_content(self, c: Content):
         """
         attach cached data indicated in Content, unless it's been attached already and is in _available
         touch the content and product in the database to appease the LRU gods
         :param c: metadatabase Content object for session attached to current thread
         :return: workspace_content_arrays
         """
         cache_entry = self._available.get(c.id)
         return cache_entry or self._activate_content(c)
 
-    def _deactivate_content_for_product(self, p: Product):
-        if p is None:
-            return
-        for c in p.content:
-            self._available.pop(c.id, None)
-
     #
     # often-used queries
     #
 
-    def _product_with_uuid(self, session, uuid) -> Product:
+    def _product_with_uuid(self, session, uuid: UUID) -> Product:
         return session.query(Product).filter_by(uuid_str=str(uuid)).first()
 
-    def _product_overview_content(self, session, prod: Product = None, uuid: UUID = None,
-                                  kind: Kind = Kind.IMAGE) -> Content:
+    def _product_overview_content(
+        self, session, prod: Optional[Product] = None, uuid: Optional[UUID] = None, kind: Kind = Kind.IMAGE
+    ) -> Optional[Content]:
         if prod is None and uuid is not None:
             # Get Product object
             try:
                 prod = session.query(Product).filter(Product.uuid_str == str(uuid)).one()
             except NoResultFound:
                 LOG.error("No product with UUID {} found".format(uuid))
                 return None
-        contents = session.query(Content).filter(Content.product_id == prod.id).order_by(Content.lod).all()
+        assert prod  # nosec B101 # suppress mypy [union-attr]
+
+        if kind == Kind.IMAGE:
+            contents = (
+                session.query(ContentImage).filter(ContentImage.product_id == prod.id).order_by(ContentImage.lod).all()
+            )
+        else:
+            contents = session.query(Content).filter(Content.product_id == prod.id)
         contents = [c for c in contents if c.info.get(Info.KIND, Kind.IMAGE) == kind]
         return None if 0 == len(contents) else contents[0]
 
-    def _product_native_content(self, session, prod: Product = None, uuid: UUID = None,
-                                kind: Kind = Kind.IMAGE) -> Content:
-        # NOTE: This assumes the last Content object is the best resolution
+    def _product_native_content(
+        self, session, prod: Optional[Product] = None, uuid: Optional[UUID] = None, kind: Kind = Kind.IMAGE
+    ) -> Optional[Content]:
+        # NOTE: This assumes the last Content object is the best resolution,
         #       but it is untested
         if prod is None and uuid is not None:
             # Get Product object
             try:
                 prod = session.query(Product).filter(Product.uuid_str == str(uuid)).one()
             except NoResultFound:
                 LOG.error("No product with UUID {} found".format(uuid))
                 return None
-        contents = session.query(Content).filter(Content.product_id == prod.id).order_by(Content.lod.desc()).all()
+        assert prod  # nosec B101 # suppress mypy [union-attr]
+
+        if kind == Kind.IMAGE:
+            contents = (
+                session.query(ContentImage)
+                .filter(ContentImage.product_id == prod.id)
+                .order_by(ContentImage.lod.desc())
+                .all()
+            )
+        else:
+            contents = session.query(Content).filter(Content.product_id == prod.id)
         contents = [c for c in contents if c.info.get(Info.KIND, Kind.IMAGE) == kind]
         return None if 0 == len(contents) else contents[-1]
 
     #
     # combining queries with data content
     #
 
-    def _overview_content_for_uuid(self, uuid, kind=Kind.IMAGE):
+    def _overview_content_for_uuid(self, uuid: UUID, kind: Kind = Kind.IMAGE) -> np.memmap:
         # FUTURE: do a compound query for this to get the Content entry
         # prod = self._product_with_uuid(uuid)
         # assert(prod is not None)
         with self._inventory as s:
-            ovc = self._product_overview_content(s, uuid=uuid)
-            assert (ovc is not None)
+            ovc = self._product_overview_content(s, uuid=uuid, kind=kind)
+            assert ovc is not None  # nosec B101
             arrays = self._cached_arrays_for_content(ovc)
             return arrays.data
 
-    def _native_content_for_uuid(self, uuid):
-        # FUTURE: do a compound query for this to get the Content entry
-        # prod = self._product_with_uuid(uuid)
-        with self._inventory as s:
-            nac = self._product_native_content(s, uuid=uuid)
-            arrays = self._cached_arrays_for_content(nac)
-            return arrays.data
-
     #
     # workspace file management
     #
 
     @property
     def _total_workspace_bytes(self):
         """
         total number of bytes in the workspace by brute force instead of metadata search
         :return:
         """
         total = 0
         for root, _, files in os.walk(self.cache_dir):
             sz = sum(os.path.getsize(os.path.join(root, name)) for name in files)
             total += sz
-            LOG.debug('%d bytes in %s' % (sz, root))
+            LOG.debug("%d bytes in %s" % (sz, root))
 
         return total
 
-    def _all_product_uuids(self):
+    def _all_product_uuids(self) -> list:
         with self._inventory as s:
             return [q.uuid for q in s.query(Product).all()]
 
     # ----------------------------------------------------------------------
-    def get_info(self, dsi_or_uuid, lod=None):
+    def get_info(self, info_or_uuid, lod=None) -> Optional[frozendict]:
         """
-        :param dsi_or_uuid: existing datasetinfo dictionary, or its UUID
+        :param info_or_uuid: existing datasetinfo dictionary, or its UUID
         :param lod: desired level of detail to focus
         :return: metadata access with mapping semantics, to be treated as read-only
         """
         from collections import ChainMap
+
         # FUTURE deprecate this
-        if isinstance(dsi_or_uuid, str):
-            uuid = UUID(dsi_or_uuid)
-        elif not isinstance(dsi_or_uuid, UUID):
-            uuid = dsi_or_uuid[Info.UUID]
+        if isinstance(info_or_uuid, str):
+            uuid = UUID(info_or_uuid)
+        elif not isinstance(info_or_uuid, UUID):
+            uuid = info_or_uuid[Info.UUID]
         else:
-            uuid = dsi_or_uuid
+            uuid = info_or_uuid
         with self._inventory as s:
             # look up the product for that uuid
             prod = self._product_with_uuid(s, uuid)
             if not prod:  # then it hasn't had its metadata scraped
-                LOG.error('no info available for UUID {}'.format(dsi_or_uuid))
+                LOG.error("no info available for UUID {}".format(info_or_uuid))
                 LOG.error("known products: {}".format(repr(self._all_product_uuids())))
                 return None
             kind = prod.info[Info.KIND]
             native_content = self._product_native_content(s, prod=prod, kind=kind)
 
             if native_content is not None:
                 # FUTURE: this is especially saddening; upgrade to finer grained query and/or deprecate .get_info
                 # once upon a time...
-                # our old model was that product == content and shares a UUID with the layer
+                # our old model was that product == content and shares a UUID with the dataset
                 # if content is available, we want to provide native content metadata along with the product metadata
                 # specifically a lot of client code assumes that resource == product == content and
                 # that singular navigation (e.g. cell_size) is norm
-                assert (native_content.info[Info.CELL_WIDTH] is not None)  # FIXME DEBUG
+                # FIXME DEBUG <- since commit 3576ff0122bd534f83422ce19479d40b7dc9e5b0
+                assert (  # nosec B101
+                    kind in [Kind.LINES, Kind.POINTS] or native_content.info[Info.CELL_WIDTH] is not None
+                )
                 return frozendict(ChainMap(native_content.info, prod.info))
             # mapping semantics for database fields, as well as key-value fields;
             # flatten to one namespace and read-only
             return frozendict(prod.info)
 
-    def get_algebraic_namespace(self, uuid):
-        if uuid is None:
-            return {}, ""
-
-        with self._inventory as s:
-            prod = self._product_with_uuid(s, uuid)
-            if prod is None:
-                return {}, ""
-            symbols = {x.key: x.value for x in prod.symbol}
-            code = prod.expression
-        return symbols, code
-
-    def _check_cache(self, path):
-        """
-        FIXME: does not work if more than one product inside a path
-        :param path: file we're checking
-        :return: uuid, info, overview_content if the data is already available without import
-        """
-        with self._inventory as s:
-            hits = s.query(Resource).filter_by(path=path).all()
-            if not hits:
-                return None
-            if len(hits) >= 1:
-                if len(hits) > 1:
-                    LOG.warning('more than one Resource found suitable, there can be only one')
-                resource = hits[0]
-                hits = list(s.query(Content).filter(
-                    Content.product_id == Product.id).filter(
-                    Product.resource_id == resource.id).order_by(
-                    Content.lod).all())
-                if len(hits) >= 1:
-                    content = hits[0]  # presumably this is closest to LOD_OVERVIEW
-                    # if len(hits)>1:
-                    #     LOG.warning('more than one Content found suitable, there can be only one')
-                    cac = self._cached_arrays_for_content(content)
-                    if not cac:
-                        LOG.error('unable to attach content')
-                        data = None
-                    else:
-                        data = cac.data
-                    return content.product.uuid, content.product.info, data
-
     @property
-    def product_names_available_in_cache(self):
+    def product_names_available_in_cache(self) -> dict:
         """
         Returns: dictionary of {UUID: product name,...}
         typically used for add-from-cache dialog
         """
         # find non-overview non-auxiliary data files
         # FIXME: also need to include coverage and sparsity paths?? really?
         zult = {}
         with self._inventory as s:
             for c in s.query(Content).order_by(Content.atime.desc()).all():
                 p = c.product
                 if p.uuid not in zult:
                     zult[p.uuid] = p.info[Info.DISPLAY_NAME]
         return zult
 
-    @property
-    def uuids_in_cache(self):
-        with self._inventory as s:
-            contents_of_cache = s.query(Content).all()
-            return list(sorted(set(c.product.uuid for c in contents_of_cache)))
-
     def recently_used_products(self, n=32) -> Dict[UUID, str]:
         with self._inventory as s:
-            return OrderedDict((p.uuid, p.info[Info.DISPLAY_NAME])
-                               for p in s.query(Product).order_by(Product.atime.desc()).limit(n).all())
+            return OrderedDict(
+                (p.uuid, p.info[Info.DISPLAY_NAME])
+                for p in s.query(Product).order_by(Product.atime.desc()).limit(n).all()
+            )
 
     def _purge_content_for_resource(self, resource: Resource, session, defer_commit=False):
         """
         remove all resource contents from the database
         if the resource original path no longer exists, also purge resource and products from database
         :param resource: resource object we
         :return: number of bytes freed from the workspace
@@ -738,24 +422,15 @@
 
         if not resource.exists():  # then purge the resource and its products as well
             S.delete(resource)
         if not defer_commit:
             S.commit()
         return total
 
-    def remove_all_workspace_content_for_resource_paths(self, paths):
-        total = 0
-        with self._inventory as s:
-            for path in paths:
-                rsr_hits = s.query(Resource).filter_by(path=path).all()
-                for rsr in rsr_hits:
-                    total += self._purge_content_for_resource(rsr, defer_commit=True)
-        return total
-
-    def purge_content_for_product_uuids(self, uuids, also_products=False):
+    def purge_content_for_product_uuids(self, uuids: list, also_products=False):
         """
         given one or more product uuids, purge the Content from the cache
         Note: this does not purge any ActiveContent that may still be using the files, but the files will be gone
         Args:
             uuids:
 
         Returns:
@@ -784,44 +459,31 @@
         possibly include a workspace setting for max workspace size in bytes?
         :return:
         """
         # get information on current cache contents
         with self._inventory as S:
             LOG.info("cleaning cache")
             total_size = self._total_workspace_bytes
-            GB = 1024 ** 3
+            GB = 1024**3
             LOG.info("total cache size is {}GB of max {}GB".format(total_size / GB, self._max_size_gb))
             max_size = self._max_size_gb * GB
             for res in S.query(Resource).order_by(Resource.atime).all():
                 if total_size < max_size:
                     break
                 total_size -= self._purge_content_for_resource(res, session=S)
                 # remove all content for lowest atimes until
 
     def close(self):
         self._clean_cache()
-        # self._S.commit()
 
     def bgnd_task_complete(self):
         """
         handle operations that should be done at the end of a threaded background task
         """
         pass
-        # self._S.commit()
-        # self._S.remove()
-
-    def idle(self):
-        """Called periodically when application is idle.
-
-        Does a clean-up tasks and returns True if more needs to be done later.
-        Time constrained to ~0.1s.
-        :return: True/False, whether or not more clean-up needs to be scheduled.
-
-        """
-        return False
 
     def get_metadata(self, uuid_or_path):
         """
         return metadata dictionary for a given product or the product being offered by a resource path (see get_info)
         Args:
             uuid_or_path: product uuid, or path to the resource path it lives in
 
@@ -834,526 +496,246 @@
         else:
             with self._inventory as s:
                 hits = list(s.query(Resource).filter_by(path=uuid_or_path).all())
                 if not hits:
                     return None
                 if len(hits) >= 1:
                     if len(hits) > 1:
-                        raise EnvironmentError('more than one Resource fits this path')
+                        raise EnvironmentError("more than one Resource fits this path")
                     resource = hits[0]
                     if len(resource.product) >= 1:
                         if len(resource.product) > 1:
-                            LOG.warning('more than one Product in this Resource, this query should be deprecated')
+                            LOG.warning("more than one Product in this Resource, this query should be deprecated")
                         prod = resource.product[0]
                         return prod.info
 
-    def collect_product_metadata_for_paths(self, paths: list,
-                                           **importer_kwargs) -> Generator[Tuple[int, frozendict], None, None]:
+    def collect_product_metadata_for_paths(
+        self, paths: list, **importer_kwargs
+    ) -> Generator[Tuple[int, frozendict], None, None]:
         """Start loading URI data into the workspace asynchronously.
 
         Args:
             paths (list): String paths to open and get metadata for
             **importer_kwargs: Keyword arguments to pass to the lower-level
                 importer class.
 
         Returns: sequence of read-only info dictionaries
 
         """
         with self._inventory as import_session:
             # FUTURE: consider returning importers instead of products,
             # since we can then re-use them to import the content instead of having to regenerate
-            # import_session = self._S
             importers = []
             num_products = 0
-            remaining_paths = []
-            if 'reader' in importer_kwargs:
-                # skip importer guessing and go straight to satpy importer
-                paths, remaining_paths = [], paths
-
-            for source_path in paths:
-                # LOG.info('collecting metadata for {}'.format(source_path))
-                # FIXME: Check if importer only accepts one path at a time
-                #        Maybe sort importers by single files versus multiple files and doing single files first?
-                # FIXME: decide whether to update database if mtime of file is newer than mtime in database
-                # Collect all the importers we are going to use and count
-                # how many products each expects to return
-                for imp in self._importers:
-                    if imp.is_relevant(source_path=source_path):
-                        hauler = imp(source_path,
-                                     database_session=import_session,
-                                     workspace_cwd=self.cache_dir,
-                                     **importer_kwargs)
-                        hauler.merge_resources()
-                        importers.append(hauler)
-                        num_products += hauler.num_products
-                        break
-                else:
-                    remaining_paths.append(source_path)
+            if "reader" not in importer_kwargs:
+                # If there is no reader in the importer_kwargs then the SatPy Import can't be used
+                return None
 
-            # Pass remaining paths to SatPy importer and see what happens
-            if remaining_paths:
-                if 'reader' not in importer_kwargs:
-                    raise NotImplementedError("Reader discovery is not "
-                                              "currently implemented in "
-                                              "the satpy importer.")
-                if 'scenes' in importer_kwargs:
+            # Pass paths to SatPy importer and see what happens
+            if paths:
+                if "reader" not in importer_kwargs:
+                    raise NotImplementedError(
+                        "Reader discovery is not " "currently implemented in " "the satpy importer."
+                    )
+                if "scenes" in importer_kwargs:
                     # another component already created the satpy scenes, use those
-                    scenes = importer_kwargs.pop('scenes')
+                    scenes = importer_kwargs.pop("scenes")
                     scenes = scenes.items()
                 else:
-                    scenes = [(remaining_paths, None)]
+                    scenes = [(paths, None)]
                 for paths, scene in scenes:
                     imp = SatpyImporter
                     these_kwargs = importer_kwargs.copy()
-                    these_kwargs['scene'] = scene
-                    hauler = imp(paths,
-                                 database_session=import_session,
-                                 workspace_cwd=self.cache_dir,
-                                 **these_kwargs)
+                    these_kwargs["scene"] = scene
+                    hauler = imp(paths, database_session=import_session, workspace_cwd=self.cache_dir, **these_kwargs)
                     hauler.merge_resources()
                     importers.append(hauler)
                     num_products += hauler.num_products
 
             for hauler in importers:
                 for prod in hauler.merge_products():
-                    assert (prod is not None)
+                    assert prod is not None  # nosec B101
                     # merge the product into our database session, since it may belong to import_session
-                    zult = frozendict(prod.info)  # self._S.merge(prod)
-                    # LOG.debug('yielding product metadata for {}'.format(
-                    #     zult.get(Info.DISPLAY_NAME, '?? unknown name ??')))
+                    zult = frozendict(prod.info)
                     yield num_products, zult
 
-    def import_product_content(self, uuid=None, prod=None, allow_cache=True, **importer_kwargs):
+    def import_product_content(
+        self,
+        uuid: UUID,
+        prod: Optional[Product] = None,
+        allow_cache=True,
+        merge_target_uuid: Optional[UUID] = None,
+        **importer_kwargs,
+    ) -> np.memmap:
         with self._inventory as S:
-            # S = self._S
             if prod is None and uuid is not None:
                 prod = self._product_with_uuid(S, uuid)
 
             self.set_product_state_flag(prod.uuid, State.ARRIVING)
             default_prod_kind = prod.info[Info.KIND]
 
             if len(prod.content):
-                LOG.info('product already has content available, using that rather than re-importing')
+                LOG.info("product already has content available, using that rather than re-importing")
                 ovc = self._product_overview_content(S, uuid=uuid, kind=default_prod_kind)
-                assert (ovc is not None)
+                assert ovc is not None  # nosec B101
                 arrays = self._cached_arrays_for_content(ovc)
                 return arrays.data
 
             truck = aImporter.from_product(prod, workspace_cwd=self.cache_dir, database_session=S, **importer_kwargs)
+            if not truck:
+                # aImporter.from_product() didn't return an Importer instance
+                # since all files represent data granules, which are already
+                # loaded and merged into existing datasets.
+                # Thus: nothing to do.
+                return None
             metadata = prod.info
             name = metadata[Info.SHORT_NAME]
 
             # FIXME: for now, just iterate the incremental load.
             #  later we want to add this to TheQueue and update the UI as we get more data loaded
             gen = truck.begin_import_products(prod.id)
             nupd = 0
             for update in gen:
                 nupd += 1
                 # we're now incrementally reading the input file
                 # data updates are coming back to us (eventually asynchronously)
                 # Content is in the metadatabase and being updated + committed, including sparsity and coverage arrays
                 if update.data is not None:
-                    # data = update.data
                     LOG.info("{} {}: {:.01f}%".format(name, update.stage_desc, update.completion * 100.0))
-            # self._data[uuid] = data = self._convert_to_memmap(str(uuid), data)
-            LOG.debug('received {} updates during import'.format(nupd))
+            LOG.debug("received {} updates during import".format(nupd))
             uuid = prod.uuid
-            self.clear_product_state_flag(prod.uuid, State.ARRIVING)
-        # S.commit()
-        # S.flush()
+            self._clear_product_state_flag(prod.uuid, State.ARRIVING)
 
         # make an ActiveContent object from the Content, now that we've imported it
         ac = self._overview_content_for_uuid(uuid, kind=default_prod_kind)
+        if ac is None:
+            return None
         return ac.data
 
-    def create_composite(self, symbols: dict, relation: dict):
-        """
-        create a layer composite in the workspace
-        :param symbols: dictionary of logical-name to uuid
-        :param relation: dictionary with information on how the relation is calculated (FUTURE)
-        """
-        raise NotImplementedError()
-
-    # def _preferred_cache_path(self, uuid):
-    #     filename = str(uuid)
-    #     return self._ws_path(filename)
-    #
-    # def _convert_to_memmap(self, uuid, data:np.ndarray):
-    #     if isinstance(data, np.memmap):
-    #         return data
-    #     # from tempfile import TemporaryFile
-    #     # fp = TemporaryFile()
-    #     pathname = self._preferred_cache_path(uuid)
-    #     fp = open(pathname, 'wb+')
-    #     mm = np.memmap(fp, dtype=data.dtype, shape=data.shape, mode='w+')
-    #     mm[:] = data[:]
-    #     return mm
-
-    @staticmethod
-    def _merge_famcat_strings(md_list, key, suffix=None):
-        zult = []
-        splatter = [md[key].split(':') for md in md_list]
-        for pieces in zip(*splatter):
-            uniq = set(pieces)
-            zult.append(','.join(sorted(uniq)))
-        if suffix:
-            zult.append(suffix)
-        return ':'.join(zult)
-
-    def _get_composite_metadata(self, info, md_list, composite_array):
-        """Combine composite dependency metadata in a logical way.
-
-        Args:
-            info: initial metadata for the composite
-            md_list: list of metadata dictionaries for each input
-            composite_array: array representing the final data values of the
-                             composite for valid min/max calculations
-
-        Returns: dict of overall metadata (same as `info`)
-
-        """
-        if not all(x[Info.PROJ] == md_list[0][Info.PROJ] for x in md_list[1:]):
-            raise ValueError("Algebraic inputs must all be the same projection")
-
-        uuid = uuidgen()
-        info[Info.UUID] = uuid
-        for k in (Info.PLATFORM, Info.INSTRUMENT, Info.SCENE):
-            if md_list[0].get(k) is None:
-                continue
-            if all(x.get(k) == md_list[0].get(k) for x in md_list[1:]):
-                info.setdefault(k, md_list[0][k])
-        info.setdefault(Info.KIND, Kind.COMPOSITE)
-        info.setdefault(Info.SHORT_NAME, '<unknown>')
-        info.setdefault(Info.DATASET_NAME, info[Info.SHORT_NAME])
-        info.setdefault(Info.UNITS, '1')
-
-        max_meta = max(md_list, key=lambda x: x[Info.SHAPE])
-        for k in (Info.PROJ, Info.ORIGIN_X, Info.ORIGIN_Y, Info.CELL_WIDTH, Info.CELL_HEIGHT, Info.SHAPE):
-            info[k] = max_meta[k]
-
-        info[Info.VALID_RANGE] = (np.nanmin(composite_array), np.nanmax(composite_array))
-        info[Info.CLIM] = (np.nanmin(composite_array), np.nanmax(composite_array))
-        info[Info.OBS_TIME] = min([x[Info.OBS_TIME] for x in md_list])
-        info[Info.SCHED_TIME] = min([x[Info.SCHED_TIME] for x in md_list])
-        # get the overall observation time
-        info[Info.OBS_DURATION] = max([
-            x[Info.OBS_TIME] + x.get(Info.OBS_DURATION, timedelta(seconds=0)) for x in md_list]) - info[Info.OBS_TIME]
-
-        # generate family and category names
-        info[Info.FAMILY] = family = self._merge_famcat_strings(md_list, Info.FAMILY, suffix=info.get(Info.SHORT_NAME))
-        info[Info.CATEGORY] = category = self._merge_famcat_strings(md_list, Info.CATEGORY)
-        info[Info.SERIAL] = serial = self._merge_famcat_strings(md_list, Info.SERIAL)
-        LOG.debug("algebraic product will be {}::{}::{}".format(family, category, serial))
-
-        return info
-
-    def create_algebraic_composite(self, operations, namespace, info=None):
-        if not info:
-            info = {}
-
-        import ast
-        try:
-            ops_ast = ast.parse(operations, mode='exec')
-            ops = compile(ast.parse(operations, mode='exec'), '<string>', 'exec')
-            result_name = ops_ast.body[-1].targets[0].id
-        except SyntaxError:
-            raise ValueError("Invalid syntax or operations in algebraic layer")
-
-        dep_metadata = {n: self.get_metadata(u) for n, u in namespace.items() if isinstance(u, UUID)}
-
-        # Get every combination of the valid mins and maxes
-        # See: https://stackoverflow.com/a/35608701/433202
-        names = list(dep_metadata.keys())
-        try:
-            valid_combos = np.array(np.meshgrid(*tuple(dep_metadata[n][Info.VALID_RANGE] for n in names))).reshape(
-                len(names), -1)
-        except KeyError:
-            badboys = [n for n in names if Info.VALID_RANGE not in dep_metadata[n]]
-            LOG.error("missing VALID_RANGE for: {}".format(repr([dep_metadata[n][Info.DISPLAY_NAME] for n in badboys])))
-            LOG.error("witness sample: {}".format(repr(dep_metadata[badboys[0]])))
-            raise
-        valids_namespace = {n: valid_combos[idx] for idx, n in enumerate(names)}
-        content = {n: self.get_content(m[Info.UUID]) for n, m in dep_metadata.items()}
-
-        # Get all content in the same shape
-        max_shape = max(x[Info.SHAPE] for x in dep_metadata.values())
-        for k, v in content.items():
-            if v.shape != max_shape:
-                f0 = int(max_shape[0] / v.shape[0])
-                f1 = int(max_shape[1] / v.shape[1])
-                v = np.ma.repeat(np.ma.repeat(v, f0, axis=0), f1, axis=1)
-                content[k] = v
-
-        # Run the code: code_object, no globals, copy of locals
-        exec(ops, None, valids_namespace)
-        if result_name not in valids_namespace:
-            raise RuntimeError("Unable to retrieve result '{}' from code execution".format(result_name))
-
-        exec(ops, None, content)
-        if result_name not in content:
-            raise RuntimeError("Unable to retrieve result '{}' from code execution".format(result_name))
-        info = self._get_composite_metadata(info, list(dep_metadata.values()), valids_namespace[result_name])
-        # update the shape
-        # NOTE: This doesn't work if the code changes the shape of the array
-        # Need to update geolocation information too
-        # info[Info.SHAPE] = content[result_name].shape
-
-        info = generate_guidebook_metadata(info)
-
-        uuid, info, data = self._create_product_from_array(info, content[result_name],
-                                                           namespace=namespace,
-                                                           codeblock=operations)
-        return uuid, info, data
-
-    def _create_product_from_array(self, info, data, namespace=None, codeblock=None):
+    def _create_product_from_array(
+        self, info: Mapping, data, namespace=None, codeblock=None
+    ) -> Tuple[UUID, Optional[frozendict], np.memmap]:
         """
         update metadatabase to include Product and Content entries for this new dataset we've calculated
         this allows the calculated data to reside in the workspace
         then return the "official" versions consistent with workspace product/content database
         Args:
             info: mapping of key-value metadata for new product
             data: ndarray with content to store, typically 2D float32
             namespace: {variable: uuid, } for calculation of this data
             codeblock: text, code to run to recalculate this data within namespace
 
         Returns:
             uuid, info, data: uuid of the new product, its official read-only metadata, and cached content ndarray
         """
         if Info.UUID not in info:
-            raise ValueError('currently require an Info.UUID be included in product')
+            raise ValueError("currently require an Info.UUID be included in product")
         parms = dict(info)
         now = datetime.utcnow()
-        parms.update(dict(
-            atime=now,
-            mtime=now,
-        ))
+        parms.update(
+            dict(
+                atime=now,
+                mtime=now,
+            )
+        )
         P = Product.from_info(parms, symbols=namespace, codeblock=codeblock)
         uuid = P.uuid
         # FUTURE: add expression and namespace information, which would require additional parameters
-        ws_filename = '{}.image'.format(str(uuid))
+        ws_filename = "{}.image".format(str(uuid))
         ws_path = os.path.join(self.cache_dir, ws_filename)
-        with open(ws_path, 'wb+') as fp:
-            mm = np.memmap(fp, dtype=data.dtype, shape=data.shape, mode='w+')
+        with open(ws_path, "wb+") as fp:
+            mm = np.memmap(fp, dtype=data.dtype, shape=data.shape, mode="w+")
             mm[:] = data[:]
 
-        parms.update(dict(
-            lod=Content.LOD_OVERVIEW,
-            path=ws_filename,
-            dtype=str(data.dtype),
-            proj4=info[Info.PROJ],
-            resolution=min(info[Info.CELL_WIDTH], info[Info.CELL_HEIGHT])
-        ))
-        rcls = dict(zip(('rows', 'cols', 'levels'), data.shape))
+        parms.update(
+            dict(
+                lod=ContentImage.LOD_OVERVIEW,
+                path=ws_filename,
+                dtype=str(data.dtype),
+                proj4=info[Info.PROJ],
+                resolution=min(info[Info.CELL_WIDTH], info[Info.CELL_HEIGHT]),
+            )
+        )
+        rcls = dict(zip(("rows", "cols", "levels"), data.shape))
         parms.update(rcls)
         LOG.debug("about to create Content with this: {}".format(repr(parms)))
 
-        C = Content.from_info(parms, only_fields=True)
+        C = ContentImage.from_info(parms, only_fields=True)
         P.content.append(C)
         # FUTURE: do we identify a Resource to go with this? Probably not
 
         # transaction with the metadatabase to add the product and content
         with self._inventory as S:
             S.add(P)
             S.add(C)
 
         # FIXME: Do I have to flush the session so the Product gets added for sure?
 
         # activate the content we just loaded into the workspace
         overview_data = self._overview_content_for_uuid(uuid)
-        # prod = self._product_with_uuid(S, uuid)
         return uuid, self.get_info(uuid), overview_data
 
-    def _bgnd_remove(self, uuid):
+    def _bgnd_remove(self, uuid: UUID):
         from uwsift.queue import TASK_DOING, TASK_PROGRESS
-        yield {TASK_DOING: 'purging memory', TASK_PROGRESS: 0.5}
+
+        yield {TASK_DOING: "purging memory", TASK_PROGRESS: 0.5}
+        LOG.debug(f"Active Content before deletion: {list(self._available.keys())}")
         with self._inventory as s:
             self._deactivate_content_for_product(self._product_with_uuid(s, uuid))
-        yield {TASK_DOING: 'purging memory', TASK_PROGRESS: 1.0}
-
-    def remove(self, dsi):
-        """Formally detach a dataset.
-
-        Removing its content from the workspace fully by the time that idle() has nothing more to do.
-
-        :param dsi: datasetinfo dictionary or UUID of a dataset
-        :return: True if successfully deleted, False if not found
-        """
-        uuid = dsi if isinstance(dsi, UUID) else dsi[Info.UUID]
-
-        if self._queue is not None:
-            self._queue.add(str(uuid), self._bgnd_remove(uuid), 'Purge dataset')
-        else:
-            # iterate over generator
-            list(self._bgnd_remove(uuid))
-        return True
+        LOG.debug(f"Active Content after deletion: {list(self._available.keys())}")
+        yield {TASK_DOING: "purging memory", TASK_PROGRESS: 1.0}
 
-    def get_content(self, dsi_or_uuid, lod=None, kind=Kind.IMAGE):
+    def get_content(self, info_or_uuid, lod=None, kind: Kind = Kind.IMAGE) -> Optional[np.memmap]:
         """
         By default, get the best-available (closest to native) np.ndarray-compatible view of the full dataset
-        :param dsi_or_uuid: existing datasetinfo dictionary, or its UUID
+        :param info_or_uuid: existing datasetinfo dictionary, or its UUID
         :param lod: desired level of detail to focus  (0 for overview)
         :return:
         """
-        if dsi_or_uuid is None:
+        if info_or_uuid is None:
             return None
-        elif isinstance(dsi_or_uuid, UUID):
-            uuid = dsi_or_uuid
-        elif isinstance(dsi_or_uuid, str):
-            uuid = UUID(dsi_or_uuid)
+        elif isinstance(info_or_uuid, UUID):
+            uuid = info_or_uuid
+        elif isinstance(info_or_uuid, str):
+            uuid = UUID(info_or_uuid)
         else:
-            uuid = dsi_or_uuid[Info.UUID]
-        # prod = self._product_with_uuid(dsi_or_uuid)
+            uuid = info_or_uuid[Info.UUID]
         # TODO: this causes a locking exception when run in a secondary thread.
         #  Keeping background operations lightweight makes sense however, so just review this
-        # prod.touch()
         with self._inventory as s:
-            content = s.query(Content).filter(
-                (Product.uuid_str == str(uuid)) & (Content.product_id == Product.id)).order_by(Content.lod.desc()).all()
+            if kind == Kind.IMAGE:
+                content = (
+                    s.query(ContentImage)
+                    .filter((Product.uuid_str == str(uuid)) & (ContentImage.product_id == Product.id))
+                    .order_by(ContentImage.lod.desc())
+                    .all()
+                )
+            else:
+                content = s.query(Content).filter((Product.uuid_str == str(uuid)) & (Content.product_id == Product.id))
+
             content = [x for x in content if x.info.get(Info.KIND, Kind.IMAGE) == kind]
             if len(content) != 1:
-                LOG.warning("More than one matching Content object for '{}'".format(dsi_or_uuid))
+                LOG.warning("More than one matching Content object for '{}'".format(info_or_uuid))
             if not len(content) or not content[0]:
-                raise AssertionError('no content in workspace for {}, must re-import'.format(uuid))
+                raise AssertionError("no content in workspace for {}, must re-import".format(uuid))
             content = content[0]
-            # content.touch()
-            # self._S.commit()  # flush any pending updates to workspace db file
 
-            # FIXME: find the content for the requested LOD, then return its ActiveContent - or attach one
-            # for now, just work with assumption of one product one content
+            # FIXME: find the content for the requested LOD, then return its
+            #  ActiveContent - or attach one
+            #  for now, just work with assumption of one product one content
             active_content = self._cached_arrays_for_content(content)
             return active_content.data
 
-    def _create_position_to_index_transform(self, dsi_or_uuid):
-        info = self.get_info(dsi_or_uuid)
-        origin_x = info[Info.ORIGIN_X]
-        origin_y = info[Info.ORIGIN_Y]
-        cell_width = info[Info.CELL_WIDTH]
-        cell_height = info[Info.CELL_HEIGHT]
-
-        def _transform(x, y, origin_x=origin_x, origin_y=origin_y, cell_width=cell_width, cell_height=cell_height):
-            col = (x - info[Info.ORIGIN_X]) / info[Info.CELL_WIDTH]
-            row = (y - info[Info.ORIGIN_Y]) / info[Info.CELL_HEIGHT]
-            return col, row
-
-        return _transform
-
-    def _create_layer_affine(self, dsi_or_uuid):
-        info = self.get_info(dsi_or_uuid)
-        affine = Affine(
-            info[Info.CELL_WIDTH],
-            0.0,
-            info[Info.ORIGIN_X],
-            0.0,
-            info[Info.CELL_HEIGHT],
-            info[Info.ORIGIN_Y],
-        )
-        return affine
-
-    def _position_to_index(self, dsi_or_uuid, xy_pos):
-        info = self.get_info(dsi_or_uuid)
-        if info is None:
-            return None, None
-        # Assume `xy_pos` is lon/lat value
-        if '+proj=latlong' in info[Info.PROJ]:
-            x, y = xy_pos[:2]
-        else:
-            x, y = Proj(info[Info.PROJ])(*xy_pos)
-        col = (x - info[Info.ORIGIN_X]) / info[Info.CELL_WIDTH]
-        row = (y - info[Info.ORIGIN_Y]) / info[Info.CELL_HEIGHT]
-
-        return np.int64(np.floor(row)), np.int64(np.floor(col))
-
-    def layer_proj(self, dsi_or_uuid):
-        """Project lon/lat probe points to image X/Y"""
-        info = self.get_info(dsi_or_uuid)
-        return Proj(info[Info.PROJ])
-
-    def _project_points(self, p, points):
-        points = np.array(points)
-        points[:, 0], points[:, 1] = p(points[:, 0], points[:, 1])
-        return points
-
-    def get_content_point(self, dsi_or_uuid, xy_pos):
-        row, col = self._position_to_index(dsi_or_uuid, xy_pos)
-        if row is None or col is None:
-            return None
-        data = self.get_content(dsi_or_uuid)
-        if not ((0 <= col < data.shape[1]) and (0 <= row < data.shape[0])):
-            raise ValueError("X/Y position is outside of image with UUID: %s", dsi_or_uuid)
-        return data[row, col]
-
-    def get_content_polygon(self, dsi_or_uuid, points):
-        data = self.get_content(dsi_or_uuid)
-        trans = self._create_layer_affine(dsi_or_uuid)
-        p = self.layer_proj(dsi_or_uuid)
-        points = self._project_points(p, points)
-        _, data = content_within_shape(data, trans, LinearRing(points))
-        return data
-
-    def highest_resolution_uuid(self, *uuids):
-        return min([self.get_info(uuid) for uuid in uuids], key=lambda i: i[Info.CELL_WIDTH])[Info.UUID]
-
-    def lowest_resolution_uuid(self, *uuids):
-        return max([self.get_info(uuid) for uuid in uuids], key=lambda i: i[Info.CELL_WIDTH])[Info.UUID]
-
-    def get_coordinate_mask_polygon(self, dsi_or_uuid, points):
-        data = self.get_content(dsi_or_uuid)
-        trans = self._create_layer_affine(dsi_or_uuid)
-        p = self.layer_proj(dsi_or_uuid)
-        points = self._project_points(p, points)
-        index_mask, data = content_within_shape(data, trans, LinearRing(points))
-        coords_mask = (index_mask[0] * trans.e + trans.f, index_mask[1] * trans.a + trans.c)
-        # coords_mask is (Y, X) corresponding to (rows, cols) like numpy
-        coords_mask = p(coords_mask[1], coords_mask[0], inverse=True)[::-1]
-        return coords_mask, data
-
-    def get_content_coordinate_mask(self, uuid, coords_mask):
-        data = self.get_content(uuid)
-        trans = self._create_layer_affine(uuid)
-        p = self.layer_proj(uuid)
-        # coords_mask is (Y, X) like a numpy array
-        coords_mask = p(coords_mask[1], coords_mask[0])[::-1]
-        index_mask = (
-            np.round((coords_mask[0] - trans.f) / trans.e).astype(np.uint),
-            np.round((coords_mask[1] - trans.c) / trans.a).astype(np.uint),
-        )
-        return data[index_mask]
+    def _deactivate_content_for_product(self, p: Optional[Product]):
+        if p is None:
+            return
+        for c in p.content:
+            self._available.pop(c.id, None)
 
-    def get_pyresample_area(self, uuid, y_slice=None, x_slice=None):
-        """Create a pyresample compatible AreaDefinition for this layer."""
-        # WARNING: Untested!
-        from pyresample.geometry import AreaDefinition
-        from pyresample.utils import proj4_str_to_dict
-        info = self.get_info(uuid)
-        if y_slice is None:
-            y_slice = slice(None, None, None)
-        if x_slice is None:
-            x_slice = slice(None, None, None)
-        if y_slice.step not in [1, None] or x_slice.step not in [1, None]:
-            raise ValueError("Slice steps other than 1 are not supported")
-        rows, cols = info[Info.SHAPE]
-        x_start = x_slice.start or 0
-        y_start = y_slice.start or 0
-        num_cols = (x_slice.stop or cols) - x_start
-        num_rows = (y_slice.stop or rows) - y_start
-        half_x = info[Info.CELL_WIDTH] / 2.
-        half_y = info[Info.CELL_HEIGHT] / 2.
-        min_x = info[Info.ORIGIN_X] - half_x + x_start * info[Info.CELL_WIDTH]
-        max_y = info[Info.ORIGIN_Y] + half_y - y_start * info[Info.CELL_HEIGHT]
-        min_y = max_y - num_rows * info[Info.CELL_HEIGHT]
-        max_x = min_x + num_cols * info[Info.CELL_WIDTH]
-        return AreaDefinition(
-            'layer area',
-            'layer area',
-            'layer area',
-            proj4_str_to_dict(info[Info.PROJ]),
-            cols, rows,
-            (min_x, min_y, max_x, max_y),
-        )
+    def _get_active_content_by_uuid(self, uuid: UUID) -> Optional[ActiveContent]:
+        with self._inventory as s:
+            prod = self._product_with_uuid(s, uuid)
+            if prod is None:
+                return None
+            content = s.query(Content).filter(Content.product_id == prod.id).one()
 
-    def __getitem__(self, datasetinfo_or_uuid):
-        """
-        return science content proxy capable of generating a numpy array when sliced
-        :param datasetinfo_or_uuid: metadata or key for the dataset
-        :return: sliceable object returning numpy arrays
-        """
-        pass
+            return self._available.get(content.id)
```

### Comparing `uwsift-1.2.3/uwsift.egg-info/SOURCES.txt` & `uwsift-2.0.0b0/uwsift.egg-info/SOURCES.txt`

 * *Files 21% similar despite different names*

```diff
@@ -1,10 +1,12 @@
+AUTHORS.md
 LICENSE.txt
 MANIFEST.in
 README.md
+pyproject.toml
 setup.cfg
 setup.py
 uwsift/__init__.py
 uwsift/__main__.py
 uwsift/common.py
 uwsift/queue.py
 uwsift/satpy_compat.py
@@ -13,18 +15,20 @@
 uwsift.egg-info/SOURCES.txt
 uwsift.egg-info/dependency_links.txt
 uwsift.egg-info/entry_points.txt
 uwsift.egg-info/not-zip-safe
 uwsift.egg-info/requires.txt
 uwsift.egg-info/top_level.txt
 uwsift/control/__init__.py
-uwsift/control/doc_ws_as_timeline_scene.py
-uwsift/control/file_behaviors.py
-uwsift/control/layer_tree.py
-uwsift/control/rgb_behaviors.py
+uwsift/control/auto_update.py
+uwsift/control/qml_utils.py
+uwsift/control/time_matcher.py
+uwsift/control/time_matcher_policies.py
+uwsift/control/time_transformer.py
+uwsift/control/time_transformer_policies.py
 uwsift/data/shadedrelief.jpg
 uwsift/data/colormaps/OAX/Cloud Amount Default.cmap
 uwsift/data/colormaps/OAX/Cloud Top Height.cmap
 uwsift/data/colormaps/OAX/Low Cloud Base.cmap
 uwsift/data/colormaps/OAX/Rain Rate.cmap
 uwsift/data/colormaps/OAX/prob_severe.cmap
 uwsift/data/colormaps/OAX/GOES-R/GOESR-L2/ACTP.cmap
@@ -73,14 +77,15 @@
 uwsift/data/fonts/Andale Mono.ttf
 uwsift/data/grib_definitions/grib2/localConcepts/kwbc/cfName.def
 uwsift/data/grib_definitions/grib2/localConcepts/kwbc/modelName.def
 uwsift/data/grib_definitions/grib2/localConcepts/kwbc/name.def
 uwsift/data/grib_definitions/grib2/localConcepts/kwbc/paramId.def
 uwsift/data/grib_definitions/grib2/localConcepts/kwbc/shortName.def
 uwsift/data/grib_definitions/grib2/localConcepts/kwbc/units.def
+uwsift/data/icons/menu.svg
 uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.README.html
 uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.VERSION.txt
 uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.dbf
 uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.prj
 uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp
 uwsift/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shx
 uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.README.html
@@ -91,77 +96,133 @@
 uwsift/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shx
 uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.README.html
 uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.VERSION.txt
 uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.dbf
 uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.prj
 uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.shp
 uwsift/data/ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.shx
+uwsift/etc/SIFT/config/area_definitions.yaml
+uwsift/etc/SIFT/config/auto_update.yaml
+uwsift/etc/SIFT/config/catalogue.yaml
+uwsift/etc/SIFT/config/default_colormaps.yaml
+uwsift/etc/SIFT/config/default_points_styles.yaml
+uwsift/etc/SIFT/config/default_reader.yaml
+uwsift/etc/SIFT/config/default_standard_names.yaml
+uwsift/etc/SIFT/config/display.yaml
+uwsift/etc/SIFT/config/external_satpy_and_readers.yaml
+uwsift/etc/SIFT/config/limit_available_readers.yaml
+uwsift/etc/SIFT/config/logging.yaml
+uwsift/etc/SIFT/config/open_file_wizard.yaml
+uwsift/etc/SIFT/config/storage.yaml
+uwsift/etc/SIFT/config/units.yaml
+uwsift/etc/SIFT/config/watchdog.yaml
+uwsift/etc/SIFT/config/readers/abi_l1b.yaml
+uwsift/etc/SIFT/config/readers/avhrr_l1b_eps.yaml
+uwsift/etc/SIFT/config/readers/fci_l1_cat_lmk_loc.yaml
+uwsift/etc/SIFT/config/readers/fci_l1_geoobs_lmk_loc.yaml
+uwsift/etc/SIFT/config/readers/fci_l1_geoobs_lmk_nav_err.yaml
+uwsift/etc/SIFT/config/readers/fci_l1c_iqt_fdhsi.yaml
+uwsift/etc/SIFT/config/readers/fci_l1c_nc.yaml
+uwsift/etc/SIFT/config/readers/fci_l2_nc.yaml
+uwsift/etc/SIFT/config/readers/gld360_ualf2.yaml
+uwsift/etc/SIFT/config/readers/li_l1b_nc.yaml
+uwsift/etc/SIFT/config/readers/li_l2_nc.yaml
+uwsift/etc/SIFT/config/readers/lsasaf_l2_frppixel_hdf.yaml
+uwsift/etc/SIFT/config/readers/modis_l1b.yaml
+uwsift/etc/SIFT/config/readers/modis_l2_fire_hdf.yaml
+uwsift/etc/SIFT/config/readers/seviri_l1b_hrit.yaml
+uwsift/etc/SIFT/config/readers/seviri_l1b_native.yaml
+uwsift/etc/SIFT/config/readers/seviri_l2_binary.yaml
+uwsift/etc/SIFT/config/readers/seviri_l2_bufr.yaml
+uwsift/etc/SIFT/config/readers/seviri_l2_grib.yaml
+uwsift/etc/SIFT/config/readers/vii_l1b_nc.yaml
+uwsift/etc/SIFT/config/readers/viirs_l2_fire_nc.yaml
 uwsift/model/__init__.py
+uwsift/model/area_definitions_manager.py
+uwsift/model/catalogue.py
 uwsift/model/composite_recipes.py
 uwsift/model/document.py
-uwsift/model/layer.py
+uwsift/model/layer_item.py
+uwsift/model/layer_model.py
+uwsift/model/product_dataset.py
 uwsift/model/shapes.py
+uwsift/model/time_manager.py
 uwsift/project/__init__.py
-uwsift/project/ahi2gtiff.py
-uwsift/project/ahi2merc.py
-uwsift/project/geocat2merc.py
 uwsift/project/organize_data_bands.py
 uwsift/project/organize_data_topics.py
 uwsift/tests/__init__.py
 uwsift/tests/conftest.py
-uwsift/tests/timeline.py
+uwsift/tests/test_satpy_import.py
+uwsift/tests/control/__init__.py
+uwsift/tests/control/fill_dir_periodically.py
+uwsift/tests/model/__init__.py
+uwsift/tests/model/conftest.py
+uwsift/tests/model/interactive_test_globbing_creator.py
+uwsift/tests/model/test_globbing_creator.py
 uwsift/tests/view/__init__.py
-uwsift/tests/view/test_colormap_dialogs.py
 uwsift/tests/view/test_export_image.py
-uwsift/tests/view/test_open_file_wizard.py
 uwsift/tests/view/test_rgb_config.py
 uwsift/tests/view/test_tile_calculator.py
 uwsift/tests/workspace/__init__.py
 uwsift/tests/workspace/test_algebraic.py
 uwsift/tests/workspace/test_importer.py
+uwsift/tests/workspace/test_statistics.py
+uwsift/ui/TimelineRuler.qml
 uwsift/ui/__init__.py
-uwsift/ui/change_colormap_dialog.ui
+uwsift/ui/build.sh
 uwsift/ui/change_colormap_dialog_ui.py
-uwsift/ui/config_rgb_layer.ui
-uwsift/ui/config_rgb_layer_ui.py
-uwsift/ui/create_algebraic_dialog.ui
-uwsift/ui/create_algebraic_dialog_ui.py
 uwsift/ui/custom_widgets.py
+uwsift/ui/dataset_statistics_widget.ui
+uwsift/ui/dataset_statistics_widget_ui.py
 uwsift/ui/export_image_dialog.ui
 uwsift/ui/export_image_dialog_ui.py
+uwsift/ui/layer_details_widget.ui
+uwsift/ui/layer_details_widget_ui.py
 uwsift/ui/open_cache_dialog.ui
 uwsift/ui/open_cache_dialog_ui.py
 uwsift/ui/open_file_wizard.ui
 uwsift/ui/open_file_wizard_ui.py
 uwsift/ui/pov_main.ui
 uwsift/ui/pov_main_ui.py
+uwsift/ui/timeline.qml
 uwsift/util/__init__.py
+uwsift/util/common.py
 uwsift/util/default_paths.py
+uwsift/util/disk_management.py
+uwsift/util/heap_analyzer.py
+uwsift/util/heap_profiler.py
+uwsift/util/logger.py
+uwsift/util/ps_analyzer.py
+uwsift/util/ps_profiler.py
+uwsift/util/storage_agent.py
+uwsift/util/watchdog.py
+uwsift/util/widgets/__init__.py
+uwsift/util/widgets/pie_dial.py
 uwsift/view/__init__.py
+uwsift/view/algebraic_config.py
 uwsift/view/cameras.py
 uwsift/view/colormap.py
-uwsift/view/colormap_dialogs.py
 uwsift/view/colormap_editor.py
-uwsift/view/create_algebraic.py
+uwsift/view/dataset_statistics_pane.py
 uwsift/view/export_image.py
 uwsift/view/layer_details.py
+uwsift/view/layer_tree_view.py
 uwsift/view/open_file_wizard.py
 uwsift/view/probes.py
 uwsift/view/rgb_config.py
 uwsift/view/scene_graph.py
 uwsift/view/test_visuals.py
 uwsift/view/texture_atlas.py
 uwsift/view/tile_calculator.py
 uwsift/view/transform.py
 uwsift/view/visuals.py
-uwsift/view/timeline/__init__.py
-uwsift/view/timeline/common.py
-uwsift/view/timeline/items.py
-uwsift/view/timeline/scene.py
 uwsift/workspace/__init__.py
+uwsift/workspace/caching_workspace.py
 uwsift/workspace/collector.py
-uwsift/workspace/goesr_pug.py
 uwsift/workspace/guidebook.py
 uwsift/workspace/importer.py
-uwsift/workspace/matrix.py
 uwsift/workspace/metadatabase.py
-uwsift/workspace/workspace.py
+uwsift/workspace/simple_workspace.py
+uwsift/workspace/statistics.py
+uwsift/workspace/workspace.py
+uwsift/workspace/utils/__init__.py
+uwsift/workspace/utils/metadata_utils.py
```

