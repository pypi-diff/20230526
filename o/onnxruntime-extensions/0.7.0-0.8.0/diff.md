# Comparing `tmp/onnxruntime_extensions-0.7.0-cp39-cp39-win_amd64.whl.zip` & `tmp/onnxruntime_extensions-0.8.0-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,47 +1,48 @@
-Zip file size: 1642919 bytes, number of entries: 45
-drwx---     6.3 fat        0 bx stor 23-Mar-02 09:56 onnxruntime_extensions/
-drwx---     6.3 fat        0 bx stor 23-Mar-02 09:56 onnxruntime_extensions-0.7.0.dist-info/
--rw-rw-rw-  2.0 fat     1162 b- defN 23-Mar-02 09:11 onnxruntime_extensions-0.7.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     5326 b- defN 23-Mar-02 09:11 onnxruntime_extensions-0.7.0.dist-info/METADATA
-?rw-rw-r--  2.0 fat     3848 b- defN 23-Mar-02 09:11 onnxruntime_extensions-0.7.0.dist-info/RECORD
--rw-rw-rw-  2.0 fat       23 b- defN 23-Mar-02 08:58 onnxruntime_extensions-0.7.0.dist-info/top_level.txt
--rw-rw-rw-  2.0 fat      100 b- defN 23-Mar-02 09:11 onnxruntime_extensions-0.7.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat     1639 b- defN 23-Mar-02 08:26 onnxruntime_extensions/cmake_helper.py
--rw-rw-rw-  2.0 fat     1440 b- defN 23-Mar-02 08:26 onnxruntime_extensions/cmd.py
-drwx---     6.3 fat        0 bx stor 23-Mar-02 09:56 onnxruntime_extensions/onnxprocess/
--rw-rw-rw-  2.0 fat      859 b- defN 23-Mar-02 08:26 onnxruntime_extensions/onnxprocess/torch_wrapper.py
--rw-rw-rw-  2.0 fat     1823 b- defN 23-Mar-02 08:26 onnxruntime_extensions/onnxprocess/_builder.py
--rw-rw-rw-  2.0 fat    73255 b- defN 23-Mar-02 08:26 onnxruntime_extensions/onnxprocess/_onnx_ops.py
--rw-rw-rw-  2.0 fat    15158 b- defN 23-Mar-02 08:26 onnxruntime_extensions/onnxprocess/_session.py
--rw-rw-rw-  2.0 fat    25410 b- defN 23-Mar-02 08:26 onnxruntime_extensions/onnxprocess/_tensor.py
--rw-rw-rw-  2.0 fat      534 b- defN 23-Mar-02 08:26 onnxruntime_extensions/onnxprocess/__init__.py
-drwx---     6.3 fat        0 bx stor 23-Mar-02 09:56 onnxruntime_extensions/pnp/
--rw-rw-rw-  2.0 fat     3730 b- defN 23-Mar-02 08:26 onnxruntime_extensions/pnp/_base.py
--rw-rw-rw-  2.0 fat     2452 b- defN 23-Mar-02 08:26 onnxruntime_extensions/pnp/_imagenet.py
--rw-rw-rw-  2.0 fat     7420 b- defN 23-Mar-02 08:26 onnxruntime_extensions/pnp/_nlp.py
--rw-rw-rw-  2.0 fat    74284 b- defN 23-Mar-02 08:26 onnxruntime_extensions/pnp/_onnx_ops.py
--rw-rw-rw-  2.0 fat    11927 b- defN 23-Mar-02 08:26 onnxruntime_extensions/pnp/_torchext.py
--rw-rw-rw-  2.0 fat     1649 b- defN 23-Mar-02 08:26 onnxruntime_extensions/pnp/_unifier.py
--rw-rw-rw-  2.0 fat    12435 b- defN 23-Mar-02 08:26 onnxruntime_extensions/pnp/_utils.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-Mar-02 08:26 onnxruntime_extensions/pnp/__init__.py
-drwx---     6.3 fat        0 bx stor 23-Mar-02 09:56 onnxruntime_extensions/tools/
--rw-rw-rw-  2.0 fat    14768 b- defN 23-Mar-02 08:26 onnxruntime_extensions/tools/add_pre_post_processing_to_model.py
-drwx---     6.3 fat        0 bx stor 23-Mar-02 09:56 onnxruntime_extensions/tools/pre_post_processing/
--rw-rw-rw-  2.0 fat    19070 b- defN 23-Mar-02 08:26 onnxruntime_extensions/tools/pre_post_processing/pre_post_processor.py
--rw-rw-rw-  2.0 fat     9130 b- defN 23-Mar-02 08:26 onnxruntime_extensions/tools/pre_post_processing/step.py
-drwx---     6.3 fat        0 bx stor 23-Mar-02 09:56 onnxruntime_extensions/tools/pre_post_processing/steps/
--rw-rw-rw-  2.0 fat     9380 b- defN 23-Mar-02 08:26 onnxruntime_extensions/tools/pre_post_processing/steps/general.py
--rw-rw-rw-  2.0 fat    14727 b- defN 23-Mar-02 08:26 onnxruntime_extensions/tools/pre_post_processing/steps/nlp.py
--rw-rw-rw-  2.0 fat    25664 b- defN 23-Mar-02 08:26 onnxruntime_extensions/tools/pre_post_processing/steps/vision.py
--rw-rw-rw-  2.0 fat      165 b- defN 23-Mar-02 08:26 onnxruntime_extensions/tools/pre_post_processing/steps/__init__.py
--rw-rw-rw-  2.0 fat     5698 b- defN 23-Mar-02 08:26 onnxruntime_extensions/tools/pre_post_processing/utils.py
--rw-rw-rw-  2.0 fat      125 b- defN 23-Mar-02 08:26 onnxruntime_extensions/tools/pre_post_processing/__init__.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Mar-02 08:26 onnxruntime_extensions/tools/__init__.py
--rw-rw-rw-  2.0 fat      392 b- defN 23-Mar-02 08:26 onnxruntime_extensions/util.py
--rw-rw-rw-  2.0 fat    11562 b- defN 23-Mar-02 08:26 onnxruntime_extensions/_cuops.py
--rw-a--     6.3 fat  4206984 bx defN 23-Mar-02 09:58 onnxruntime_extensions/_extensions_pydll.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat     5777 b- defN 23-Mar-02 08:26 onnxruntime_extensions/_ocos.py
--rw-rw-rw-  2.0 fat     4902 b- defN 23-Mar-02 08:26 onnxruntime_extensions/_ortapi2.py
--rw-rw-rw-  2.0 fat       75 b- defN 23-Mar-02 08:58 onnxruntime_extensions/_version.py
--rw-rw-rw-  2.0 fat      926 b- defN 23-Mar-02 08:26 onnxruntime_extensions/__init__.py
-45 files, 4574314 bytes uncompressed, 1635201 bytes compressed:  64.3%
+Zip file size: 1743650 bytes, number of entries: 46
+drwx---     6.3 fat        0 bx stor 23-May-23 21:44 onnxruntime_extensions/
+drwx---     6.3 fat        0 bx stor 23-May-23 21:44 onnxruntime_extensions-0.8.0.dist-info/
+-rw-rw-rw-  2.0 fat     1162 b- defN 23-May-23 21:12 onnxruntime_extensions-0.8.0.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     5224 b- defN 23-May-23 21:12 onnxruntime_extensions-0.8.0.dist-info/METADATA
+-rw-rw-r--  2.0 fat     3935 b- defN 23-May-23 21:12 onnxruntime_extensions-0.8.0.dist-info/RECORD
+-rw-rw-rw-  2.0 fat       23 b- defN 23-May-23 20:59 onnxruntime_extensions-0.8.0.dist-info/top_level.txt
+-rw-rw-rw-  2.0 fat      100 b- defN 23-May-23 21:12 onnxruntime_extensions-0.8.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat     1639 b- defN 23-May-23 20:25 onnxruntime_extensions/cmake_helper.py
+-rw-rw-rw-  2.0 fat     2124 b- defN 23-May-23 20:25 onnxruntime_extensions/cmd.py
+-rw-rw-rw-  2.0 fat     2806 b- defN 23-May-23 20:25 onnxruntime_extensions/cvt.py
+drwx---     6.3 fat        0 bx stor 23-May-23 21:44 onnxruntime_extensions/onnxprocess/
+-rw-rw-rw-  2.0 fat      859 b- defN 23-May-23 20:25 onnxruntime_extensions/onnxprocess/torch_wrapper.py
+-rw-rw-rw-  2.0 fat     1844 b- defN 23-May-23 20:25 onnxruntime_extensions/onnxprocess/_builder.py
+-rw-rw-rw-  2.0 fat    73255 b- defN 23-May-23 20:25 onnxruntime_extensions/onnxprocess/_onnx_ops.py
+-rw-rw-rw-  2.0 fat    15158 b- defN 23-May-23 20:25 onnxruntime_extensions/onnxprocess/_session.py
+-rw-rw-rw-  2.0 fat    25410 b- defN 23-May-23 20:25 onnxruntime_extensions/onnxprocess/_tensor.py
+-rw-rw-rw-  2.0 fat      534 b- defN 23-May-23 20:25 onnxruntime_extensions/onnxprocess/__init__.py
+drwx---     6.3 fat        0 bx stor 23-May-23 21:44 onnxruntime_extensions/pnp/
+-rw-rw-rw-  2.0 fat     3928 b- defN 23-May-23 20:25 onnxruntime_extensions/pnp/_base.py
+-rw-rw-rw-  2.0 fat     2452 b- defN 23-May-23 20:25 onnxruntime_extensions/pnp/_imagenet.py
+-rw-rw-rw-  2.0 fat     7420 b- defN 23-May-23 20:25 onnxruntime_extensions/pnp/_nlp.py
+-rw-rw-rw-  2.0 fat    74398 b- defN 23-May-23 20:25 onnxruntime_extensions/pnp/_onnx_ops.py
+-rw-rw-rw-  2.0 fat    11927 b- defN 23-May-23 20:25 onnxruntime_extensions/pnp/_torchext.py
+-rw-rw-rw-  2.0 fat     1649 b- defN 23-May-23 20:25 onnxruntime_extensions/pnp/_unifier.py
+-rw-rw-rw-  2.0 fat    13061 b- defN 23-May-23 20:25 onnxruntime_extensions/pnp/_utils.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-May-23 20:25 onnxruntime_extensions/pnp/__init__.py
+drwx---     6.3 fat        0 bx stor 23-May-23 21:44 onnxruntime_extensions/tools/
+-rw-rw-rw-  2.0 fat    23211 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/add_pre_post_processing_to_model.py
+drwx---     6.3 fat        0 bx stor 23-May-23 21:44 onnxruntime_extensions/tools/pre_post_processing/
+-rw-rw-rw-  2.0 fat    19445 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/pre_post_processing/pre_post_processor.py
+-rw-rw-rw-  2.0 fat     9130 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/pre_post_processing/step.py
+drwx---     6.3 fat        0 bx stor 23-May-23 21:44 onnxruntime_extensions/tools/pre_post_processing/steps/
+-rw-rw-rw-  2.0 fat     9380 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/pre_post_processing/steps/general.py
+-rw-rw-rw-  2.0 fat    14727 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/pre_post_processing/steps/nlp.py
+-rw-rw-rw-  2.0 fat    44443 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/pre_post_processing/steps/vision.py
+-rw-rw-rw-  2.0 fat      165 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/pre_post_processing/steps/__init__.py
+-rw-rw-rw-  2.0 fat     5698 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/pre_post_processing/utils.py
+-rw-rw-rw-  2.0 fat      125 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/pre_post_processing/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/__init__.py
+-rw-rw-rw-  2.0 fat     4419 b- defN 23-May-23 20:25 onnxruntime_extensions/util.py
+-rw-rw-rw-  2.0 fat    13867 b- defN 23-May-23 20:25 onnxruntime_extensions/_cuops.py
+-rw-a--     6.3 fat  4420016 bx defN 23-May-23 21:46 onnxruntime_extensions/_extensions_pydll.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat     6489 b- defN 23-May-23 20:25 onnxruntime_extensions/_ocos.py
+-rw-rw-rw-  2.0 fat     6599 b- defN 23-May-23 20:25 onnxruntime_extensions/_ortapi2.py
+-rw-rw-rw-  2.0 fat       75 b- defN 23-May-23 20:59 onnxruntime_extensions/_version.py
+-rw-rw-rw-  2.0 fat      929 b- defN 23-May-23 20:25 onnxruntime_extensions/__init__.py
+46 files, 4828121 bytes uncompressed, 1735798 bytes compressed:  64.0%
```

## zipnote {}

```diff
@@ -1,34 +1,37 @@
 Filename: onnxruntime_extensions/
 Comment: 
 
-Filename: onnxruntime_extensions-0.7.0.dist-info/
+Filename: onnxruntime_extensions-0.8.0.dist-info/
 Comment: 
 
-Filename: onnxruntime_extensions-0.7.0.dist-info/LICENSE
+Filename: onnxruntime_extensions-0.8.0.dist-info/LICENSE
 Comment: 
 
-Filename: onnxruntime_extensions-0.7.0.dist-info/METADATA
+Filename: onnxruntime_extensions-0.8.0.dist-info/METADATA
 Comment: 
 
-Filename: onnxruntime_extensions-0.7.0.dist-info/RECORD
+Filename: onnxruntime_extensions-0.8.0.dist-info/RECORD
 Comment: 
 
-Filename: onnxruntime_extensions-0.7.0.dist-info/top_level.txt
+Filename: onnxruntime_extensions-0.8.0.dist-info/top_level.txt
 Comment: 
 
-Filename: onnxruntime_extensions-0.7.0.dist-info/WHEEL
+Filename: onnxruntime_extensions-0.8.0.dist-info/WHEEL
 Comment: 
 
 Filename: onnxruntime_extensions/cmake_helper.py
 Comment: 
 
 Filename: onnxruntime_extensions/cmd.py
 Comment: 
 
+Filename: onnxruntime_extensions/cvt.py
+Comment: 
+
 Filename: onnxruntime_extensions/onnxprocess/
 Comment: 
 
 Filename: onnxruntime_extensions/onnxprocess/torch_wrapper.py
 Comment: 
 
 Filename: onnxruntime_extensions/onnxprocess/_builder.py
```

## onnxruntime_extensions/cmd.py

```diff
@@ -1,9 +1,9 @@
 import os
-import fire
+import argparse
 import onnx
 import numpy
 
 from onnx import onnx_pb, save_tensor, numpy_helper
 from ._ortapi2 import OrtPyFunction
 
 
@@ -32,9 +32,26 @@
                 save_tensor(numpy_helper.from_array(_x, op_func.inputs[_idx].name), fn)
             onnx.save_model(op_func.onnx_model, os.path.join(testdir, 'model.onnx'))
 
     def selfcheck(self, *args):
         print("The extensions loaded, status: OK.")
 
 
+def main():
+    parser = argparse.ArgumentParser(description="ORT Extension commands")
+    parser.add_argument("command", choices=["run", "selfcheck"])
+    parser.add_argument("--model", default="model.onnx", help="Path to the ONNX model file")
+    parser.add_argument("--testdata-dir", help="Path to the test data directory")
+    parser.add_argument("args", nargs=argparse.REMAINDER, help="Additional arguments")
+
+    args = parser.parse_args()
+
+    ort_commands = ORTExtCommands(model=args.model, testdata_dir=args.testdata_dir)
+
+    if args.command == "run":
+        ort_commands.run(*args.args)
+    elif args.command == "selfcheck":
+        ort_commands.selfcheck(*args.args)
+
+
 if __name__ == '__main__':
-    fire.Fire(ORTExtCommands)
+    main()
```

## onnxruntime_extensions/onnxprocess/_builder.py

```diff
@@ -1,11 +1,12 @@
 import json
 import pathlib
 from ._onnx_ops import make_model_ex
-from .._ortapi2 import SingleOpGraph, default_opset_domain, GPT2Tokenizer, VectorToString
+from .._cuops import SingleOpGraph, GPT2Tokenizer, VectorToString
+from .._ortapi2 import default_opset_domain
 
 
 def is_path(name_or_buffer):
     return isinstance(name_or_buffer, str) or isinstance(name_or_buffer, pathlib.Path)
 
 
 class _GPT2Tokenizer(GPT2Tokenizer):
```

## onnxruntime_extensions/pnp/_base.py

```diff
@@ -1,14 +1,16 @@
 import io
 import onnx
 import torch
 from typing import Any
 from onnx.onnx_pb import TensorProto
 from torch.onnx import TrainingMode, export as _export
 
+from ._onnx_ops import OPSET_TO_IR_VERSION
+
 
 def _export_f(model, *args,
               opset_version=None,
               output_path=None,
               output_seq=0,
               export_params=True,
               verbose=False,
@@ -28,14 +30,17 @@
                 operator_export_type=operator_export_type, opset_version=opset_version,
                 do_constant_folding=do_constant_folding,
                 dynamic_axes=dynamic_axes,
                 keep_initializers_as_inputs=keep_initializers_as_inputs,
                 custom_opsets=custom_opsets)
 
         mdl = onnx.load_model(io.BytesIO(f.getvalue()))
+        for ops in mdl.opset_import:
+            if ops.domain in ('', 'ai.onnx'):
+                mdl.ir_version = OPSET_TO_IR_VERSION[ops.version]
         if output_path is not None:
             if output_seq > 0:
                 output_path.replace('.onnx', '.{}.onnx'.format(output_seq))
             onnx.save_model(mdl, output_path)
         return mdl
```

## onnxruntime_extensions/pnp/_onnx_ops.py

```diff
@@ -11,14 +11,16 @@
 DEFAULT_OPSET_NUMBER = 13  # The maximum opset supported by the converter in the code branch.
 # From https://github.com/onnx/onnx/blob/master/docs/Versioning.md
 OPSET_TO_IR_VERSION = {
     1: 3, 2: 3, 3: 3, 4: 3, 5: 3, 6: 3,
     7: 3, 8: 3, 9: 4, 10: 5, 11: 6, 12: 7,
     13: 7, 14: 7, 15: 8, 16: 8, 17: 8
 }
+if hasattr(helper, 'VERSION_TABLE'):
+    OPSET_TO_IR_VERSION = {row[2]: row[1] for row in helper.VERSION_TABLE}
 
 
 def _get_main_opset_version(model):
     """
     Returns the main opset version.
     """
     for op in model.opset_import:
```

## onnxruntime_extensions/pnp/_utils.py

```diff
@@ -1,10 +1,10 @@
 import copy
 import onnx
-from onnx import numpy_helper
+from onnx import helper, numpy_helper
 from collections import namedtuple
 
 
 class _Container:
     def __init__(self):
         self.parent = None
         self.initializer=[]
@@ -267,24 +267,36 @@
                     replaceable = True
                     break
             if replaceable:
                 new_input = copy.deepcopy(_n.input)
                 del _n.input[:]
                 _n.input.extend([port_mapping[_i] if _i in port_mapping else _i for _i in new_input])
 
-        name = ''
+        name = "_".join([_mdl.graph.name for _mdl in models])
         domains = set()
         _opset = []
         for _mdl in models:
             for _ops in _mdl.opset_import:
-                if _ops.domain not in domains:
-                    domains.update([_ops.domain])
-                    _opset.append(_ops)
-            name = name + '_' + _mdl.graph.name if name else _mdl.graph.name
+                domain = _ops.domain if _ops.domain else "ai.onnx"
+                if domain in domains:
+                    if domain == "ai.onnx":
+                      assert _ops.version == _opset[0].version, \
+                        f"ai.onnx domain version doesn't match {_ops.version} != {_opset[0].version}"
+                else:
+                    domains.add(domain)
+                    if domain == "ai.onnx":
+                        _opset.insert(0, _ops)
+                    else:
+                        _opset.append(_ops)
 
         inits = cls._remove_unused_initializers(nodes, container.initializer)
-        helper = onnx.helper
         g = helper.make_graph(nodes, name, inputs, outputs,
                               initializer=inits,
                               value_info=container.value_info)
-        m = helper.make_model(g, opset_imports=_opset)
+
+        if hasattr(helper, 'make_model_gen_version'):
+            # make_model_gen_version doesn't accept the custom domain.
+            m = helper.make_model_gen_version(g, opset_imports=_opset[:1])
+            m.opset_import.extend(_opset[1:])
+        else:
+            m = helper.make_model(g, opset_imports=_opset)
         return m
```

## onnxruntime_extensions/tools/add_pre_post_processing_to_model.py

```diff
@@ -158,14 +158,142 @@
         ]
     )
 
     new_model = pipeline.run(model)
     onnx.save_model(new_model, str(output_file.resolve()))
 
 
+def yolo_detection(model_file: Path, output_file: Path, output_format: str = 'jpg',
+                   onnx_opset: int = 16, num_classes: int = 80, input_shape: List[int] = None):
+    """
+    SSD-like model and Faster-RCNN-like model are including NMS inside already, You can find it from onnx model zoo.
+
+    A pure detection model accept fix-sized(say 1,3,640,640) image as input, and output a list of bounding boxes, which
+    the numbers are determinate by anchors.
+
+    This function target for Yolo detection model. It support YOLOv3-yolov8 models theoretically.
+    You should assure this model has only one input, and the input shape is [1, 3, h, w].
+    The model has either one or more outputs. 
+        If the model has one output, the output shape is [1,num_boxes, coor+(obj)+cls] 
+            or [1, coor+(obj)+cls, num_boxes].
+        If the model has more than one outputs, you should assure the first output shape is 
+            [1, num_boxes, coor+(obj)+cls] or [1, coor+(obj)+cls, num_boxes].
+    Note: (obj) means it's optional.
+
+    :param model_file: The input model file path.
+    :param output_file: The output file path, where the finalized model saved to.
+    :param output_format: The output image format, jpg or png.
+    :param onnx_opset: The opset version of onnx model, default(16).
+    :param num_classes: The number of classes, default(80).
+    :param input_shape: The shape of input image (height,width), default will be asked from model input.
+    """
+    model = onnx.load(str(model_file.resolve(strict=True)))
+    inputs = [create_named_value("image", onnx.TensorProto.UINT8, ["num_bytes"])]
+
+    model_input_shape = model.graph.input[0].type.tensor_type.shape
+    model_output_shape = model.graph.output[0].type.tensor_type.shape
+
+    # We will use the input_shape to create the model if provided by user.
+    if input_shape is not None:
+        assert len(input_shape) == 2, "The input_shape should be [h, w]."
+        w_in = input_shape[1]
+        h_in = input_shape[0]
+    else:
+        assert (model_input_shape.dim[-1].HasField("dim_value") and
+                model_input_shape.dim[-2].HasField("dim_value")), "please provide input_shape in the command args."
+
+        w_in = model_input_shape.dim[-1].dim_value
+        h_in = model_input_shape.dim[-2].dim_value
+
+    # Yolov5(v3,v7) has an output of shape (batchSize, 25200, 85) (Num classes + box[x,y,w,h] + confidence[c])
+    # Yolov8 has an output of shape (batchSize, 84,  8400) (Num classes + box[x,y,w,h])
+    # https://github.com/ultralytics/ultralytics/blob/e5cb35edfc3bbc9d7d7db8a6042778a751f0e39e/examples/YOLOv8-CPP-Inference/inference.cpp#L31-L33
+    # We always want the box info to be the last dim for each of iteration.
+    # For new variants like YoloV8, we need to add an transpose op to permute output back.
+    need_transpose = False
+
+    output_shape = [model_output_shape.dim[i].dim_value if model_output_shape.dim[i].HasField("dim_value") else -1
+                    for i in [-2, -1]]
+    if output_shape[0] != -1 and output_shape[1] != -1:
+        need_transpose = output_shape[0] < output_shape[1] 
+    else:
+        assert len(model.graph.input) == 1, "Doesn't support adding pre and post-processing for multi-inputs model."
+        try:
+            import numpy as np
+            import onnxruntime
+        except ImportError:
+            raise ImportError(
+                """Please install onnxruntime and numpy to run this script. eg 'pip install onnxruntime numpy'.
+Because we need to execute the model to determine the output shape in order to add the correct post-processing""")
+
+        # Generate a random input to run the model and infer the output shape.
+        session = onnxruntime.InferenceSession(str(model_file), providers=["CPUExecutionProvider"])
+        input_name = session.get_inputs()[0].name
+        input_type = onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[model.graph.input[0].type.tensor_type.elem_type]
+        inp = {input_name: np.random.rand(1, 3, h_in, w_in).astype(dtype=input_type)}
+        outputs = session.run(None,  inp)[0]
+        assert len(outputs.shape) == 3 and outputs.shape[0] == 1, "shape of the first model output is not (1, n, m)"
+        if outputs.shape[1] < outputs.shape[2]:
+            need_transpose = True
+        assert num_classes+4 == outputs.shape[2] or num_classes+5 == outputs.shape[2], \
+            "The output shape is neither (1, num_boxes, num_classes+4(reg)) nor (1, num_boxes, num_classes+5(reg+obj))"
+
+    pipeline = PrePostProcessor(inputs, onnx_opset)
+    # precess steps are responsible for converting any jpg/png image to CHW BGR float32 tensor
+    # jpg-->BGR(Image Tensor)-->Resize (scaled Image)-->LetterBox (Fix sized Image)-->(from HWC to)CHW-->float32-->1CHW
+    pipeline.add_pre_processing(
+        [
+            ConvertImageToBGR(),  # jpg/png image to BGR in HWC layout
+            # Resize an arbitrary sized image to a fixed size in not_larger policy
+            Resize((h_in, w_in), policy='not_larger'),
+            LetterBox(target_shape=(h_in, w_in)),  # padding or cropping the image to (h_in, w_in)
+            ChannelsLastToChannelsFirst(),  # HWC to CHW
+            ImageBytesToFloat(),  # Convert to float in range 0..1
+            Unsqueeze([0]),  # add batch, CHW --> 1CHW
+        ]
+    )
+    # NMS and drawing boxes
+    post_processing_steps = [
+        Squeeze([0]), # - Squeeze to remove batch dimension
+        SplitOutBoxAndScore(num_classes=num_classes), # Separate bounding box and confidence outputs
+        SelectBestBoundingBoxesByNMS(), # Apply NMS to suppress bounding boxes
+        (ScaleBoundingBoxes(),  # Scale bounding box coords back to original image
+         [
+            # A connection from original image to ScaleBoundingBoxes
+            # A connection from the resized image to ScaleBoundingBoxes
+            # A connection from the LetterBoxed image to ScaleBoundingBoxes
+            # We can use the three image to calculate the scale factor and offset.
+            # With scale and offset, we can scale the bounding box back to the original image.
+            utils.IoMapEntry("ConvertImageToBGR", producer_idx=0, consumer_idx=1),
+            utils.IoMapEntry("Resize", producer_idx=0, consumer_idx=2),
+            utils.IoMapEntry("LetterBox", producer_idx=0, consumer_idx=3),
+        ]),
+        # DrawBoundingBoxes on the original image
+        # Model imported from pytorch has CENTER_XYWH format
+        # two mode for how to color box,
+        #   1. colour_by_classes=True, (colour_by_classes), 2. colour_by_classes=False,(colour_by_confidence)
+        (DrawBoundingBoxes(mode='CENTER_XYWH', num_classes=num_classes, colour_by_classes=True),
+         [
+            utils.IoMapEntry("ConvertImageToBGR", producer_idx=0, consumer_idx=0),
+            utils.IoMapEntry("ScaleBoundingBoxes", producer_idx=0, consumer_idx=1),
+        ]),
+        # Encode to jpg/png
+        ConvertBGRToImage(image_format=output_format),
+    ]
+    # transpose to (num_boxes, coor+conf) if needed
+    if need_transpose:
+        post_processing_steps.insert(1, Transpose([1, 0]))
+
+    pipeline.add_post_processing(post_processing_steps)
+
+    new_model = pipeline.run(model)
+    new_mode = onnx.shape_inference.infer_shapes(new_model)
+    onnx.save_model(new_model, str(output_file.resolve()))
+
+
 class NLPTaskType(enum.Enum):
     TokenClassification = enum.auto()
     QuestionAnswering = enum.auto()
     SequenceClassification = enum.auto()
     NextSentencePrediction = enum.auto()
 
 
@@ -242,45 +370,51 @@
         os.path.basename(__file__),
         description="""Add pre and post processing to a model.
 
         Currently supports updating:
         Vision models:
             - super resolution with YCbCr input
             - imagenet trained mobilenet
+            - object detection with YOLOv3-YOLOV8
+
         NLP models:
-        
             - MobileBert with different tasks
             - XLM-Roberta with classification task
 
         For Vision models:
-            To customize, the logic in the `mobilenet` and `superresolution` functions can be used as a guide.
+            To customize, the logic in the `mobilenet`, `superresolution` and `yolo_detection` functions can be used as a guide.
         Create a pipeline and add the required pre/post processing 'Steps' in the order required. Configure 
         individual steps as needed.
         
         For NLP models:
            `transformers_and_bert` can be used for MobileBert QuestionAnswering/Classification tasks,
         or serve as a guide of how to add pre/post processing to a transformer model.
         Usually pre-processing includes adding a tokenizer. Post-processing includes conversion of output_ids to text.
         
         You might need to pass the tokenizer model file (bert vocab file or SentencePieceTokenizer model) 
         and task_type to the function.
 
         The updated model will be written in the same location as the original model, 
         with '.onnx' updated to '.with_pre_post_processing.onnx'
+
+        Example usage:
+            object detection:
+            - python -m onnxruntime_extensions.tools.add_pre_post_processing_to_model -t yolo -num_classes 80 --input_shape 640,640 yolov8n.onnx  
         """,
     )
 
     parser.add_argument(
         "-t",
         "--model_type",
         type=str,
         required=True,
         choices=[
             "superresolution",
             "mobilenet",
+            "yolo",
             "transformers",
         ],
         help="Model type.",
     )
 
     parser.add_argument(
         "-s",
@@ -302,14 +436,29 @@
         required=False,
         choices=["jpg", "png"],
         default="png",
         help="Image output format for superresolution model to produce.",
     )
 
     parser.add_argument(
+        "--num_classes",
+        type=int,
+        default=80,
+        help="Number of classes in object detection model.",
+    )
+
+    parser.add_argument(
+        "--input_shape",
+        type=str,
+        default="",
+        help="To specify input image shape(height,width) for the model. Such as \"224,224\", \
+              Tools will ask onnx model for input shape if input_shape is not specified.",
+    )
+
+    parser.add_argument(
         "--nlp_task_type",
         type=str,
         choices=["QuestionAnswering",
                  "SequenceClassification",
                  "NextSentencePrediction"],
         required=False,
         help="The downstream task for NLP model.",
@@ -343,16 +492,20 @@
     model_path = args.model.resolve(strict=True)
     new_model_path = model_path.with_suffix(".with_pre_post_processing.onnx")
 
     if args.model_type == "mobilenet":
         source = ModelSource.PYTORCH if args.model_source == "pytorch" else ModelSource.TENSORFLOW
         mobilenet(model_path, new_model_path, source, args.opset)
     elif args.model_type == "superresolution":
-        superresolution(model_path, new_model_path,
-                        args.output_format, args.opset)
+        superresolution(model_path, new_model_path, args.output_format, args.opset)
+    elif args.model_type == "yolo":
+        input_shape = None
+        if args.input_shape != "":
+            input_shape = [int(x) for x in args.input_shape.split(",")]
+        yolo_detection(model_path, new_model_path, args.output_format, args.opset, args.num_classes, input_shape)
     else:
         if args.vocab_file is None or args.nlp_task_type is None or args.tokenizer_type is None:
             parser.error("Please provide vocab file/nlp_task_type/tokenizer_type.")
         transformers_and_bert(model_path, new_model_path, args.tokenizer_type, args.vocab_file, args.nlp_task_type)
 
 
 if __name__ == "__main__":
```

## onnxruntime_extensions/tools/pre_post_processing/pre_post_processor.py

```diff
@@ -53,18 +53,18 @@
         #
         # Post-processing we connect the original model output to the Step input
         #   - step_idx is for Step.input_names, and name is in graph.output
         self._pre_processing_joins = None  # type: Union[None,List[Tuple[Union[Step, str], int, str]]]
         self._post_processing_joins = None  # type: Union[None,List[Tuple[Union[Step, str], int, str]]]
 
         self._inputs = inputs if inputs else []
-        
+
         # preserve outputs from IOMapEntry, avoid it's consumed by the Follow-up steps.
         # we now can support a output value has more than one consumers with IOEntryValuePreserver.
-        # IOEntryValuePreserver will preserve the output value and add it to the graph output 
+        # IOEntryValuePreserver will preserve the output value and add it to the graph output
         # until consumer step is done.
         self.outputs_preserver = []  # type: List[IOEntryValuePreserver]
 
     def add_pre_processing(self, items: List[Union[Step, Tuple[Step, List[IoMapEntry]]]]):
         """
         Add the pre-processing steps. The last step is automatically joined to the original model inputs.
 
@@ -202,15 +202,15 @@
             # we may need a natty way to get possible outputs after merge_graphs
             step_graph_outputs = [o.name for o in pre_process_graph.output]
             io_map = []  # type: List[Tuple[str, str]]
             for step, step_idx, graph_input in self._pre_processing_joins:
                 io_map.append((step.output_names[step_idx], graph_input))
                 step_graph_outputs.remove((step.output_names[step_idx]))
 
-            # add outputs from previous IoMapEntry producers to maintain them as graph outputs 
+            # add outputs from previous IoMapEntry producers to maintain them as graph outputs
             # until consumed by the final Step that requires them.
             step_graph_outputs += [
                 o.name for o in graph.output if o.name not in step_graph_outputs]
             external_outputs = [
                 i.output for i in self.outputs_preserver if i.is_active and i.output not in step_graph_outputs]
             if external_outputs:
                 step_graph_outputs.extend(external_outputs)
@@ -249,15 +249,19 @@
             graph = onnx.compose.merge_graphs(graph, post_process_graph, io_map)
 
         # Make the output names nicer by removing prefixing from naming that occurred when applying the steps
         graph = PrePostProcessor.__cleanup_graph_output_names(graph)
 
         opset_imports = [onnx.helper.make_operatorsetid(domain, opset)
                          for domain, opset in self._custom_op_checker_context.opset_imports.items()]
-        new_model = onnx.helper.make_model(graph, opset_imports=opset_imports)
+        # find_min_ir_version_for doesn't support custom domains until ONNX 1.14 so extract the ONNX opset from the
+        # imports and only pass that in.
+        ir_version = onnx.helper.find_min_ir_version_for([entry for entry in opset_imports
+                                                          if entry.domain == "" or entry.domain == "ai.onnx"])
+        new_model = onnx.helper.make_model(graph, opset_imports=opset_imports, ir_version=ir_version)
 
         onnx.checker.check_model(new_model)
 
         return new_model
 
     def __add_processing(
         self,
@@ -271,15 +275,15 @@
         Args:
             processors: List of processors to add items to.
             processor_connections: Populated with connections between each step. 1:1 with entries in processors.
             items: Items to add to processors.
                    Can be:
                      A Step instance. This will be implicitly joined to the immediately previous Step if one exists.
                      A tuple of (Step instance, list of IoMapEntry)
-                      The IoMapEntry values are used to manually join an output from a producer Step to an input 
+                      The IoMapEntry values are used to manually join an output from a producer Step to an input
                       of the current Step.
                         In each IoMapEntry, if a step name is provided the producer Step will be searched for in all
                         predecessor steps. It is valid for a post-processor step to consume output from a
                         pre-processor step.
         """
 
         for item in items:
```

## onnxruntime_extensions/tools/pre_post_processing/steps/vision.py

```diff
@@ -7,14 +7,16 @@
 from typing import List, Optional, Tuple, Union
 from ..step import Step
 from .general import Transpose
 
 #
 # Image conversion
 #
+
+
 class ConvertImageToBGR(Step):
     """
     Convert the bytes of an image by decoding to BGR ordered uint8 values.
     Supported input formats: jpg, png
     Input shape: {num_encoded_bytes}
     Output shape: {input_image_height, input_image_width, 3}
     """
@@ -277,31 +279,40 @@
 #
 class Resize(Step):
     """
     Resize input data. Aspect ratio is maintained.
     e.g. if image is 1200 x 600 and 300 x 300 is requested the result will be 600 x 300
     """
 
-    def __init__(self, resize_to: Union[int, Tuple[int, int]], layout: str = "HWC", name: Optional[str] = None):
+    def __init__(self, resize_to: Union[int, Tuple[int, int]], layout: str = "HWC",
+                 policy: str = "not_smaller", name: Optional[str] = None):
         """
         Args:
             resize_to: Target size. Can be a single value or a tuple with (target_height, target_width).
                        The aspect ratio will be maintained and neither height or width in the result will be smaller
                        than the requested value.
             layout: Input layout. 'NCHW', 'NHWC', 'CHW', 'HWC' and 'HW' are supported.
+            policy: not_smaller (default) 
+                        the sizes are adjusted so that no extent of the output is larger than the specified size, 
+                        while keeping the original aspect ratio
+                    not_larger
+                        the sizes are adjusted so that no extent of the output is smaller than the specified size, 
+                        while keeping the original aspect ratio.
+                    Please refer to https://github.com/onnx/onnx/blob/main/docs/Operators.md#Resize for more details.
             name: Optional name. Defaults to 'Resize'
         """
         super().__init__(["image"], ["resized_image"], name)
         if isinstance(resize_to, int):
             self._height = self._width = resize_to
         else:
             assert isinstance(resize_to, tuple)
             self._height, self._width = resize_to
 
         self._layout = layout
+        self.policy_ = policy
 
     def _create_graph_for_step(self, graph: onnx.GraphProto, onnx_opset: int):
         input_type_str, input_shape_str = self._get_input_type_and_shape_strs(graph, 0)
         dims = input_shape_str.split(",")
 
         # adjust for layout
         # resize will use the largest ratio so both sides won't necessarily match the requested height and width.
@@ -366,27 +377,35 @@
 
         split_input_shape_attr = "axis = 0"
         split_new_sizes_attr = "axis = 0"
         if onnx_opset >= 18:
             # Split now requires the number of outputs to be specified even though that can be easily inferred...
             split_input_shape_attr += f", num_outputs = {len(dims)}"
             split_new_sizes_attr += ", num_outputs = 2"
+        
+        # Resize-18 has the attribute "not_larger/not_smaller" to specify the resize policy, however
+        # we want to support older opsets as well. 
+        assert (self.policy_ in ["not_smaller", "not_larger"], 
+                f"Unsupported resize policy of {self.policy_}, must be 'not_smaller' or 'not_larger'")
+        ratio_resize_func = "ReduceMax"
+        if self.policy_ == "not_larger":
+            ratio_resize_func = "ReduceMin"
 
         resize_graph = onnx.parser.parse_graph(
             f"""\
             resize ({input_type_str}[{input_shape_str}] {self.input_names[0]}) => 
                 ({input_type_str}[{output_shape_str}] {self.output_names[0]})
             {{
                 target_size = Constant <value = float[2] {{{float(self._height)}, {float(self._width)}}}> ()
                 image_shape = Shape ({self.input_names[0]})
                 {split_str} = Split <{split_input_shape_attr}> (image_shape)
                 hw = Concat <axis = 0> (h, w)
                 f_hw = Cast <to = 1> (hw)
                 ratios = Div (target_size, f_hw)
-                ratio_resize = ReduceMax (ratios)
+                ratio_resize = {ratio_resize_func} (ratios)
                 f_hw2_exact = Mul (f_hw, ratio_resize)
                 f_hw2_round = Round (f_hw2_exact)
                 hw2 = Cast <to = 7> (f_hw2_round)
                 h2, w2 = Split <{split_new_sizes_attr}> (hw2)
                 {u64_1_str}
                 sizes_resize = Concat <axis = 0> ({sizes_str})
                 {resize_str}
@@ -600,7 +619,365 @@
         """
         Args:
             has_batch_dim: Set to True if the input has a batch dimension (i.e. is NHWC)
             name: Optional step name. Defaults to 'ChannelsLastToChannelsFirst'
         """
         perms = [0, 3, 1, 2] if has_batch_dim else [2, 0, 1]
         super().__init__(perms, name)
+
+
+class DrawBoundingBoxes(Step):
+    """
+    Draw boxes on BGR image at given position, image is channel last and ordered by BGR.
+    Input shape: <uint8_t>{height, width, 3<BGR>}
+    boxes: <float>{num_boxes, 6<x, y, x/w, y/h, score, class>}
+        The coordinates is the absolute pixel values in the picture. Its value is determined by `mode`.
+        we have different modes to represent the coordinates of the box.[XYXY, XYWH, CENTER_XYWH].
+        Please refer to the following link for more details. https://keras.io/api/keras_cv/bounding_box/formats/
+        **score** is the confidence of the box(object score * class probability) and **class** is the class of the box.
+
+    Output shape: <uint8_t>{height, width, 3<BGR>}
+    """
+
+    def __init__(self, mode: str = "XYXY", thickness: int = 4, num_classes: int = 10,
+                 colour_by_classes=False, name: Optional[str] = None):
+        """
+        Args:
+            mode: The mode of the boxes, 
+                    "XYXY" (xmin ymin xmax ymax)  All values in the XYXY format should be absolute pixel values.
+                    "XYWH" (xmin ymin width height) 
+                    "CENTER_XYWH" (x_center, y_center, width, height) 
+                                  All values in the CENTER_XYWH format should be absolute pixel values.
+
+
+            thickness: Thickness of the box edge
+            num_colours: Number of colours to use
+                         We support 10 predefined colours and the other classes more than 10 wouldn't be drawn.
+                         colors are [Red, Yellow, Lime, Cyan, Blue, Magenta, Orange, Maroon, Green, Navy]
+                         and are used in that order. i.e. result with best score will use red. 
+            colour_by_classes: Colour boxes by classes or by score. 
+                               If `True` we use a colour for each unique class, with all results from the top 
+                               `num_colours` classes displayed. A colour is only used for a single class. 
+                               If `False`, we draw boxes for the top `num_colours` results. A colour is used 
+                               for a single result, regardless of class.
+            name: Optional name of step. Defaults to 'DrawBoundingBoxes'
+        """
+        super().__init__(["image", "boxes"], ["image_out"], name)
+        self.thickness_ = thickness
+        self.num_classes_ = num_classes
+        self.colour_by_classes_ = colour_by_classes
+        self.mode_ = mode
+
+    def _create_graph_for_step(self, graph: onnx.GraphProto, onnx_opset: int):
+        input0_type_str, input0_shape_str = self._get_input_type_and_shape_strs(graph, 0)
+        input1_type_str, input1_shape_str = self._get_input_type_and_shape_strs(graph, 1)
+        assert input0_type_str == "uint8" and input1_type_str == "float"
+
+        assert str(input1_shape_str.split(",")[-1]) == "6"
+
+
+        output_shape_str = input0_shape_str
+        converter_graph = onnx.parser.parse_graph(
+            f"""\
+            bounding_box (uint8[{input0_shape_str}] {self.input_names[0]}, float[{input1_shape_str}] {self.input_names[1]}) 
+                => (uint8[{output_shape_str}] {self.output_names[0]})  
+            {{
+                {self.output_names[0]} = com.microsoft.extensions.DrawBoundingBoxes({self.input_names[0]}, {self.input_names[1]})
+            }}
+            """
+        )
+        op_attr = ["thickness", "num_classes", "colour_by_classes","mode"]
+        token_model_attr = []
+        token_model_attr.append(onnx.helper.make_attribute(op_attr[0], self.thickness_))
+        token_model_attr.append(onnx.helper.make_attribute(op_attr[1], self.num_classes_))
+        token_model_attr.append(onnx.helper.make_attribute(op_attr[2], int(self.colour_by_classes_)))
+        token_model_attr.append(onnx.helper.make_attribute(op_attr[3], self.mode_))
+        converter_graph.node[0].attribute.extend(token_model_attr)
+
+        return converter_graph
+
+
+class LetterBox(Step):
+    """
+    Image is channel last and ordered by BGR.
+    mainly used in object detection, it mostly follows behind resize operation. 
+    This step either add border or crop the image to satisfy network input.
+    -----          bbbbbbbbb
+    |img|    --- > bb-----bb  
+    -----          bb|img|bb
+                   bb-----bb
+                   bbbbbbbbb
+    If target_shape is less than the original image, it will crop the image in a center mode.
+    And the padding values will be negative and the Pad op performs cropping.
+
+    Input shape: <uint8_t>{height, width, 3<BGR>}
+    target_shape: <uint8_t>{out_height, out_width, 3<BGR>}
+    Output shape: specified by target_shape
+    """
+
+    def __init__(self, target_shape: Union[int, Tuple[int, int]], fill_value=0, name: Optional[str] = None):
+        """
+        Args:
+            target_shape: the size of the output image
+            fill_value:  a constant value used to fill the border
+            name: Optional name of step. Defaults to 'LetterBox'
+        """            
+        super().__init__(["image"], ["image_pad"], name)
+
+        self.target_shape_ = target_shape
+        self.fill_value_ = fill_value
+
+    def _create_graph_for_step(self, graph: onnx.GraphProto, onnx_opset: int):
+        input0_type_str, input0_shape_str = self._get_input_type_and_shape_strs(graph, 0)
+
+        assert len(input0_shape_str.split(',')) == 3, " expected BGR image"
+
+        target_shape_str = f"{self.target_shape_[0]}, {self.target_shape_[1]}, 3"
+
+        split_input_shape_attr = "axis = 0"
+        if onnx_opset >= 18:
+            # Split now requires the number of outputs to be specified even though that can be easily inferred...
+            split_input_shape_attr += f", num_outputs = 3"
+
+        converter_graph = onnx.parser.parse_graph(
+            f"""\
+            LetterBox (uint8[{input0_shape_str}] {self.input_names[0]}) 
+                => (uint8[{target_shape_str}] {self.output_names[0]})  
+            {{
+                target_size = Constant <value = int64[2] {{{(self.target_shape_[0])}, {(self.target_shape_[1])}}}> ()
+                i64_2 = Constant <value = int64[1] {{2}}>()
+                i64_0 = Constant <value = int64[1] {{0}}>()
+                const_val = Constant <value = uint8[1] {{{self.fill_value_}}}> ()
+                image_shape = Shape ({self.input_names[0]})
+                h,w,c = Split <{split_input_shape_attr}> (image_shape)
+                hw = Concat <axis = 0> (h, w)
+                pad_hw = Sub (target_size, hw)
+                half_pad_hw = Div (pad_hw, i64_2)
+                remainder_pad_hw = Sub (pad_hw, half_pad_hw)
+                pad_value = Concat <axis = 0> (half_pad_hw, i64_0,remainder_pad_hw,i64_0)
+                {self.output_names[0]} = Pad({self.input_names[0]}, pad_value, const_val)
+            }}
+            """
+        )
+
+        return converter_graph
+
+
+class SplitOutBoxAndScore(Step):
+    r"""
+    Split the output of the model into boxes and scores. This step will also handle the optional object score.
+    Input shape: <float>{num_boxes, 4/5+num_classes}
+    Output shape: <float>{num_boxes, 4}, <float>{num_boxes, num_classes}
+    |x1,x2,x3,x4, (obj), cls_1, ... cls_num|
+            /\
+           /  \
+    |x1,x2,x3,x4|  |cls_1, ... clx_num|*(obj)
+    obj is optional, if it is not present, it will be set to 1.0
+    This is where 4/5 comes from, '4' represent coordinates and the fifth object probability.
+    """
+    def __init__(self, num_classes:int = 80, name: Optional[str] = None):
+        """
+        Args:
+            num_classes: number of classes
+            name: Optional name of step. Defaults to 'SplitOutBoxAndScore'
+        """
+            
+        super().__init__(["box_and_score"], ["_pre_boxes", "_pre_scores"], name)
+        self.num_classes_ = num_classes
+
+    def _create_graph_for_step(self, graph: onnx.GraphProto, onnx_opset: int):
+        input0_type_str, input0_shape_str = self._get_input_type_and_shape_strs(graph, 0)
+
+        input_shape_list = input0_shape_str.split(',')
+        assert len(input_shape_list) == 2, " expected [num_boxes, 4/5+num_classes]"
+
+        target_shape_str_0 = f"{input_shape_list[0]}, 4"
+        target_shape_str_1 = f"{input_shape_list[0]}, _{self._step_num}_class"
+
+        converter_graph = onnx.parser.parse_graph(
+            f"""\
+            SplitOutBoxAndScore (float[{input0_shape_str}] {self.input_names[0]}) 
+                => (float[{target_shape_str_0}] {self.output_names[0]}, float[{target_shape_str_1}] {self.output_names[1]})  
+            {{
+
+                i64_neg1 = Constant <value = int64[1] {{-1}}>()
+                i64_4 = Constant <value = int64[1] {{4}}>()
+                i64_0 = Constant <value = int64[1] {{0}}>()
+                fp32_1 = Constant <value = float[1] {{1.0}}>()
+                i64_classes = Constant <value = int64[1] {{{self.num_classes_}}}>()
+                out_shape = Shape ({self.input_names[0]})
+                class_and_coor_dim = Gather (out_shape, i64_neg1)
+                coor_and_obj = Sub (class_and_coor_dim, i64_classes)
+                obj_0_or_1 = Sub (coor_and_obj, i64_4)
+                bool_num_obj_0_or_1 = Cast<to=9>(obj_0_or_1)
+
+                box_obj_class_concat = Concat <axis = 0> (i64_4, obj_0_or_1, i64_classes)
+                boxes_o, scores_obj_o, scores_cls_o = Split <axis = -1> ({self.input_names[0]}, box_obj_class_concat)
+                scores_obj_not_null = Concat <axis = -1> (scores_obj_o, boxes_o)
+                coef_obj_cat =  Where(bool_num_obj_0_or_1, scores_obj_not_null,fp32_1)
+                coef_obj = Gather <axis=-1> (coef_obj_cat, i64_0)
+                scores_o = Mul (scores_cls_o, coef_obj)
+                {self.output_names[0]} = Identity (boxes_o)
+                {self.output_names[1]} = Identity (scores_o)
+
+            }}
+            """
+        )
+        return converter_graph
+
+
+class SelectBestBoundingBoxesByNMS(Step):
+    """
+    Non-maximum suppression (NMS) is to filter out redundant bounding boxes.
+    This step is used to warp the boxes and scores into onnx SelectBestBoundingBoxesByNMS op.
+    Input:
+        boxes:  float[num_boxes, 4]
+        scores:  shape float[num_boxes, num_classes]
+
+    Output:
+        nms_out: float[_few_num_boxes, 6<coordinate+score+class>]
+    """
+
+    def __init__(self, iou_threshold:float = 0.5, score_threshold:float = 0.67, 
+                 max_detections:int = 300, name: Optional[str] = None):
+        """
+        Args:
+        Please refer to https://github.com/onnx/onnx/blob/main/docs/Operators.md#SelectBestBoundingBoxesByNMS
+        for more details about the parameters.
+            iou_threshold:  same as SelectBestBoundingBoxesByNMS op, intersection /union of boxes 
+            score_threshold:  If this box's score is lower than score_threshold, it will be removed.
+            max_detections:  max number of boxes to be selected
+            name: Optional name of step. Defaults to 'SelectBestBoundingBoxesByNMS'
+        """
+        super().__init__(["boxes", "scores"], ["nms_out"], name)
+        self.iou_threshold_ = iou_threshold
+        self.score_threshold_ = score_threshold
+        self.max_detections_ = max_detections
+
+
+    def _create_graph_for_step(self, graph: onnx.GraphProto, onnx_opset: int):
+        input0_type_str, input0_shape_str = self._get_input_type_and_shape_strs(graph, 0)
+        input1_type_str, input1_shape_str = self._get_input_type_and_shape_strs(graph, 1)
+
+        input0_shape_list = input0_shape_str.split(',')
+        assert len(input0_shape_list) == 2, " expected [num_boxes, 4]"
+
+        target_shape_str = f"_{self._step_num}_nms_boxes, 6"
+
+        reduce_score = '(score_select_nm,i64_neg1)' if onnx_opset >= 18 else '<axes=[-1]>(score_select_nm)'
+
+        converter_graph = onnx.parser.parse_graph(
+            f"""\
+            SelectBestBoundingBoxesByNMS (float[{input0_shape_str}] {self.input_names[0]},float[{input1_shape_str}] {self.input_names[1]}) 
+                => (float[{target_shape_str}] {self.output_names[0]})  
+            {{
+                i64_2 = Constant <value = int64[1] {{2}}>()
+                i64_0 = Constant <value = int64[1] {{0}}>()
+                i64_1 = Constant <value = int64[1] {{1}}>()
+                i64_max_obj = Constant <value = int64[1] {{{self.max_detections_}}}>()
+                i64_neg1 = Constant <value = int64[1] {{-1}}>()
+                fp32_iou_th = Constant <value = float[1] {{{self.iou_threshold_}}}>()
+                fp32_score_th = Constant <value = float[1] {{{self.score_threshold_}}}>()
+
+                boxes_i = Identity ({self.input_names[0]})
+                scores_i = Identity({self.input_names[1]})
+                scores_c_b = Transpose<perm=[1,0]>(scores_i)
+                batch_boxes = Unsqueeze(boxes_i, i64_0)
+                batch_scores = Unsqueeze(scores_c_b, i64_0)
+
+                nmsbox = NonMaxSuppression<center_point_box =1>(batch_boxes, batch_scores, i64_max_obj,fp32_iou_th,fp32_score_th)
+                classes_i64 = Gather <axis=-1>(nmsbox,i64_1)
+                class_select = Cast <to = 1>(classes_i64)
+
+                boxes_idx_us = Gather <axis=-1>(nmsbox,i64_2)
+                boxes_idx = Squeeze(boxes_idx_us, i64_neg1)
+                boxes_select = Gather <axis=0>(boxes_i, boxes_idx)
+
+                score_select_nm = Gather <axis=0>(scores_i, boxes_idx)
+                score_select = ReduceMax{reduce_score}
+
+                {self.output_names[0]} = Concat <axis = -1> (boxes_select, score_select, class_select)
+            }}
+            """
+        )
+        return converter_graph
+
+
+class ScaleBoundingBoxes(Step):
+    """
+    Mapping boxes coordinate to scale in original image.
+    The coordinate of boxes from detection model is relative to the input image of network, 
+    image is scaled and padded/cropped. So we need to do a linear mapping to get the real coordinate of original image.
+    input:
+        box_of_nms_out: output of NMS, shape [num_boxes, 6]
+        original_image: original image decoded from jpg/png<uint8_t>[H, W, 3<BGR>]
+        scaled_image: scaled image, but without padding/crop[<uint8_t>[H1, W1, 3<BGR>]
+        letter_boxed_image: scaled image and with padding/crop[<uint8_t>[H2, W3, 3<BGR>]
+    
+    output:
+        scaled_box_out: shape [num_boxes, 6] with coordinate mapped to original image.
+    """
+
+    def __init__(self, name: Optional[str] = None):
+        """
+        Args:
+            name: Optional name of step. Defaults to 'ScaleBoundingBoxes'
+        """
+        super().__init__(["box_of_nms_out", "original_image", "scaled_image",
+                          "letter_boxed_image"], ["scaled_box_out"], name)
+
+    def _create_graph_for_step(self, graph: onnx.GraphProto, onnx_opset: int):
+        graph_input_param = []
+        target_shape = []
+        for idx,input_name in enumerate(self.input_names):
+            input_type_str, input_shape_str = self._get_input_type_and_shape_strs(graph, idx)
+            graph_input_param.append(f"{input_type_str}[{input_shape_str}] {input_name}")
+            target_shape.append(input_shape_str)
+        graph_input_param = ','.join(graph_input_param)
+
+        target_shape = target_shape[:1]
+        graph_output_param = []
+        for idx,output_name in enumerate(self.output_names):
+            graph_output_param.append(f"float[{target_shape[idx]}] {output_name}")
+        graph_output_param = ','.join(graph_output_param)
+
+        def split_num_ouputs(num_outputs: int):
+            split_input_shape_attr= ''
+            if onnx_opset >= 18:
+                split_input_shape_attr = f", num_outputs = {num_outputs}"
+            return split_input_shape_attr
+
+        converter_graph = onnx.parser.parse_graph(
+            f"""\
+            ScaleBoundingBoxes ({graph_input_param}) 
+                => ({graph_output_param})  
+            {{
+                i64_2 = Constant <value = int64[1] {{2}}>()
+
+                ori_shape = Shape ({self.input_names[1]})
+                scaled_shape = Shape ({self.input_names[2]})
+                lettered_shape = Shape ({self.input_names[3]})
+                oh,ow,oc = Split <axis = 0 {split_num_ouputs(3)}> (ori_shape)
+                sh,sw,sc = Split <axis = 0 {split_num_ouputs(3)}> (scaled_shape)
+                lh,lw,lc = Split <axis = 0 {split_num_ouputs(3)}> (lettered_shape)
+                swh = Concat <axis = -1> (sw,sh)
+                lwh = Concat <axis = -1> (lw,lh)
+                
+                f_oh = Cast <to = 1> (oh)
+                f_sh = Cast <to = 1> (sh)
+                ratios = Div (f_oh, f_sh)
+                
+                pad_wh = Sub (lwh, swh)
+                half_pad_wh = Div (pad_wh, i64_2)
+                f_half_pad_wh = Cast <to = 1> (half_pad_wh)
+
+                boxes_xy,boxes_wh_orxy,boxes_score_class = Split <axis=-1 {split_num_ouputs(3)}>({self.input_names[0]})
+                offset_boxes_xy = Sub (boxes_xy, f_half_pad_wh)
+                restored_boxes = Concat <axis=-1> (offset_boxes_xy, boxes_wh_orxy)
+                scaled_boxes_coor = Mul (restored_boxes, ratios)
+                restored_boxes_res = Concat <axis=-1> (scaled_boxes_coor, boxes_score_class)
+
+                {self.output_names[0]} = Identity (restored_boxes_res)
+            }}
+            """
+        )
+        return converter_graph
```

## onnxruntime_extensions/util.py

```diff
@@ -1,15 +1,123 @@
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+import onnx
 import pathlib
 import inspect
 
+import numpy as np
 
-# some util function for testing and tools
 
+# some util function for testing and tools
 def get_test_data_file(*sub_dirs):
     case_file = inspect.currentframe().f_back.f_code.co_filename
     test_dir = pathlib.Path(case_file).parent
     return str(test_dir.joinpath(*sub_dirs).resolve())
 
 
-def read_file(path):
-    with open(str(path)) as file_content:
+def read_file(path, mode='r'):
+    with open(str(path), mode) as file_content:
         return file_content.read()
+
+
+def mel_filterbank(
+        n_fft: int, n_mels: int = 80, sr=16000, min_mel=0, max_mel=45.245640471924965, dtype=np.float32):
+    """
+    Compute a Mel-filterbank. The filters are stored in the rows, the columns
+    and it is Slaney normalized mel-scale filterbank.
+    """
+    fbank = np.zeros((n_mels, n_fft // 2 + 1), dtype=dtype)
+
+    # the centers of the frequency bins for the DFT
+    freq_bins = np.fft.rfftfreq(n=n_fft, d=1.0 / sr)
+
+    mel = np.linspace(min_mel, max_mel, n_mels + 2)
+    # Fill in the linear scale
+    f_min = 0.0
+    f_sp = 200.0 / 3
+    freqs = f_min + f_sp * mel
+
+    # And now the nonlinear scale
+    min_log_hz = 1000.0  # beginning of log region (Hz)
+    min_log_mel = (min_log_hz - f_min) / f_sp  # same (Mels)
+    logstep = np.log(6.4) / 27.0  # step size for log region
+
+    log_t = mel >= min_log_mel
+    freqs[log_t] = min_log_hz * np.exp(logstep * (mel[log_t] - min_log_mel))
+    mel_bins = freqs
+
+    mel_spacing = np.diff(mel_bins)
+
+    ramps = mel_bins.reshape(-1, 1) - freq_bins.reshape(1, -1)
+    for i in range(n_mels):
+        left = -ramps[i] / mel_spacing[i]
+        right = ramps[i + 2] / mel_spacing[i + 1]
+
+        # intersect them with each other and zero
+        fbank[i] = np.maximum(0, np.minimum(left, right))
+
+    energy_norm = 2.0 / (mel_bins[2 : n_mels + 2] - mel_bins[:n_mels])
+    fbank *= energy_norm[:, np.newaxis]
+    return fbank
+
+
+def remove_unused_constants(subgraph):
+    nodes = [_n for _n in subgraph.node]
+
+    # Find the names of all input tensors for all nodes in the subgraph
+    input_tensors = set()
+    for node in nodes:
+        for input_name in node.input:
+            input_tensors.add(input_name)
+
+    # Remove Constant nodes whose output is not used by any other nodes
+    nodes_to_remove = []
+    for node in nodes:
+        if node.op_type == 'Constant':
+            output_name = node.output[0]
+            if output_name not in input_tensors:
+                nodes_to_remove.append(node)
+
+    for node in nodes_to_remove:
+        subgraph.node.remove(node)
+
+    # Recursively process subgraphs within this subgraph
+    for node in nodes:
+        for attr in node.attribute:
+            if attr.type == onnx.AttributeProto.GRAPH:
+                remove_unused_constants(attr.g)
+            elif attr.type == onnx.AttributeProto.GRAPHS:
+                for subgraph in attr.graphs:
+                    remove_unused_constants(subgraph)
+
+
+def remove_unused_initializers(subgraph, top_level_initializers=None):
+    if top_level_initializers is None:
+        top_level_initializers = []
+        remove_unused_constants(subgraph)
+    initializers = [_i for _i in subgraph.initializer]
+    nodes = subgraph.node
+
+    # Find the names of all input tensors for all nodes in the subgraph
+    input_tensors = set()
+    for node in nodes:
+        for input_name in node.input:
+            input_tensors.add(input_name)
+
+    # Combine top-level and current subgraph initializers
+    all_initializers = initializers + top_level_initializers
+
+    # Filter the initializers by checking if their names are in the list of used input tensors
+    used_initializers = [init for init in all_initializers if init.name in input_tensors]
+
+    # Update the subgraph's initializers
+    del subgraph.initializer[:]
+    subgraph.initializer.extend([init for init in used_initializers if init in initializers])
+
+    # Recursively process subgraphs within this subgraph
+    for node in nodes:
+        for attr in node.attribute:
+            if attr.type == onnx.AttributeProto.GRAPH:
+                remove_unused_initializers(attr.g, top_level_initializers)
+            elif attr.type == onnx.AttributeProto.GRAPHS:
+                for subgraph in attr.graphs:
+                    remove_unused_initializers(subgraph, top_level_initializers)
```

## onnxruntime_extensions/_cuops.py

```diff
@@ -51,14 +51,60 @@
     def get_outputs(cls):
         return [
             cls.io_def("input_ids", onnx.TensorProto.INT64, [None, None]),
             cls.io_def('attention_mask', onnx.TensorProto.INT64, [None, None])
         ]
 
 
+class CLIPTokenizer(CustomOp):
+
+    @classmethod
+    def get_inputs(cls):
+        return [
+            cls.io_def('input_text', onnx_proto.TensorProto.STRING, [None])
+        ]
+
+    @classmethod
+    def get_outputs(cls):
+        return [
+            cls.io_def("input_ids", onnx.TensorProto.INT64, [None, None]),
+            cls.io_def('attention_mask', onnx.TensorProto.INT64, [None, None]),
+            cls.io_def('offset_mapping', onnx.TensorProto.INT64, [None, None, 2])
+        ]
+
+
+class RobertaTokenizer(CustomOp):
+
+    @classmethod
+    def get_inputs(cls):
+        return [
+            cls.io_def('input_text', onnx_proto.TensorProto.STRING, [None])
+        ]
+
+    @classmethod
+    def get_outputs(cls):
+        return [
+            cls.io_def("input_ids", onnx.TensorProto.INT64, [None, None]),
+            cls.io_def('attention_mask', onnx.TensorProto.INT64, [None, None]),
+            cls.io_def('offset_mapping', onnx.TensorProto.INT64, [None, None, 2])
+        ]
+
+
+class BpeDecoder(CustomOp):
+    @classmethod
+    def get_inputs(cls):
+        return [
+            cls.io_def("ids", onnx.TensorProto.INT64, [])
+        ]
+
+    @classmethod
+    def get_outputs(cls):
+        return [cls.io_def('str', onnx_proto.TensorProto.STRING, [])]
+
+
 class VectorToString(CustomOp):
 
     @classmethod
     def get_inputs(cls):
         return [cls.io_def("token_ids", onnx.TensorProto.INT64, [])]
 
     @classmethod
@@ -340,14 +386,46 @@
     @classmethod
     def get_outputs(cls):
         return [
             cls.io_def('decoded_image', onnx_proto.TensorProto.UINT8, [None, None, 3])
         ]
 
 
+class AudioDecoder(CustomOp):
+    @classmethod
+    def get_inputs(cls):
+        return [
+            cls.io_def('audio_stream', onnx_proto.TensorProto.UINT8, [1, None])
+        ]
+
+    @classmethod
+    def get_outputs(cls):
+        return [
+            cls.io_def('floatPCM', onnx_proto.TensorProto.FLOAT, [1, None])
+        ]
+
+
+class StftNorm(CustomOp):
+    @classmethod
+    def get_inputs(cls):
+        return [
+            cls.io_def('pcm_wave', onnx_proto.TensorProto.FLOAT, [1, None]),
+            cls.io_def('n_fft', onnx_proto.TensorProto.INT64, []),
+            cls.io_def('hop_length', onnx_proto.TensorProto.INT64, []),
+            cls.io_def('window', onnx_proto.TensorProto.FLOAT, [None]),
+            cls.io_def('frame_size', onnx_proto.TensorProto.INT64, []),
+        ]
+
+    @classmethod
+    def get_outputs(cls):
+        return [
+            cls.io_def('stft_norm', onnx_proto.TensorProto.FLOAT, [1, None, None])
+        ]
+
+
 class SingleOpGraph:
 
     @classmethod
     def get_next_id(cls):
         if not hasattr(cls, '_id_counter'):
             cls._id_counter = 0
         cls._id_counter += 1
@@ -383,7 +461,11 @@
     return d[:, ::-1]
 
 
 Opdef.create(_argsort_op,
              op_type='ArgSort',
              inputs=[PyCustomOpDef.dt_float, PyCustomOpDef.dt_int64],
              outputs=[PyCustomOpDef.dt_int64])
+
+
+class CustomOpConverter:
+    pass
```

## onnxruntime_extensions/_ocos.py

```diff
@@ -56,30 +56,46 @@
         opdef._nativedef.input_types = inputs
         outputs = kwargs.get('outputs', None)
         if outputs is None:
             outputs = [PyCustomOpDef.dt_float]
         opdef._nativedef.output_types = outputs
         attrs = kwargs.get('attrs', None)
         if attrs is None:
-            attrs = []
+            attrs = {}
+        elif isinstance(attrs, (list, tuple)):
+                attrs = {k: PyCustomOpDef.dt_string for k in attrs}
         opdef._nativedef.attrs = attrs
         add_custom_op(opdef._nativedef)
         return opdef
 
     def __call__(self, *args, **kwargs):
         return self.body(*args, **kwargs)
 
+    def cast_attributes(self, attributes):
+        res = {}
+        for k, v in attributes.items():
+            if self._nativedef.attrs[k] == PyCustomOpDef.dt_int64:
+                res[k] = int(v)
+            elif self._nativedef.attrs[k] == PyCustomOpDef.dt_float:
+                res[k] = float(v)
+            elif self._nativedef.attrs[k] == PyCustomOpDef.dt_string:
+                res[k] = v
+            else:
+                raise RuntimeError("Unsupported attribute type {}.".format(
+                    self._nativedef.attrs[k]))
+        return res
+
 
 def _on_pyop_invocation(k_id, feed, attributes):
     if k_id not in Opdef._odlist:
         raise RuntimeError(
             "Unable to find function id={}. "
             "Did you decorate the operator with @onnx_op?.".format(k_id))
     op_ = Opdef._odlist[k_id]
-    rv = op_.body(*feed, **attributes)
+    rv = op_.body(*feed, **op_.cast_attributes(attributes))
     if isinstance(rv, tuple):
         # Multiple outputs.
         res = []
         for r in rv:
             res.append(r.shape)
             res.append(r.flatten().tolist())
         res = tuple(res)
```

## onnxruntime_extensions/_ortapi2.py

```diff
@@ -1,71 +1,99 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 ###############################################################################
 
 import numpy as np
-import onnxruntime as _ort
 from ._ocos import default_opset_domain, get_library_path  # noqa
-from ._cuops import *  # noqa
+from ._cuops import onnx, onnx_proto, CustomOpConverter, SingleOpGraph
+
+_ort_check_passed = False
+try:
+    from packaging import version as _ver
+    import onnxruntime as _ort
+    if _ver.parse(_ort.__version__) >= _ver.parse("1.10.0"):
+        _ort_check_passed = True
+except ImportError:
+    pass
+
+if not _ort_check_passed:
+    raise RuntimeError("please install ONNXRuntime/ONNXRuntime-GPU >= 1.10.0")
 
 
 def get_opset_version_from_ort():
     _ORT_OPSET_SUPPORT_TABLE = {
         "1.5": 11,
         "1.6": 12,
         "1.7": 13,
         "1.8": 14,
         "1.9": 15,
         "1.10": 15,
         "1.11": 16,
-        "1.12": 17
+        "1.12": 17,
+        "1.13": 17,
+        "1.14": 18,
     }
 
     ort_ver_string = '.'.join(_ort.__version__.split('.')[0:2])
+    max_ver = max(_ORT_OPSET_SUPPORT_TABLE, key=_ORT_OPSET_SUPPORT_TABLE.get)
+    if ort_ver_string > max_ver:
+        ort_ver_string = max_ver
     return _ORT_OPSET_SUPPORT_TABLE.get(ort_ver_string, 11)
 
 
 def make_onnx_model(graph, opset_version=0, extra_domain=default_opset_domain(), extra_opset_version=1):
     if opset_version == 0:
         opset_version = get_opset_version_from_ort()
     fn_mm = onnx.helper.make_model_gen_version if hasattr(onnx.helper, 'make_model_gen_version'
                                                           ) else onnx.helper.make_model
     model = fn_mm(graph, opset_imports=[
         onnx.helper.make_operatorsetid('ai.onnx', opset_version)])
-    model.opset_import.extend([onnx.helper.make_operatorsetid(extra_domain, extra_opset_version)])
+    model.opset_import.extend(
+        [onnx.helper.make_operatorsetid(extra_domain, extra_opset_version)])
     return model
 
 
 class OrtPyFunction:
 
-    __name__ = 'OrtPyFunction'
-
     @classmethod
     def get_ort_session_options(cls):
         # ONNXRuntime has an issue to support reusing the SessionOptions object.
         # Create a new one every time here
         so = _ort.SessionOptions()
         so.register_custom_ops_library(get_library_path())
         return so
 
-    def __init__(self):
+    def __init__(self, cpu_only=None):
         self._onnx_model = None
         self.ort_session = None
         self.default_inputs = {}
+        self.execution_providers = ['CPUExecutionProvider']
+        if not cpu_only:
+            if _ort.get_device() == 'GPU':
+                self.execution_providers = ['CUDAExecutionProvider']
 
     def create_from_customop(self, op_type, *args, **kwargs):
-        graph = SingleOpGraph.build_my_graph(op_type, *args, **kwargs)
+        cvt = kwargs.get('cvt', None)
+        if cvt is None:
+            cvt = args[0] if len(args) > 0 and isinstance(
+                args[0], CustomOpConverter) else None
+            args = args[1:]
+        else:
+            del kwargs['cvt']
+
+        new_kwargs = kwargs if cvt is None else cvt(**kwargs)
+        graph = SingleOpGraph.build_my_graph(op_type, *args, **new_kwargs)
         self._bind(make_onnx_model(graph))
         return self
 
     def add_default_input(self, **kwargs):
         inputs = {
-            ky_: val_ if isinstance(val_, (np.ndarray, np.generic)) else \
-                np.asarray(list(val_), dtype=np.uint8) for ky_, val_ in kwargs.items()
+            ky_: val_ if isinstance(val_, (np.ndarray, np.generic)) else
+            np.asarray(list(val_), dtype=np.uint8) for ky_, val_ in kwargs.items()
         }
 
         self.default_inputs.update(inputs)
 
     @property
     def onnx_model(self):
         assert self._oxml is not None, "No onnx model attached yet."
@@ -75,57 +103,77 @@
     def input_names(self):
         return [vi_.name for vi_ in self.onnx_model.graph.input]
 
     @property
     def output_names(self):
         return [vi_.name for vi_ in self.onnx_model.graph.output]
 
-    def _bind(self, oxml):
+    def _bind(self, oxml, model_path=None):
         self.inputs = list(oxml.graph.input)
         self.outputs = list(oxml.graph.output)
         self._oxml = oxml
+        if model_path is not None:
+            self.ort_session = _ort.InferenceSession(
+                model_path, self.get_ort_session_options(),
+                self.execution_providers)
         return self
 
     def _ensure_ort_session(self):
         if self.ort_session is None:
-            sess = _ort.InferenceSession(self.onnx_model.SerializeToString(), self.get_ort_session_options())
+            sess = _ort.InferenceSession(
+                self.onnx_model.SerializeToString(), self.get_ort_session_options(),
+                self.execution_providers)
             self.ort_session = sess
 
         return self.ort_session
 
+    @staticmethod
+    def _get_kwarg_device(kwargs):
+        cpuonly = kwargs.get('cpu_only', None)
+        if cpuonly is not None:
+            del kwargs['cpu_only']
+        return cpuonly
+
     @classmethod
     def from_customop(cls, op_type, *args, **kwargs):
-        return cls().create_from_customop(op_type, *args, **kwargs)
+        return cls(cls._get_kwarg_device(kwargs)).create_from_customop(op_type, *args, **kwargs)
 
     @classmethod
     def from_model(cls, path_or_model, *args, **kwargs):
-        return cls()._bind(onnx.load_model(path_or_model) if isinstance(path_or_model, str) else path_or_model)
+        mpath = None
+        if isinstance(path_or_model, str):
+            oxml = onnx.load_model(path_or_model)
+            mpath = path_or_model
+        else:
+            oxml = path_or_model
+        return cls(cls._get_kwarg_device(kwargs))._bind(oxml, mpath)
 
     def _argument_map(self, *args, **kwargs):
         idx = 0
         feed = {}
         for i_ in self.inputs:
             if i_.name in self.default_inputs:
                 feed[i_.name] = self.default_inputs[i_.name]
                 continue
 
             x = args[idx]
             ts_x = np.array(x) if isinstance(x, (int, float, bool)) else x
-            # an annoying bug is numpy by default is int32, while pytorch is int64.
-            # so cast the input here automatically.
+            # numpy by default is int32 in some platforms, sometimes it is int64.
             feed[i_.name] = \
-                ts_x.astype(np.int64) if i_.type.tensor_type.elem_type == onnx_proto.TensorProto.INT64 else ts_x
+                ts_x.astype(
+                    np.int64) if i_.type.tensor_type.elem_type == onnx_proto.TensorProto.INT64 else ts_x
             idx += 1
 
         # feed.update(kwargs)
         return feed
 
     def __call__(self, *args, **kwargs):
         self._ensure_ort_session()
-        outputs = self.ort_session.run(None, self._argument_map(*args, **kwargs))
+        outputs = self.ort_session.run(
+            None, self._argument_map(*args, **kwargs))
         return outputs[0] if len(outputs) == 1 else tuple(outputs)
 
 
 def optimize_model(model_or_file, output_file):
     sess_options = OrtPyFunction.get_ort_session_options()
     sess_options.graph_optimization_level = _ort.GraphOptimizationLevel.ORT_ENABLE_BASIC
     sess_options.optimized_model_filepath = output_file
```

## onnxruntime_extensions/_version.py

```diff
@@ -1,2 +1,2 @@
 # Generated by setup.py, DON'T MANUALLY UPDATE IT!
-__version__ = "0.7.0"
+__version__ = "0.8.0"
```

## onnxruntime_extensions/__init__.py

```diff
@@ -1,27 +1,27 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 ###############################################################################
 
 """
-The entry point to onnxruntime custom op library
+The entry point to onnxruntime-extensions package.
 """
 
 __author__ = "Microsoft"
 
+
 from ._version import __version__
 from ._ocos import get_library_path  # noqa
 from ._ocos import Opdef, PyCustomOpDef # noqa
 from ._ocos import hash_64 # noqa
 from ._ocos import enable_py_op  # noqa
 from ._ocos import expand_onnx_inputs  # noqa
 from ._ocos import hook_model_op  # noqa
 from ._ocos import default_opset_domain  # noqa
 from ._cuops import *  # noqa
-from ._ortapi2 import OrtPyFunction as PyOrtFunction # backward compatibility
+from ._ortapi2 import OrtPyFunction as PyOrtFunction  # backward compatibility
 from ._ortapi2 import OrtPyFunction, optimize_model, make_onnx_model, ONNXRuntimeError
 
 
 onnx_op = Opdef.declare
 PyOp = PyCustomOpDef
-
```

## Comparing `onnxruntime_extensions-0.7.0.dist-info/LICENSE` & `onnxruntime_extensions-0.8.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `onnxruntime_extensions-0.7.0.dist-info/METADATA` & `onnxruntime_extensions-0.8.0.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: onnxruntime-extensions
-Version: 0.7.0
+Version: 0.8.0
 Summary: ONNXRuntime Extensions
 Home-page: https://github.com/microsoft/onnxruntime-extensions
 Author: Microsoft Corporation
 Author-email: onnxruntime@microsoft.com
 License: MIT License
 Classifier: Development Status :: 4 - Beta
 Classifier: Environment :: Console
@@ -15,39 +15,39 @@
 Classifier: Programming Language :: C++
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: Implementation :: CPython
 Classifier: License :: OSI Approved :: MIT License
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: onnx (>=1.9.0)
-Requires-Dist: onnxruntime (>=1.10.0)
 
 # ONNXRuntime-Extensions
 
 [![Build Status](https://aiinfra.visualstudio.com/Lotus/_apis/build/status/onnxruntime-extensions/extensions.wheel?branchName=main)](https://aiinfra.visualstudio.com/Lotus/_build/latest?definitionId=1085&branchName=main)
 
 ## What's ONNXRuntime-Extensions
 
 Introduction: ONNXRuntime-Extensions is a library that extends the capability of the ONNX models and inference with ONNX Runtime, via ONNX Runtime Custom Operator ABIs. It includes a set of [ONNX Runtime Custom Operator](https://onnxruntime.ai/docs/reference/operators/add-custom-op.html) to support the common pre- and post-processing operators for vision, text, and nlp models. And it supports multiple languages and platforms, like Python on Windows/Linux/macOS, some mobile platforms like Android and iOS, and Web-Assembly etc. The basic workflow is to enhance a ONNX model firstly and then do the model inference with ONNX Runtime and ONNXRuntime-Extensions package.
 
-<table>
-<tr>
-<td>⚠️</td>
-<td>
-<strong>NOTE:</strong> most ONNXRuntime-Extensions packages are in <strong><em>active development</em></strong> and most packages require building from source. The package information will be updated here if it is published.
-</td>
-</tr>
-</table>
 
-## Quickstart with the experimental Python package
+## Quickstart
 
-#### <strong>on Windows</strong>
+### **Python installation**
 ```bash
+pip install onnxruntime-extensions
+````
+
+
+### **nightly build**
+
+#### <strong>on Windows</strong>
+```cmd
 pip install --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple/ onnxruntime-extensions
 ```
+Please ensure that you have met the prerequisites of onnxruntime-extensions (e.g., onnx and onnxruntime) in your Python environment.
 #### <strong>on Linux/macOS</strong>
 the packages are not ready yet, so it could be installed from source. Please make sure the compiler toolkit like gcc(later than g++ 8.0) or clang, and the tool <strong>cmake</strong> are installed before the following command
 ```bash
 python -m pip install git+https://github.com/microsoft/onnxruntime-extensions.git
 ```
 
 
@@ -95,15 +95,15 @@
 
 1. [CustomOp conversion by pytorch.onnx.exporter](https://github.com/microsoft/onnxruntime-extensions/blob/main/tutorials/pytorch_custom_ops_tutorial.ipynb)
 2. [CustomOp conversion by tf2onnx](https://github.com/microsoft/onnxruntime-extensions/blob/main/tutorials/tf2onnx_custom_ops_tutorial.ipynb)
 
 
 ## Add a new custom operator to onnxruntime-extensions
 
-You can contribute customop C++ implementations directly in this repository if they have general applicability to other users. In addition, if you want to quickly verify the ONNX model with Python, you can wrap the custom operator with PyOp.
+You can contribute customop C++ implementations directly in this repository if they have general applicability to other users. In addition, if you want to quickly verify the ONNX model with Python, you can wrap the custom operator with **[PyOp](docs/pyop.md)**.
 
 ```python
 import numpy
 from onnxruntime_extensions import PyOp, onnx_op
 
 # Implement the CustomOp by decorating a function with onnx_op
 @onnx_op(op_type="Inverse", inputs=[PyOp.dt_float])
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## Comparing `onnxruntime_extensions-0.7.0.dist-info/RECORD` & `onnxruntime_extensions-0.8.0.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,38 +1,39 @@
-onnxruntime_extensions/__init__.py,sha256=hgOkSMjzY5-_yUeg4-bwPIr9u_64-0irDCYSNwmXMeA,926
-onnxruntime_extensions/_cuops.py,sha256=15e8EY5fyXL6QRidGCcffmR-ERgIYDiNfl9KgxKfWFA,11562
-onnxruntime_extensions/_extensions_pydll.cp39-win_amd64.pyd,sha256=5v4yYotEb3RupJYvpCSg454gdAT-ysqA5_L95a_I8Bs,4196864
-onnxruntime_extensions/_ocos.py,sha256=p6aOIPWBWGKsyXEb3q16rhdh77Dt7f55nBEfp4v5SRs,5777
-onnxruntime_extensions/_ortapi2.py,sha256=-ttanK-iOs5fpqVHDe96ZCqjujUvJKOJpjPFvDH8e_g,4902
-onnxruntime_extensions/_version.py,sha256=BWo7hTpGR5dRo5XjqXTcz26ew6yqu-kjJW2kluXEQzM,75
+onnxruntime_extensions/__init__.py,sha256=swmC9_r2HdBj4gcm3_Pg9YmWCXTVbzpaiG75_2e_6eE,929
+onnxruntime_extensions/_cuops.py,sha256=Xsuh7liVYxw3-xrz8az7sA7acr_eco5HzYYcHYCibs8,13867
+onnxruntime_extensions/_extensions_pydll.cp39-win_amd64.pyd,sha256=NNuAz5i0yGCGEZtbnulKHFIsl4AGwZ1B4HC_1pgGSww,4409856
+onnxruntime_extensions/_ocos.py,sha256=VNpaQuRZ4jrPl-PZ_rFdVQAWulsTPiybVX_286pGOZw,6489
+onnxruntime_extensions/_ortapi2.py,sha256=bZfk3C0cRiaXq_i-MCo1ghMRIbrLQ-6L8sgiYmRrogw,6599
+onnxruntime_extensions/_version.py,sha256=6NOfYujzAt6PDNfc4W5EqVPkylNawZdE9tUbM9z7zHk,75
 onnxruntime_extensions/cmake_helper.py,sha256=ms5eXDjEbbGLw3s8cYjYA_BR7CYdP1SIIvy0rxxryEs,1639
-onnxruntime_extensions/cmd.py,sha256=53ogSfBiaYyRqIULH1yiww544XixKW879RpQSdJjzCs,1440
-onnxruntime_extensions/util.py,sha256=GVOBOWOFBPoXqCcIuOHqMRHQxGHYm38Jv2xKwNhwlwo,392
+onnxruntime_extensions/cmd.py,sha256=GIWnaW-QNe4436DqoYaUi_5hcMtL5ccQ8GygdFJM_RI,2124
+onnxruntime_extensions/cvt.py,sha256=uyq2M9WTLPxltk12UKCsqsff_hFer6OCqI5ioRVhFYU,2806
+onnxruntime_extensions/util.py,sha256=XfEpi3_m63-2w7nabNLmEf4sGy63i3qPlfBrTe7zyyA,4419
 onnxruntime_extensions/onnxprocess/__init__.py,sha256=BnveHXnu2nTQNbCLeZujZgZwO9A3yWFbQGTDthCFbIc,534
-onnxruntime_extensions/onnxprocess/_builder.py,sha256=H-0H5PVait8BSGG5pesX3oV2wXGR7MQS2qBeKUTdkBk,1823
+onnxruntime_extensions/onnxprocess/_builder.py,sha256=L_afKeE7Wc4mWJ47eVXQ2stvmal_37QVTQZgKmt0ZK8,1844
 onnxruntime_extensions/onnxprocess/_onnx_ops.py,sha256=CYa1_yOEqpnfyvIea_gYpyRzjh82LVH6QB-eKvoEa-w,73255
 onnxruntime_extensions/onnxprocess/_session.py,sha256=lVm8U2UxF_FONBIAsRT87noBfVYz53CIx1FReaCTa0o,15158
 onnxruntime_extensions/onnxprocess/_tensor.py,sha256=COboqrHcfyCNL-H4byQNu0G0F7bV8vOlRe21lUKC2OU,25410
 onnxruntime_extensions/onnxprocess/torch_wrapper.py,sha256=nFF6wqiXUQy_PoZn2SunnoErBRo-2FD-R-1kHLavXVE,859
 onnxruntime_extensions/pnp/__init__.py,sha256=IKR_f0Ts-XtByS9jynv5-ShV-3PbN0mHmx4jGbtT2sQ,495
-onnxruntime_extensions/pnp/_base.py,sha256=AxW9RhX7i-mTL6oLxTQUdmGckoO3Q56jATOcsHVVGTc,3730
+onnxruntime_extensions/pnp/_base.py,sha256=PHR_5R2WuWQ6gIfF9d0TIEjNbPmRaDrHmrR1Mlj3U5A,3928
 onnxruntime_extensions/pnp/_imagenet.py,sha256=LcRzsr3uCURwUIfAjY8iDWExpG3FUqj_jOWmuW0bp6U,2452
 onnxruntime_extensions/pnp/_nlp.py,sha256=lhPGkGLZuKWCyp9ZEBvzxw1jOv85VBOK-5XMoXvY2bE,7420
-onnxruntime_extensions/pnp/_onnx_ops.py,sha256=bAcfepJ5-zbZGJSPm0rDZTrg318NSDWYQwem4CZFquM,74284
+onnxruntime_extensions/pnp/_onnx_ops.py,sha256=836Bc_RUFJOVt4iS2c_vX1Un2LKmUlYemz3-2-aNFeg,74398
 onnxruntime_extensions/pnp/_torchext.py,sha256=SVYut2bBKvEbwl4luEF6IaLpU5FFJ2IfwMHIbGiP3TI,11927
 onnxruntime_extensions/pnp/_unifier.py,sha256=FPQYL1Z6f1Tv2qRsnhW_is9k7-GmCYhf6ZIXPU95gmU,1649
-onnxruntime_extensions/pnp/_utils.py,sha256=mc7AmYViuHL0d44TjwVouw_VnMQSFUGgz33nqRbwHZg,12435
+onnxruntime_extensions/pnp/_utils.py,sha256=xBh7-_VstgqXlhBaQ_6E5GV6341ywCRQsrJZZZtYaCc,13061
 onnxruntime_extensions/tools/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-onnxruntime_extensions/tools/add_pre_post_processing_to_model.py,sha256=GA_AHvXQjeY53ysv2BZd42lw3iZ7NtWZnW0N5DM1CnM,14768
+onnxruntime_extensions/tools/add_pre_post_processing_to_model.py,sha256=6J65GBk1bn_j25VJX4L7CFtDNWg4dVuE1M-iQwPQdY4,23211
 onnxruntime_extensions/tools/pre_post_processing/__init__.py,sha256=YKxCtG2McBExYYmcf1tbqDquqIS1iTs4iPx86MBcfRo,125
-onnxruntime_extensions/tools/pre_post_processing/pre_post_processor.py,sha256=y1CgwGWROwMOY1caYdLZYHCpztdhu2LOyuxrHKz7lJE,19070
+onnxruntime_extensions/tools/pre_post_processing/pre_post_processor.py,sha256=xbLAVoQ9r7sofn0rFQHKqE3KIh5VI7cKx9REtlR55fQ,19445
 onnxruntime_extensions/tools/pre_post_processing/step.py,sha256=4hH-Ve53sqUVl5xyuHkFQeasmD2-WnDLURaG6zgmmHc,9130
 onnxruntime_extensions/tools/pre_post_processing/utils.py,sha256=-0soK4Md5EAp5fQ6tTXhI9EC-VOz-J6lST9arWwvXx0,5698
 onnxruntime_extensions/tools/pre_post_processing/steps/__init__.py,sha256=pdVRZBE7jCym0V8RpyVoiFeq1d4CwQJAh589pvbff-g,165
 onnxruntime_extensions/tools/pre_post_processing/steps/general.py,sha256=9rNSdcquTMy5YQaLCppbKgbit8u1CD3vvMWd09efITs,9380
 onnxruntime_extensions/tools/pre_post_processing/steps/nlp.py,sha256=ZHHv8LoikPALJ-tBLHmJxk-ac8kLInNnrw8EmZS4JxQ,14727
-onnxruntime_extensions/tools/pre_post_processing/steps/vision.py,sha256=us-3lqeQ_YQpDqtJPXb3Jfyn_3D8DQjE_FdFLncs5TQ,25664
-onnxruntime_extensions-0.7.0.dist-info/LICENSE,sha256=mQaUD2Gx8LUz-n2ZuvVReLKAj74RPqUd-_rYVyzNXys,1162
-onnxruntime_extensions-0.7.0.dist-info/METADATA,sha256=CW32HRRjmunJcucmt0MSwifhfixgoc6gz3zOREnuCrw,5326
-onnxruntime_extensions-0.7.0.dist-info/WHEEL,sha256=J_4V_gB-O6Y7Pn6lk91K27JaIhI-q07YM5J8Ufzqla4,100
-onnxruntime_extensions-0.7.0.dist-info/top_level.txt,sha256=XyAgQDKyXsf6_0MJb58kRdHwigpTn7A7kl9diBEjs8M,23
-onnxruntime_extensions-0.7.0.dist-info/RECORD,,
+onnxruntime_extensions/tools/pre_post_processing/steps/vision.py,sha256=HSl43Voswo6GkZTosx5oFCLLdrMTzdCsFpCIqc1Faik,44443
+onnxruntime_extensions-0.8.0.dist-info/LICENSE,sha256=mQaUD2Gx8LUz-n2ZuvVReLKAj74RPqUd-_rYVyzNXys,1162
+onnxruntime_extensions-0.8.0.dist-info/METADATA,sha256=le57yFFfYSiIM8nfH22sEAznATNQzPZaY2_76Kd2gzE,5224
+onnxruntime_extensions-0.8.0.dist-info/WHEEL,sha256=eep6QWEFiQfg2wcclssb_WY-D33AnLYLnEKGA9Rn-VU,100
+onnxruntime_extensions-0.8.0.dist-info/top_level.txt,sha256=XyAgQDKyXsf6_0MJb58kRdHwigpTn7A7kl9diBEjs8M,23
+onnxruntime_extensions-0.8.0.dist-info/RECORD,,
```

