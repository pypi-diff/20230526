# Comparing `tmp/optool-0.2.0-py3-none-any.whl.zip` & `tmp/optool-0.3.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,38 +1,39 @@
-Zip file size: 50031 bytes, number of entries: 36
--rw-rw-rw-  2.0 unx        6 b- defN 23-Mar-24 14:35 optool/VERSION
--rw-r--r--  2.0 unx     4993 b- defN 23-Mar-24 14:35 optool/__init__.py
--rw-r--r--  2.0 unx      949 b- defN 23-Mar-24 14:35 optool/conversions.py
--rw-r--r--  2.0 unx     1094 b- defN 23-Mar-24 14:35 optool/languages.py
--rw-r--r--  2.0 unx     4095 b- defN 23-Mar-24 14:35 optool/logging.py
--rw-r--r--  2.0 unx     7758 b- defN 23-Mar-24 14:35 optool/math.py
--rw-r--r--  2.0 unx     1148 b- defN 23-Mar-24 14:35 optool/orthography.py
--rw-r--r--  2.0 unx     1814 b- defN 23-Mar-24 14:35 optool/parallel.py
--rw-rw-rw-  2.0 unx        0 b- defN 23-Mar-24 14:35 optool/py.typed
--rw-r--r--  2.0 unx    31001 b- defN 23-Mar-24 14:35 optool/uom.py
--rw-r--r--  2.0 unx      833 b- defN 23-Mar-24 14:35 optool/util.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-24 14:35 optool/fields/__init__.py
--rw-r--r--  2.0 unx     4075 b- defN 23-Mar-24 14:35 optool/fields/callables.py
--rw-r--r--  2.0 unx     4363 b- defN 23-Mar-24 14:35 optool/fields/containers.py
--rw-r--r--  2.0 unx     2229 b- defN 23-Mar-24 14:35 optool/fields/dataframe.py
--rw-r--r--  2.0 unx      403 b- defN 23-Mar-24 14:35 optool/fields/misc.py
--rw-r--r--  2.0 unx     7116 b- defN 23-Mar-24 14:35 optool/fields/numeric.py
--rw-r--r--  2.0 unx     5055 b- defN 23-Mar-24 14:35 optool/fields/quantities.py
--rw-r--r--  2.0 unx     6353 b- defN 23-Mar-24 14:35 optool/fields/series.py
--rw-r--r--  2.0 unx     2413 b- defN 23-Mar-24 14:35 optool/fields/symbolic.py
--rw-r--r--  2.0 unx     6556 b- defN 23-Mar-24 14:35 optool/fields/util.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-24 14:35 optool/optimization/__init__.py
--rw-r--r--  2.0 unx     4217 b- defN 23-Mar-24 14:35 optool/optimization/constraints.py
--rw-r--r--  2.0 unx     4535 b- defN 23-Mar-24 14:35 optool/optimization/helpers.py
--rw-r--r--  2.0 unx     8014 b- defN 23-Mar-24 14:35 optool/optimization/ode.py
--rw-r--r--  2.0 unx    21377 b- defN 23-Mar-24 14:35 optool/optimization/problem.py
--rw-r--r--  2.0 unx     7159 b- defN 23-Mar-24 14:35 optool/optimization/variables.py
--rw-r--r--  2.0 unx     2755 b- defN 23-Mar-24 14:35 optool/serialization/__init__.py
--rw-r--r--  2.0 unx     2457 b- defN 23-Mar-24 14:35 optool/serialization/dataframes.py
--rw-r--r--  2.0 unx      538 b- defN 23-Mar-24 14:35 optool/serialization/numeric.py
--rw-r--r--  2.0 unx      729 b- defN 23-Mar-24 14:35 optool/serialization/quantities.py
--rw-rw-rw-  2.0 unx     1081 b- defN 23-Mar-24 14:36 optool-0.2.0.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx     9017 b- defN 23-Mar-24 14:36 optool-0.2.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Mar-24 14:36 optool-0.2.0.dist-info/WHEEL
--rw-r--r--  2.0 unx        7 b- defN 23-Mar-24 14:36 optool-0.2.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2879 b- defN 23-Mar-24 14:36 optool-0.2.0.dist-info/RECORD
-36 files, 157111 bytes uncompressed, 45471 bytes compressed:  71.1%
+Zip file size: 50248 bytes, number of entries: 37
+-rw-rw-rw-  2.0 unx        6 b- defN 23-May-26 08:49 optool/VERSION
+-rw-r--r--  2.0 unx     5469 b- defN 23-May-26 08:49 optool/__init__.py
+-rw-r--r--  2.0 unx      949 b- defN 23-May-26 08:49 optool/conversions.py
+-rw-r--r--  2.0 unx     1094 b- defN 23-May-26 08:49 optool/languages.py
+-rw-r--r--  2.0 unx     4095 b- defN 23-May-26 08:49 optool/logging.py
+-rw-r--r--  2.0 unx     7758 b- defN 23-May-26 08:49 optool/math.py
+-rw-r--r--  2.0 unx     1148 b- defN 23-May-26 08:49 optool/orthography.py
+-rw-r--r--  2.0 unx     1814 b- defN 23-May-26 08:49 optool/parallel.py
+-rw-rw-rw-  2.0 unx        0 b- defN 23-May-26 08:49 optool/py.typed
+-rw-r--r--  2.0 unx    31001 b- defN 23-May-26 08:49 optool/uom.py
+-rw-r--r--  2.0 unx      833 b- defN 23-May-26 08:49 optool/util.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-26 08:49 optool/fields/__init__.py
+-rw-r--r--  2.0 unx     3914 b- defN 23-May-26 08:49 optool/fields/callables.py
+-rw-r--r--  2.0 unx     4363 b- defN 23-May-26 08:49 optool/fields/containers.py
+-rw-r--r--  2.0 unx     2122 b- defN 23-May-26 08:49 optool/fields/dataframe.py
+-rw-r--r--  2.0 unx      829 b- defN 23-May-26 08:49 optool/fields/misc.py
+-rw-r--r--  2.0 unx     6940 b- defN 23-May-26 08:49 optool/fields/numeric.py
+-rw-r--r--  2.0 unx     5143 b- defN 23-May-26 08:49 optool/fields/quantities.py
+-rw-r--r--  2.0 unx     6155 b- defN 23-May-26 08:49 optool/fields/series.py
+-rw-r--r--  2.0 unx     2335 b- defN 23-May-26 08:49 optool/fields/symbolic.py
+-rw-r--r--  2.0 unx     5848 b- defN 23-May-26 08:49 optool/fields/util.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-26 08:49 optool/optimization/__init__.py
+-rw-r--r--  2.0 unx     4206 b- defN 23-May-26 08:49 optool/optimization/constraints.py
+-rw-r--r--  2.0 unx     4535 b- defN 23-May-26 08:49 optool/optimization/helpers.py
+-rw-r--r--  2.0 unx     8014 b- defN 23-May-26 08:49 optool/optimization/ode.py
+-rw-r--r--  2.0 unx    21135 b- defN 23-May-26 08:49 optool/optimization/problem.py
+-rw-r--r--  2.0 unx     7159 b- defN 23-May-26 08:49 optool/optimization/variables.py
+-rw-r--r--  2.0 unx     2862 b- defN 23-May-26 08:49 optool/serialization/__init__.py
+-rw-r--r--  2.0 unx     1317 b- defN 23-May-26 08:49 optool/serialization/datetime_objects.py
+-rw-r--r--  2.0 unx      616 b- defN 23-May-26 08:49 optool/serialization/numpy_objects.py
+-rw-r--r--  2.0 unx     2880 b- defN 23-May-26 08:49 optool/serialization/pandas_objects.py
+-rw-r--r--  2.0 unx     1217 b- defN 23-May-26 08:49 optool/serialization/pint_objects.py
+-rw-rw-rw-  2.0 unx     1081 b- defN 23-May-26 08:49 optool-0.3.0.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx     7771 b- defN 23-May-26 08:49 optool-0.3.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-May-26 08:49 optool-0.3.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        7 b- defN 23-May-26 08:49 optool-0.3.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2989 b- defN 23-May-26 08:49 optool-0.3.0.dist-info/RECORD
+37 files, 157697 bytes uncompressed, 45508 bytes compressed:  71.1%
```

## zipnote {}

```diff
@@ -78,32 +78,35 @@
 
 Filename: optool/optimization/variables.py
 Comment: 
 
 Filename: optool/serialization/__init__.py
 Comment: 
 
-Filename: optool/serialization/dataframes.py
+Filename: optool/serialization/datetime_objects.py
 Comment: 
 
-Filename: optool/serialization/numeric.py
+Filename: optool/serialization/numpy_objects.py
 Comment: 
 
-Filename: optool/serialization/quantities.py
+Filename: optool/serialization/pandas_objects.py
 Comment: 
 
-Filename: optool-0.2.0.dist-info/LICENSE.txt
+Filename: optool/serialization/pint_objects.py
 Comment: 
 
-Filename: optool-0.2.0.dist-info/METADATA
+Filename: optool-0.3.0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: optool-0.2.0.dist-info/WHEEL
+Filename: optool-0.3.0.dist-info/METADATA
 Comment: 
 
-Filename: optool-0.2.0.dist-info/top_level.txt
+Filename: optool-0.3.0.dist-info/WHEEL
 Comment: 
 
-Filename: optool-0.2.0.dist-info/RECORD
+Filename: optool-0.3.0.dist-info/top_level.txt
+Comment: 
+
+Filename: optool-0.3.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## optool/VERSION

```diff
@@ -1 +1 @@
-0.2.0
+0.3.0
```

## optool/__init__.py

```diff
@@ -4,17 +4,19 @@
 
 import numpy as np
 import pydantic
 from pydantic import BaseConfig, Extra
 
 from optool.logging import LOGGER
 from optool.serialization import SerializationAssistant
-from optool.serialization.dataframes import DataFrameSerializer, SeriesSerializer
-from optool.serialization.numeric import NumpySerializer
-from optool.serialization.quantities import QuantitySerializer, UnitSerializer
+from optool.serialization.datetime_objects import DatetimeSerializer, ZoneInfoSerializer
+from optool.serialization.numpy_objects import NumpyNdArraySerializer
+from optool.serialization.pandas_objects import (PandasDataFrameSerializer, PandasDatetimeIndexSerializer,
+                                                 PandasRangeIndexSerializer, PandasSeriesSerializer)
+from optool.serialization.pint_objects import PintArraySerializer, PintQuantitySerializer, PintUnitSerializer
 
 
 def _recursive_dict_eq(dict1, dict2):
     """
     Recursively compare two dictionaries, handling arrays.
     """
     if set(dict1.keys()) != set(dict2.keys()):
@@ -122,19 +124,24 @@
 
         json_loads = SerializationAssistant.json_loader
         """
         Customized JSON loader that can deserialize specified objects.
         """
 
         json_encoders = SerializationAssistant.register(
-            QuantitySerializer(),
-            NumpySerializer(),
-            UnitSerializer(),
-            DataFrameSerializer(),
-            SeriesSerializer(),
+            NumpyNdArraySerializer(),
+            PintQuantitySerializer(),
+            PintUnitSerializer(),
+            PintArraySerializer(),
+            PandasDataFrameSerializer(),
+            PandasSeriesSerializer(),
+            PandasRangeIndexSerializer(),
+            PandasDatetimeIndexSerializer(),
+            ZoneInfoSerializer(),
+            DatetimeSerializer(),
         )
         """
         Customized JSON encoder that can serialize specified objects.
         """
 
 
 validate_arguments = pydantic.validate_arguments(config=dict(arbitrary_types_allowed=True))
```

## optool/fields/callables.py

```diff
@@ -1,31 +1,26 @@
 from __future__ import annotations
 
 import inspect
 from typing import Any, Callable, Optional, Type, Union
 
-from pydantic.errors import PydanticValueError
-
 from optool.fields.util import check_only_one_specified, get_type_validator
 
 
-class CallableParameterError(PydanticValueError):
+class CallableParameterError(ValueError):
 
     def __init__(self, *, spec: str, expected: Any, value: inspect.FullArgSpec) -> None:
-        super().__init__(spec=spec, expected=expected, actual=value)
-
-    msg_template = 'expected the {spec} {expected}, but got a value with argument specification {actual}'
+        super().__init__(f"expected the {spec} {expected}, but got a value with argument specification {value}")
 
 
-class UnverifiableCallableParameterError(PydanticValueError):
+class UnverifiableCallableParameterError(ValueError):
 
     def __init__(self, *, spec: str, expected: Any, value: inspect.FullArgSpec) -> None:
-        super().__init__(spec=spec, expected=expected, actual=value)
-
-    msg_template = 'cannot verify the callable with argument specification {actual} for expected {spec} {expected}'
+        super().__init__(f"cannot verify the callable with argument specification {value} "
+                         f"for expected {spec} {expected}")
 
 
 class ConstrainedCallable:
     """
     Pydantic-compatible field type for :py:class:`typing.Callable` objects.
 
     See Also:
```

## optool/fields/dataframe.py

```diff
@@ -1,23 +1,20 @@
 from typing import TYPE_CHECKING, Any, Type
 
 import pandas as pd
 from pandas import DatetimeIndex, Index, TimedeltaIndex
-from pydantic.errors import PydanticValueError
 from pydantic.fields import ModelField
 
 from optool.fields.util import get_type_validator
 
 
-class IndexTypeError(PydanticValueError):
+class IndexTypeError(ValueError):
 
     def __init__(self, *, expected: Type[Index], value: pd.DataFrame) -> None:
-        super().__init__(expected=expected, actual=type(value.index))
-
-    msg_template = 'expected index type {expected}, but got a DataFrame with index type {actual}'
+        super().__init__(f"expected index type {expected}, but got a DataFrame with index type {type(value.index)}")
 
 
 class ConstrainedDataFrame(pd.DataFrame):
     """
     Pydantic-compatible field type for :py:class:`pandas.DataFrame` objects, which allows to specify the index type.
 
     See Also:
```

## optool/fields/misc.py

```diff
@@ -1,19 +1,36 @@
 from typing import TYPE_CHECKING
 
 from pydantic import ConstrainedFloat, ConstrainedStr
 
 if TYPE_CHECKING:
     NonEmptyStr = str
+
+    FractionFloat = float
     PositiveFiniteFloat = float
+    NonNegativeFiniteFloat = float
 
 else:
 
     class NonEmptyStr(ConstrainedStr):
         strict = True
         strip_whitespace = True
         min_length = 1
 
+    class FractionFloat(ConstrainedFloat):
+        """
+        A number that needs to be greater or equal to zero and smaller or equal to one.
+        """
+        strict = False
+        ge = 0.0
+        le = 1.0
+        allow_inf_nan = False
+
     class PositiveFiniteFloat(ConstrainedFloat):
         strict = False
         gt = 0
         allow_inf_nan = False
+
+    class NonNegativeFiniteFloat(ConstrainedFloat):
+        strict = False
+        ge = 0
+        allow_inf_nan = False
```

## optool/fields/numeric.py

```diff
@@ -1,42 +1,36 @@
 from __future__ import annotations
 
 import itertools
 import numbers
 from typing import TYPE_CHECKING, Any, Generic, Iterable, Optional, Type, TypeVar, Union
 
 import numpy as np
-from pydantic.errors import PydanticValueError
 from pydantic.fields import ModelField
 
 from optool.fields.util import WrongTypeError, check_only_one_specified, check_sub_fields_level, get_type_validator
 
 
-class ShapeError(PydanticValueError):
+class ShapeError(ValueError):
 
     def __init__(self, *, expected: tuple[int, ...], value: np.ndarray) -> None:
-        super().__init__(expected=expected, actual=value.shape)
+        super().__init__(f"expected the shape {expected}, but got a value with shape {value.shape}")
 
-    msg_template = 'expected the shape {expected}, but got a value with shape {actual}'
 
-
-class DimensionsError(PydanticValueError):
+class DimensionsError(ValueError):
 
     def __init__(self, *, expected: int, value: np.ndarray) -> None:
-        super().__init__(expected=expected, actual=np.ndim(value))
-
-    msg_template = 'expected {expected} dimension(s), but got a value with {actual} dimension(s)'
+        super().__init__(f"expected {expected} dimension(s), but got a value with {np.ndim(value)} dimension(s)")
 
 
-class ArrayWriteableError(PydanticValueError):
+class ArrayWriteableError(ValueError):
 
     def __init__(self, *, expected: bool, value: np.ndarray) -> None:
-        super().__init__(expected=expected, actual=value.flags.writeable)
-
-    msg_template = 'expected writeable is {expected}, but got a value with writeable flag set to {actual}'
+        super().__init__(f"expected writeable is {expected}, "
+                         f"but got a value with writeable flag set to {value.flags.writeable}")
 
 
 T = TypeVar("T", bound=np.generic)
 
 
 class ConstrainedNdArray(Generic[T], np.ndarray[Any, np.dtype[T]]):
     """
@@ -45,15 +39,15 @@
     The approach is inspired by https://github.com/cheind/pydantic-numpy.
 
     See Also:
         `Pydantic documentation: Custom Data Types <https://docs.pydantic.dev/usage/types/#custom-data-types>`_ and
         class :py:class:`pydantic.types.ConstrainedInt` or similar of :py:mod:`pydantic`
     """
 
-    strict: bool = False
+    strict: bool = True
     dimensions: Optional[int] = None
     shape_spec: Optional[tuple[int, ...]] = None
     writeable: bool = True
 
     @classmethod
     def __get_validators__(cls):
         yield get_type_validator(np.ndarray, lambda x: x.dtype) if cls.strict else cls.validate_ndarray
@@ -162,14 +156,15 @@
         dimensions = 1
 
     class ImmutableArray(Array[T]):
         """
         Pydantic-compatible field type for one-dimensional :py:class:`numpy.ndarray` objects, where the flag
         ``writeable`` is set to False.
         """
+        strict = False
         writeable = False
 
     class StrictNdArray(ConstrainedNdArray[T]):
         strict = True
 
     class Row(ConstrainedNdArray[T]):
         """
```

## optool/fields/quantities.py

```diff
@@ -1,64 +1,52 @@
 from __future__ import annotations
 
 from numbers import Number
-from typing import TYPE_CHECKING, Any, Dict, Generic, Optional, TypeVar
+from typing import TYPE_CHECKING, Any, Generic, TypeVar
 
 from pint import Unit
 from pydantic import ValidationError
-from pydantic.errors import PydanticTypeError, PydanticValueError
 from pydantic.fields import ModelField
-from pydantic.utils import update_not_none
 
 from optool.fields.util import WrongTypeError, check_validation_is_passed_on_to_sub_types, get_type_validator
 from optool.uom import UNITS, PhysicalDimension, Quantity
 
 
-class DimensionalityError(PydanticValueError):
+class DimensionalityError(ValueError):
 
     def __init__(self, *, expected: str, value: Quantity) -> None:
-        super().__init__(expected=expected, actual=value.dimensionality)
+        super().__init__(f"expected the dimensionality {expected}, "
+                         f"but got a value with dimensionality {value.dimensionality}")
 
-    msg_template = 'expected the dimensionality {expected}, but got a value with dimensionality {actual}'
 
-
-class UnsupportedMagnitudeConversion(PydanticTypeError):
+class UnsupportedMagnitudeConversion(ValueError):
 
     def __init__(self, *, value: Any) -> None:
-        super().__init__(type=type(value))
-
-    msg_template = 'the value of {type} cannot be converted automatically'
+        super().__init__(f"the value of {type(value)} cannot be converted automatically")
 
 
-class UnitParseError(PydanticValueError):
+class UnitParseError(ValueError):
 
     def __init__(self, *, unit: str) -> None:
-        super().__init__(unit=unit)
+        super().__init__(f"cannot parse the unit {unit}")
 
-    msg_template = 'cannot parse the unit {unit}'
 
+D = TypeVar("D", bound=PhysicalDimension)
 
-class ConstrainedUnit(Unit):
+
+class ConstrainedUnit(Unit, Generic[D]):
     """
     Pydantic-compatible field type for :py:class:`pint.Unit` objects, which allows to specify the desired
     dimensionality.
 
     See Also:
         `Pydantic documentation: Custom Data Types <https://docs.pydantic.dev/usage/types/#custom-data-types>`_ and
         class :py:class:`pydantic.types.ConstrainedInt` or similar of :py:mod:`pydantic`
     """
-    strict: bool = False
-    dimensionality: Optional[str] = None
-
-    @classmethod
-    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:
-        update_not_none(
-            field_schema,
-            dimensionality=cls.dimensionality,
-        )
+    strict: bool = True
 
     @classmethod
     def __get_validators__(cls):
         yield get_type_validator(Unit) if cls.strict else cls.validate_unit
         yield cls.validate_dimensionality
 
     @classmethod
@@ -71,43 +59,45 @@
                 return UNITS.parse_units(value)
             except Exception as e:
                 raise UnitParseError(unit=value) from e
 
         raise WrongTypeError(expected=(Unit, str), value=value)
 
     @classmethod
-    def validate_dimensionality(cls, value: Unit) -> Unit:
-        if cls.dimensionality is None or value.dimensionality == UNITS.get_dimensionality(cls.dimensionality):
-            return value
-        raise DimensionalityError(expected=cls.dimensionality, value=value)
+    def validate_dimensionality(cls, val: Unit, field: ModelField) -> Unit:
+        if not field.sub_fields or field.sub_fields[0].type_ == Any:
+            return val
+
+        dimension = field.sub_fields[0].type_
+        if not issubclass(dimension, PhysicalDimension):
+            raise TypeError(f"Unsupported {dimension}, should be a {PhysicalDimension.__name__!r} or 'typing.Any'.")
+        elif val.dimensionality != UNITS.get_dimensionality(dimension.dimensionality):
+            raise DimensionalityError(expected=dimension.dimensionality, value=val)
+        return val
 
 
 T = TypeVar("T")  # Allow storing anything as magnitude in Quantity
-D = TypeVar("D", bound=PhysicalDimension)
 
 
-class ConstrainedQuantity(Quantity, Generic[D, T]):  # TODO try to switch arguments
+class ConstrainedQuantity(Quantity, Generic[D, T]):
     """
     Pydantic-compatible field type for :py:class:`pint.Quantity` objects, which allows to specify the desired
     dimensionality.
 
     See Also:
         Class :py:class:`pydantic.types.ConstrainedInt` or similar of :py:mod:`pydantic`.
     """
 
-    strict: bool = False
-
-    # gt: Optional[] = None
-    # ge: OptionalInt = None
-    # lt: OptionalInt = None
-    # le: OptionalInt = None
+    strict: bool = True
+    strict_subtypes: bool = True
 
     @classmethod
     def __get_validators__(cls):
-        yield get_type_validator(Quantity, lambda x: type(x.m)) if cls.strict else cls.validate_quantity
+        subtype_provider = (lambda x: type(x.m)) if cls.strict_subtypes else None
+        yield get_type_validator(Quantity, subtype_provider) if cls.strict else cls.validate_quantity
         yield cls.validate_dimensionality
         yield cls.validate_magnitude
 
     @classmethod
     def validate_quantity(cls, val: Any, field: ModelField) -> Quantity:
         try:
             return Quantity(val)
@@ -138,17 +128,27 @@
             raise ValidationError([error], cls)
 
         return Quantity(valid_value, val.u)
 
 
 if TYPE_CHECKING:
     UnitLike = Unit
+    StrictUnit = Unit
 
     QuantityLike = Quantity
+    StrictQuantity = Quantity
 
 else:
 
-    class UnitLike(ConstrainedUnit):
+    class UnitLike(ConstrainedUnit[D]):
         strict = False
 
+    class StrictUnit(ConstrainedUnit[D]):
+        strict = True
+
     class QuantityLike(ConstrainedQuantity[D, T]):
-        pass
+        strict = False
+        strict_subtypes = False
+
+    class StrictQuantity(ConstrainedQuantity[D, T]):
+        strict = True
+        strict_subtypes = False
```

## optool/fields/series.py

```diff
@@ -4,43 +4,37 @@
 from typing import TYPE_CHECKING, Any, Generic, Iterable, Sequence, Type, TypeVar, cast
 
 import numpy as np
 import pandas as pd
 from pandas import DatetimeIndex, Index, TimedeltaIndex
 from pint_pandas import PintArray
 from pydantic import ValidationError
-from pydantic.errors import PydanticValueError
 from pydantic.fields import ModelField
 
 from optool.fields.util import WrongTypeError, check_sub_fields_level, get_type_validator
 from optool.uom import PhysicalDimension, Quantity
 
 
-class IndexTypeError(PydanticValueError):
+class IndexTypeError(ValueError):
 
     def __init__(self, *, expected: Type[Index], value: pd.Series) -> None:
-        super().__init__(expected=expected, actual=type(value.index))
+        super().__init__(f"expected index type {expected}, but got a series with index type {type(value.index)}")
 
-    msg_template = 'expected index type {expected}, but got a series with index type {actual}'
 
-
-class DimensionalityError(PydanticValueError):
+class DimensionalityError(ValueError):
 
     def __init__(self, *, expected: str, value: pd.Series) -> None:
-        super().__init__(expected=expected, actual=value.dtype)
-
-    msg_template = 'expected the dimensionality {expected}, but got a series with data-type {actual}'
+        super().__init__(f"expected the dimensionality {expected}, but got a series with data-type {value.dtype}")
 
 
-class ArrayWriteableError(PydanticValueError):
+class ArrayWriteableError(ValueError):
 
     def __init__(self, *, expected: bool, value: np.ndarray) -> None:
-        super().__init__(expected=expected, actual=value.flags.writeable)
-
-    msg_template = 'expected writeable is {expected}, but got a value with writeable flag set to {actual}'
+        super().__init__(f"expected writeable is {expected}, "
+                         f"but got a value with writeable flag set to {value.flags.writeable}")
 
 
 T = TypeVar("T")  # Allow storing anything as data-type in Series
 
 
 class ConstrainedSeries(pd.Series, Generic[T]):
     """
```

## optool/fields/symbolic.py

```diff
@@ -1,25 +1,23 @@
 from __future__ import annotations
 
 import itertools
 from typing import TYPE_CHECKING, Optional
 
 import casadi
 import numpy as np
-from pydantic.errors import PydanticValueError
 
 from optool.fields.util import get_type_validator
 
 
-class ShapeError(PydanticValueError):
+class ShapeError(ValueError):
 
     def __init__(self, *, expected: tuple[int, ...], value: casadi.SX) -> None:
-        super().__init__(expected=expected, actual=value.size())
-
-    msg_template = 'expected the shape {expected}, but got a value with shape ("called size" in CasADi) {actual}'
+        super().__init__(f"expected the shape {expected}, "
+                         f"but got a value with shape ('called size' in CasADi) {value.size()}")
 
 
 class ConstrainedCasadiSymbol:
     """
     Pydantic-compatible field type for :py:class:`casadi.SX` objects.
 
     See Also:
```

## optool/fields/util.py

```diff
@@ -1,58 +1,44 @@
 from __future__ import annotations
 
 import re
 from typing import Any, Callable, Iterable, Optional, Type, TypeVar, Union
 
-from pydantic.errors import PydanticTypeError, PydanticValueError
 from pydantic.fields import ModelField
 from pydantic.validators import find_validators
 
 from optool import BaseModel
 
 TypeDefinition = Union[type, tuple[type, ...]]
 ValidationFunc = Callable[[Any], Any]
 
 T = TypeVar("T")
 
 
-class WrongTypeError(PydanticTypeError):
+class WrongTypeError(ValueError):
 
     def __init__(self, *, expected: TypeDefinition, value: Any) -> None:
-        super().__init__(expected=expected, actual=type(value))
-        self.msg_template = 'expected {expected}, but got {actual}'
+        super().__init__(f"expected {expected}, but got {value}")
 
 
-class WrongSubTypeError(PydanticTypeError):
+class WrongSubTypeError(ValueError):
 
     def __init__(self, *, expected_type: TypeDefinition, expected_subtype: TypeDefinition,
                  actual_subtype: TypeDefinition, value: Any) -> None:
-        super().__init__(expected_type=expected_type,
-                         expected_subtype=expected_subtype,
-                         actual_subtype=actual_subtype,
-                         actual_type=type(value))
-        self.msg_template = 'expected sub-type {expected_subtype} of {expected_type}, ' \
-                            'but got {actual_subtype} of {actual_type}'
+        super().__init__(f"expected sub-type {expected_subtype} of {expected_type}, "
+                         f"but got {actual_subtype} of {type(value)}")
 
 
-class ArbitrarySubTypeError(PydanticTypeError):
+class ArbitrarySubTypeError(ValueError):
 
     def __init__(self, *, name: str, field: ModelField) -> None:
         sub_type = None if field.sub_fields is None else field.sub_fields[0].type_
-        super().__init__(name=name, type=field.type_, sub_type=sub_type)
-        self.msg_template = 'the sub-field of {name!r} has the type {type} (with sub-type {sub_type}), but {type} ' \
-                            'does not offer any specific validators that would be able to handle sub-types'
-
-
-class ValidationFunctionError(PydanticValueError):
-
-    def __init__(self, *, value: Any, msg_template: Optional[str] = None) -> None:
-        super().__init__(value=value)
-        if msg_template:
-            self.msg_template = msg_template
+        super().__init__(f"the sub-field of {name!r} has the type {field.type_} (with sub-type {sub_type}), "
+                         f"but {field.type_} does not offer any specific validators that would be able to handle "
+                         f"sub-types")
 
 
 class _ConfigWithArbitraryTypesNotAllowed(BaseModel.Config):
     arbitrary_types_allowed = False
 
 
 def has_specific_type_validator(type_: Type[Any]) -> bool:
@@ -142,27 +128,29 @@
         value: The value to validate.
         validators: The validator function(s).
         msg_template: The message to show in case the validation fails, may contain ``{value}`` to refer to the value.
 
     Returns:
         The given value in case the validation is successful.
     """
+    msg_template = msg_template or "Validation failed for {value}"
+    error = ValueError(msg_template.format(value=value))
     if isinstance(validators, bool):
         if validators:
             return value
-        raise ValidationFunctionError(value=value, msg_template=msg_template)
+        raise error
 
     for validator in validators if isinstance(validators, Iterable) else [validators]:
         try:
             satisfied = validator(value)
         except Exception as e:
-            raise ValidationFunctionError(value=value, msg_template=msg_template) from e
+            raise error from e
 
         if not satisfied:
-            raise ValidationFunctionError(value=value, msg_template=msg_template)
+            raise error
 
     return value
 
 
 def validate_each(value: Iterable,
                   validators: Union[bool, ValidationFunc, Iterable[ValidationFunc]],
                   msg_template: Optional[str] = None) -> None:
```

## optool/optimization/constraints.py

```diff
@@ -47,17 +47,17 @@
 
     expression: QuantityLike[Any, Union[ImmutableArray, CasadiColumn]] = Field(allow_mutation=False)
     """
     The expression of the constraint.
     """
 
 
-class IntervalConstraint(OptimizationConstraint):
+class BoxConstraint(OptimizationConstraint):
     """
-    Creates an interval constraint.
+    Creates a box constraint.
     """
 
     expression: QuantityLike[Any, CasadiColumn] = Field(allow_mutation=False,
                                                         exclude=True)  # Cannot handle casadi elements
     """
     The expression of the constraint.
     """
```

## optool/optimization/problem.py

```diff
@@ -14,15 +14,15 @@
 from optool.fields.containers import conlist
 from optool.fields.numeric import ImmutableArray
 from optool.fields.quantities import QuantityLike, UnitLike
 from optool.fields.symbolic import CasadiColumn, CasadiScalar
 from optool.fields.util import validate, validate_each
 from optool.logging import LOGGER
 from optool.math import is_symbolic, num_elements
-from optool.optimization.constraints import ExpressionConstraint, IntervalConstraint, OptimizationConstraint
+from optool.optimization.constraints import BoxConstraint, ExpressionConstraint, OptimizationConstraint
 from optool.optimization.helpers import DebugInfo, IpoptOption, SolverResponse
 from optool.optimization.ode import IntegrationMethod, OrdinaryDifferentialEquation
 from optool.optimization.variables import CasadiVariable, OptimizationVariable
 from optool.uom import UNITS, Quantity
 from optool.util import ValueRange
 
 T = TypeVar("T", bound=OptimizationConstraint)
@@ -153,32 +153,28 @@
         raise NotImplementedError()
 
     @abstractmethod
     def add_equality_constraint(self, lhs, rhs, nominal_value=None) -> OptimizationConstraint:
         raise NotImplementedError()
 
     @abstractmethod
-    def add_interval_constraint(self,
-                                lower_bound,
-                                expression,
-                                upper_bound,
-                                nominal_value=None) -> OptimizationConstraint:
+    def add_box_constraint(self, lower_bound, expression, upper_bound, nominal_value=None) -> OptimizationConstraint:
         raise NotImplementedError()
 
     @abstractmethod
     def add_greater_than_constraint(self, lower_bound, expression, nominal_value=None) -> OptimizationConstraint:
         raise NotImplementedError()
 
     @abstractmethod
     def add_less_than_constraint(self, expression, upper_bound, nominal_value=None) -> OptimizationConstraint:
         raise NotImplementedError()
 
     @abstractmethod
     def add_continuity_constraint(self, integration_method: IntegrationMethod, ode: OrdinaryDifferentialEquation,
-                                  timestamps: DatetimeIndex) -> IntervalConstraint:
+                                  timestamps: DatetimeIndex) -> BoxConstraint:
         raise NotImplementedError()
 
     @staticmethod
     def casadi(name=""):
         return CasadiProblem(name=name)
 
 
@@ -204,16 +200,16 @@
     """The objective to minimize."""
 
     @final
     @validator('constraints')
     def _is_supported_constraint(cls, value):
         types = [type(element) for element in value]
         for element in value:
-            validate(element, lambda x: isinstance(x, IntervalConstraint),
-                     f"All constraints of {cls.__name__} must be IntervalConstraints, but have {types}.")
+            validate(element, lambda x: isinstance(x, BoxConstraint),
+                     f"All constraints of {cls.__name__} must be {BoxConstraint.__name__}, but have {types}.")
         return value
 
     def parse(self, solver: str, options=None) -> None:
         if options is None:
             options = {}
         if self._solver is not None:
             raise ValueError("The problem has already been parsed.")
@@ -221,15 +217,15 @@
         LOGGER.info("Parsing the problem entitled {!r}", self.name)
 
         if unnamed_constraints := [val for val in self._get_constraints(OptimizationConstraint) if val.name == ""]:
             LOGGER.debug("There are {} unnamed constraints that are automatically named now.", len(unnamed_constraints))
             for i, val in enumerate(unnamed_constraints):
                 val.name = f"(unnamed constraint {i})"
 
-        g_constraints = self._get_constraints(IntervalConstraint)
+        g_constraints = self._get_constraints(BoxConstraint)
         h_constraints = self._get_constraints(ExpressionConstraint)
 
         LOGGER.debug("Assembling {} variables with following names and sizes: {}.", len(self.variables),
                      [(val.name, val.length()) for val in self.variables])
         LOGGER.debug("Assembling the following {} constraints: {}.", len(self.constraints),
                      [f"{val.__class__.__name__}({val.name})" for val in self.constraints])
 
@@ -289,21 +285,21 @@
         # lagrange_multipliers = np.split(sol["lam_x"].full().squeeze(), end_split_indices)
 
         for i, val in enumerate(self.variables):
             val.solution = normed_values[i] * val.nominal_values
             # val.lagrange_multipliers = lagrange_multipliers[i]
             debug_info.normed_variable_values.append(ValueRange.of(val.name, normed_values[i]))
 
-        # Interval constraints
-        interval_constraints = self._get_constraints(IntervalConstraint)
-        constraint_lengths = [val.length() for val in interval_constraints]
+        # Box constraints
+        box_constraints = self._get_constraints(BoxConstraint)
+        constraint_lengths = [val.length() for val in box_constraints]
         end_split_indices = np.cumsum(constraint_lengths)
         lagrange_multipliers = np.split(sol["lam_g"].full().squeeze(), end_split_indices)
 
-        for i, val in enumerate(interval_constraints):
+        for i, val in enumerate(box_constraints):
             lagrange_multipliers_unit = robust_divide_units(unit_of_objective, val.get_unit())
             val.lagrange_multipliers = Quantity(lagrange_multipliers[i] / val.nominal_value, lagrange_multipliers_unit)
             debug_info.normed_constraints_lagrange_multipliers.append(ValueRange.of(val.name, lagrange_multipliers[i]))
 
         # Parameters
         # end_split_indices = np.cumsum([val.length() for val in obj.parameters])
         # lagrange_multipliers = np.split(sol["lam_p"].full().squeeze(), end_split_indices)
@@ -321,101 +317,102 @@
 
     def _get_normed_replicated_variable_values(self, field_name: str) -> np.ndarray:
         normed_values = [getattr(val, field_name) / val.nominal_values for val in self.variables]
         dimensionless_values = [val.m_as(UNITS.dimensionless) for val in normed_values]
         return np.concatenate(dimensionless_values)  # type: ignore
 
     def _get_normed_replicated_constraint_bounds(self, method_name: str):
-        interval_constraints = self._get_constraints(IntervalConstraint)
-        dimensionless_values = [getattr(val, method_name)() for val in interval_constraints]
+        box_constraints = self._get_constraints(BoxConstraint)
+        dimensionless_values = [getattr(val, method_name)() for val in box_constraints]
         return np.concatenate(dimensionless_values)  # type: ignore
 
     @validate_arguments
     def new_variable(self, name: StrictStr, n: StrictInt, unit: Optional[UnitLike] = None) -> CasadiVariable:
         variable = OptimizationVariable.casadi(name, n, unit)
         self.variables.append(variable)
         return variable
 
     @validate_arguments
     def add_equality_constraint(self,
                                 lhs: QuantityLike[Any, Union[ImmutableArray, CasadiColumn]],
                                 rhs: QuantityLike[Any, Union[ImmutableArray, CasadiColumn]],
-                                nominal_value: StrictFloat = 1.0) -> IntervalConstraint:
+                                nominal_value: StrictFloat = 1.0) -> BoxConstraint:
 
         if num_elements(lhs) != num_elements(rhs):
             raise ValueError(f"The number of values on the right-hand side and on the left-hand side must be equal, "
                              f"but have {num_elements(lhs)} and {num_elements(rhs)}, respectively.")
 
         difference = lhs - rhs
         zero = Quantity(0.0, difference.units)
-        constraint = IntervalConstraint(name=f"constraint-{len(self.constraints)}",
-                                        lower_bound=zero,
-                                        expression=difference / nominal_value,
-                                        upper_bound=zero,
-                                        nominal_value=nominal_value)
+        constraint = BoxConstraint(name=f"constraint-{len(self.constraints)}",
+                                   lower_bound=zero,
+                                   expression=difference / nominal_value,
+                                   upper_bound=zero,
+                                   nominal_value=nominal_value)
         self.constraints.append(constraint)
         return constraint
 
     @validate_arguments
-    def add_interval_constraint(self,
-                                lower_bound: QuantityLike[Any, ImmutableArray],
-                                expression: QuantityLike[Any, Union[ImmutableArray, CasadiColumn]],
-                                upper_bound: QuantityLike[Any, ImmutableArray],
-                                nominal_value: StrictFloat = 1.0) -> IntervalConstraint:
+    def add_box_constraint(self,
+                           lower_bound: QuantityLike[Any, Union[float, ImmutableArray]],
+                           expression: QuantityLike[Any, Union[ImmutableArray, CasadiColumn]],
+                           upper_bound: QuantityLike[Any, Union[float, ImmutableArray]],
+                           nominal_value: StrictFloat = 1.0) -> BoxConstraint:
 
-        if len({num_elements(val) for val in [lower_bound, expression, upper_bound]}) != 1:
+        expression_length = num_elements(expression)
+        if any(num_elements(bound) not in [1, expression_length] for bound in [lower_bound, upper_bound]):
             raise ValueError(f"The number of values of the lower bound, the value, and the upper bound are not equal, "
                              f"but have {num_elements(lower_bound)}, {num_elements(expression)}, "
                              f"and {num_elements(upper_bound)}, respectively.")
 
-        constraint = IntervalConstraint(name=f"constraint-{len(self.constraints)}",
-                                        lower_bound=lower_bound / nominal_value,
-                                        expression=expression / nominal_value,
-                                        upper_bound=upper_bound / nominal_value,
-                                        nominal_value=nominal_value)
+        constraint = BoxConstraint(name=f"constraint-{len(self.constraints)}",
+                                   lower_bound=lower_bound / nominal_value,
+                                   expression=expression / nominal_value,
+                                   upper_bound=upper_bound / nominal_value,
+                                   nominal_value=nominal_value)
         self.constraints.append(constraint)
         return constraint
 
     @validate_arguments
     def add_greater_than_constraint(self,
                                     lower_bound: QuantityLike[Any, Union[ImmutableArray, CasadiColumn]],
                                     expression: QuantityLike[Any, Union[ImmutableArray, CasadiColumn]],
-                                    nominal_value: StrictFloat = 1.0) -> IntervalConstraint:
+                                    nominal_value: StrictFloat = 1.0) -> BoxConstraint:
 
         if is_symbolic(lower_bound):
             expression = expression - lower_bound
             lower_bound = Quantity(0.0, expression.units)
-        constraint = IntervalConstraint(name=f"constraint-{len(self.constraints)}",
-                                        lower_bound=lower_bound / nominal_value,
-                                        expression=expression / nominal_value,
-                                        upper_bound=None,
-                                        nominal_value=nominal_value)
+        constraint = BoxConstraint(name=f"constraint-{len(self.constraints)}",
+                                   lower_bound=lower_bound / nominal_value,
+                                   expression=expression / nominal_value,
+                                   upper_bound=None,
+                                   nominal_value=nominal_value)
         self.constraints.append(constraint)
         return constraint
 
     @validate_arguments
     def add_less_than_constraint(self,
                                  expression: QuantityLike[Any, Union[ImmutableArray, CasadiColumn]],
                                  upper_bound: QuantityLike[Any, Union[ImmutableArray, CasadiColumn]],
-                                 nominal_value: StrictFloat = 1.0) -> IntervalConstraint:
+                                 nominal_value: StrictFloat = 1.0) -> BoxConstraint:
 
         if is_symbolic(upper_bound):
             expression = expression - upper_bound
             upper_bound = Quantity(0.0, expression.units)
-        constraint = IntervalConstraint(name=f"constraint-{len(self.constraints)}",
-                                        lower_bound=None,
-                                        expression=expression / nominal_value,
-                                        upper_bound=upper_bound / nominal_value,
-                                        nominal_value=nominal_value)
+        constraint = BoxConstraint(name=f"constraint-{len(self.constraints)}",
+                                   lower_bound=None,
+                                   expression=expression / nominal_value,
+                                   upper_bound=upper_bound / nominal_value,
+                                   nominal_value=nominal_value)
         self.constraints.append(constraint)
         return constraint
 
     @validate_arguments
     def add_continuity_constraint(self, integration_method: IntegrationMethod, ode: OrdinaryDifferentialEquation,
-                                  timestamps: DatetimeIndex) -> IntervalConstraint:
+                                  timestamps: DatetimeIndex) -> BoxConstraint:
 
         eq = integration_method.integrate(ode, timestamps)
         constraint = self.add_equality_constraint(eq.lhs, eq.rhs)
         constraint.name = ode.name
         return constraint
 
     @staticmethod
```

## optool/serialization/__init__.py

```diff
@@ -1,19 +1,18 @@
 from __future__ import annotations
 
 import json
 from abc import ABC, abstractmethod
 from collections import OrderedDict
 from typing import Any, Callable, Dict, ForwardRef, Generic, Type, TypeVar, Union, get_args
 
-from pydantic.typing import AnyCallable
-
 from optool.logging import LOGGER
 
 T = TypeVar("T")
+AllowedSerializedDictKeys = Union[str, int, float, bool, None]
 
 
 class Serializer(ABC, Generic[T]):
     _type_T: type
 
     def __init_subclass__(cls) -> None:
         # Get the generic type. Approach taken from https://stackoverflow.com/a/71720366
@@ -25,47 +24,47 @@
         return cls._type_T
 
     @classmethod
     def get_type_name(cls) -> str:
         return str(cls.get_type())
 
     @abstractmethod
-    def serialize(self, obj: T) -> Dict[str, Any]:
+    def serialize(self, obj: T) -> Dict[AllowedSerializedDictKeys, Any]:
         """
         Serializes an object to a dictionary of primitive types.
         """
         raise NotImplementedError()
 
     @abstractmethod
-    def deserialize(self, raw: Dict[str, Any]) -> T:
+    def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> T:
         """
         Deserializes a dictionary of primitive types to an object.
         """
         raise NotImplementedError()
 
 
 class SerializationAssistant:
     _serializers: Dict[str, Serializer] = {}
 
     @classmethod
-    def register(cls, *serializers: Serializer) -> Dict[Union[Type[Any], str, ForwardRef], AnyCallable]:
+    def register(cls, *serializers: Serializer) -> Dict[Union[Type[Any], str, ForwardRef], Callable]:
         for serializer in serializers:
             obj_type = serializer.get_type()
             obj_type_name = serializer.get_type_name()
             LOGGER.debug("Registering serializer for {}.", obj_type)
             if obj_type_name in cls._serializers:
                 raise ValueError(f"There is already an entry in the registry for {obj_type_name}.")
             cls._serializers[obj_type_name] = serializer
 
         return {serializer.get_type(): cls._create_json_encoder(serializer) for serializer in serializers}
 
     @staticmethod
-    def _create_json_encoder(serializer: Serializer[T]) -> Callable[[T], Dict[str, Any]]:
+    def _create_json_encoder(serializer: Serializer[T]) -> Callable[[T], Dict[AllowedSerializedDictKeys, Any]]:
 
-        def _encode_obj(obj: T) -> Dict[str, Any]:
+        def _encode_obj(obj: T) -> Dict[AllowedSerializedDictKeys, Any]:
             return OrderedDict({'obj_type': serializer.get_type_name()}, **serializer.serialize(obj))
 
         return _encode_obj
 
     @classmethod
     def json_loader(cls, raw: Union[str, bytes]) -> Any:
         return json.loads(raw, object_pairs_hook=cls._parse_raw)
```

## Comparing `optool/serialization/dataframes.py` & `optool/serialization/pandas_objects.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,66 +1,78 @@
 from __future__ import annotations
 
+from datetime import datetime
 from typing import Any, Dict
 
 import pandas as pd
-import pytz
+from pint_pandas import PintArray
 
-from optool.serialization import Serializer
+from optool.serialization import AllowedSerializedDictKeys, Serializer
 
 
-class DataFrameSerializer(Serializer[pd.DataFrame]):
+class PandasRangeIndexSerializer(Serializer[pd.RangeIndex]):
 
-    def serialize(self, obj: pd.DataFrame) -> Dict[str, Any]:
-        if not isinstance(obj.index, pd.RangeIndex):
-            obj = obj.copy()
-            index_name = obj.index.name
-            obj['non_serializable_index'] = obj.index
-            obj = obj.reset_index(drop=True)  # replaces index with RangeIndex
-            obj.index.name = index_name
+    def serialize(self, obj: pd.RangeIndex) -> Dict[AllowedSerializedDictKeys, Any]:
+        return dict(start=obj.start, stop=obj.stop, step=obj.step, name=obj.name)
 
-        return obj.to_dict()
+    def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> pd.RangeIndex:
+        return pd.RangeIndex(**raw)
 
-    def deserialize(self, raw: Dict[str, Any]) -> pd.DataFrame:
-        obj = pd.DataFrame.from_dict(raw)
-        if 'non_serializable_index' in obj:
-            index_name = obj.index.name
-            obj.set_index('non_serializable_index', drop=True, inplace=True)
-            obj.index.name = index_name
 
-        return obj
+class PandasDatetimeIndexSerializer(Serializer[pd.DatetimeIndex]):
+    _DICT_KEY_VALUES = 'values'
+    _DICT_KEY_FORMAT = 'format'
+    _DICT_KEY_FREQUENCY = 'freq'
+    _DICT_KEY_TIMEZONE = 'timezone'
+    _DATE_FMT = "%Y-%m-%d %H:%M:%S.%f"
+    _DATE_FMT_TZ = _DATE_FMT + " %z"
 
+    def serialize(self, obj: pd.DatetimeIndex) -> Dict[AllowedSerializedDictKeys, Any]:
+        date_fmt = self._DATE_FMT_TZ if obj.tzinfo else self._DATE_FMT
+        return {
+            self._DICT_KEY_VALUES: obj.strftime(date_fmt).to_list(),
+            self._DICT_KEY_FORMAT: date_fmt,
+            self._DICT_KEY_FREQUENCY: obj.freqstr,
+            self._DICT_KEY_TIMEZONE: obj.tzinfo,
+        }
+
+    def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> pd.DatetimeIndex:
+        date_fmt = raw[self._DICT_KEY_FORMAT]
+        dt_array = [datetime.strptime(item, date_fmt) for item in raw[self._DICT_KEY_VALUES]]
 
-class SeriesSerializer(Serializer[pd.Series]):
-    _DICT_KEY_TIMEZONE = 'timezone'
+        if tz_info := raw[self._DICT_KEY_TIMEZONE]:
+            dt_array = [item.astimezone(tz_info) for item in dt_array]
 
-    _DATE_FMT = "%Y-%m-%d %H:%M:%S.%f"
-    _DATE_FMT_TZ = _DATE_FMT + " %Z %z"
+        obj = pd.DatetimeIndex(dt_array)
+
+        if freq := raw[self._DICT_KEY_FREQUENCY]:
+            obj.freq = freq
 
-    def serialize(self, obj: pd.Series) -> Dict[str, Any]:
+        return obj
+
+
+class PandasSeriesSerializer(Serializer[pd.Series]):
+
+    def serialize(self, obj: pd.Series) -> Dict[AllowedSerializedDictKeys, Any]:
         if obj.ndim != 1:
             raise ValueError(f"The number of dimensions of a pandas series must be 1, but is {obj.ndim}.")
+        return dict(name=obj.name, index=obj.index, data=obj.values)
+
+    def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> pd.Series:
+        return pd.Series(**raw)
+
 
-        if isinstance(obj.index, pd.RangeIndex):
-            return obj.to_dict()
+class PandasDataFrameSerializer(Serializer[pd.DataFrame]):
+    _DICT_KEY_INDEX = '__index__'
 
-        if isinstance(obj.index, pd.DatetimeIndex):
-            obj = obj.copy()
-            (tz_name, tz_fmt) = (obj.index.tz.zone, self._DATE_FMT_TZ) if obj.index.tz else (None, self._DATE_FMT)
-            obj.index = obj.index.strftime(tz_fmt)  # replace the index
-            dct = obj.to_dict()
-            dct[self._DICT_KEY_TIMEZONE] = tz_name
-            return dct
-        raise TypeError(f"Cannot serialize a pandas series with index of type {obj.index.__class__.__name__}.")
-
-    def deserialize(self, raw: Dict[str, Any]) -> pd.Series:
-        if self._DICT_KEY_TIMEZONE not in raw:
-            return pd.Series(raw)
-
-        tz_name = raw.pop(self._DICT_KEY_TIMEZONE)
-        obj = pd.Series(raw)
-        obj.index = pd.DatetimeIndex(obj.index)  # replace the index
-        if tz_name:
-            obj.index = obj.index.tz_convert(pytz.timezone(tz_name))  # restore the localized time zone
-            if list(raw.keys()) != list(obj.index.strftime(self._DATE_FMT_TZ)):
-                raise ValueError("The date-time index does  not have the same time zone anymore.")
+    def serialize(self, obj: pd.DataFrame) -> Dict[AllowedSerializedDictKeys, Any]:
+        columns_dict = obj.to_dict(orient='list')
+        for key in columns_dict:
+            if isinstance(obj[key].values, PintArray):
+                columns_dict[key] = obj[key].values
+        return {self._DICT_KEY_INDEX: obj.index, **columns_dict}
+
+    def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> pd.DataFrame:
+        index = raw.pop(self._DICT_KEY_INDEX)
+        obj = pd.DataFrame.from_dict(raw, orient='columns')
+        obj.index = index
         return obj
```

## Comparing `optool/serialization/numeric.py` & `optool/serialization/numpy_objects.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 from __future__ import annotations
 
 from typing import Any, Dict
 
 import numpy as np
 
-from optool.serialization import Serializer
+from optool.serialization import AllowedSerializedDictKeys, Serializer
 
 
-class NumpySerializer(Serializer[np.ndarray]):
+class NumpyNdArraySerializer(Serializer[np.ndarray]):
 
-    def serialize(self, obj: np.ndarray) -> Dict[str, Any]:
+    def serialize(self, obj: np.ndarray) -> Dict[AllowedSerializedDictKeys, Any]:
         return {"datatype": obj.dtype.name, "writeable": obj.flags.writeable, "values": obj.tolist()}
 
-    def deserialize(self, raw: Dict[str, Any]) -> np.ndarray:
+    def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> np.ndarray:
         value = np.asarray(raw["values"], dtype=raw["datatype"])
         value.setflags(write=raw['writeable'])
         return value
```

## Comparing `optool/serialization/quantities.py` & `optool/serialization/pint_objects.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,27 +1,37 @@
 from __future__ import annotations
 
 from typing import Any, Dict
 
 from pint import Unit
 from pint.util import to_units_container
+from pint_pandas import PintArray
 
-from optool.serialization import Serializer
+from optool.serialization import AllowedSerializedDictKeys, Serializer
 from optool.uom import UNITS, Quantity
 
 
-class QuantitySerializer(Serializer[Quantity]):
+class PintQuantitySerializer(Serializer[Quantity]):
 
-    def serialize(self, obj: Quantity) -> Dict[str, Any]:
+    def serialize(self, obj: Quantity) -> Dict[AllowedSerializedDictKeys, Any]:
         return {'mag': obj.m, 'unit': obj.u}
 
-    def deserialize(self, raw: Dict[str, Any]) -> Quantity:
+    def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> Quantity:
         return Quantity(raw['mag'], raw['unit'])
 
 
-class UnitSerializer(Serializer[Unit]):
+class PintUnitSerializer(Serializer[Unit]):
 
-    def serialize(self, obj: Unit) -> Dict[str, Any]:
+    def serialize(self, obj: Unit) -> Dict[AllowedSerializedDictKeys, Any]:
         return dict(to_units_container(obj))
 
-    def deserialize(self, raw: Dict[str, Any]) -> Unit:
+    def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> Unit:
         return UNITS.Unit(UNITS.UnitsContainer(raw))
+
+
+class PintArraySerializer(Serializer[PintArray]):
+
+    def serialize(self, obj: PintArray) -> Dict[AllowedSerializedDictKeys, Any]:
+        return {'mag': obj.quantity.m, 'unit': obj.quantity.u}
+
+    def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> PintArray:
+        return PintArray(raw['mag'], raw['unit'])
```

## Comparing `optool-0.2.0.dist-info/LICENSE.txt` & `optool-0.3.0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `optool-0.2.0.dist-info/METADATA` & `optool-0.3.0.dist-info/METADATA`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: optool
-Version: 0.2.0
+Version: 0.3.0
 Summary: Optimization tools
 Author: Andreas Ritter
 Author-email: anritter@idsc.mavt.ethz.ch
 License: MIT
 Project-URL: Source, https://gitlab.com/ocsept/optool
 Project-URL: Documentation, https://gitlab.com/ocsept/documentation
 Project-URL: API, https://ocsept.gitlab.io/optool
@@ -16,18 +16,17 @@
 License-File: LICENSE.txt
 Requires-Dist: casadi
 Requires-Dist: humanize
 Requires-Dist: inflection
 Requires-Dist: loguru
 Requires-Dist: numpy
 Requires-Dist: pandas
-Requires-Dist: pint
+Requires-Dist: pint (~=0.20.1)
 Requires-Dist: pint-pandas
 Requires-Dist: pydantic
-Requires-Dist: pytz
 Provides-Extra: testing
 Requires-Dist: setuptools ; extra == 'testing'
 Requires-Dist: pytest ; extra == 'testing'
 Requires-Dist: pytest-cov ; extra == 'testing'
 
 [![PyPI-Server](https://img.shields.io/pypi/v/optool.svg)](https://pypi.org/project/optool/)
 [![Built Status](https://gitlab.com/ocsept/optool/badges/master/pipeline.svg)](https://gitlab.com/ocsept/optool/)
@@ -130,29 +129,7 @@
   files typically used for project configurations.
 - [tox] automates and standardizes testing in Python by offering a generic virtualenv management and test CLI. It allows
   to run predefined test procedures on various environments on personal computers as well as on CI servers.
 - [types-pytz](https://pypi.org/project/types-pytz/) is the type stub package for the pytz package.
 - [yapf](https://pypi.org/project/yapf/) formats Python source code files.
 
 [tox]: https://tox.readthedocs.io/en/stable/
-
-# Changelog
-
-## Version 0.1.0 (Initial release)
-
-## Version 0.2.0 (Complete revision of the code)
-
-- Replaces [`pyfields`](https://pypi.org/project/pyfields/) with [`pydantic`](https://pypi.org/project/pydantic/).
-- Adds Pydantic-compatible fields to validate quantities, time-series, numerical arrays, Pandas DataFrames, etc.
-- Adds many physical quantities for validating values via Pydantic fields.
-- Revised API documentation.
-- Additional documentation is now written in Markdown instead of reStructuredText.
-- Fixes a bug on Windows related to file separator in path name.
-- Adds check if validation of subtypes is passed on.
-- Revises formulation of ordinary differential equations (ODEs).
-- Adds new tests that use ODE representation and parallelization feature.
-- Removes dependency on [`matplotlib`](https://matplotlib.org) and all related plotting utilities.
-- Adds possibility to compare Pydantic models that have Numpy arrays as fields.
-- Allows coercion from types compatible to floats in `PositiveFiniteFloat` (default is set to `strict = False`).
-- Removes class `Horizon`. Instead, the
-  class [`DatetimeIndex`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html) of
-  Pandas is used.
```

## Comparing `optool-0.2.0.dist-info/RECORD` & `optool-0.3.0.dist-info/RECORD`

 * *Files 17% similar despite different names*

```diff
@@ -1,36 +1,37 @@
-optool/VERSION,sha256=H5MN0fEzwfl6lP46y42zQ3LPTAH_2ys_9Mpy-UlBIek,6
-optool/__init__.py,sha256=fptQZQqvTcS4MVnfCzUjCY2Hsrm7LWw0T6PHuK5yclA,4993
+optool/VERSION,sha256=2RXMldbKj0euKXcT7UbU5cXZnd0p_Dxh4mO98wXytbA,6
+optool/__init__.py,sha256=NWssAsxFZGSHcfJ2bCZ4kj3mYySr_9Tl9Meu17o_XzI,5469
 optool/conversions.py,sha256=eUbpDZ4Ep1MB-o-QdMYHUiiz0QlzCaRp21wzVWfUvFU,949
 optool/languages.py,sha256=TLG-Ez5dqiyIy4JlHxy39WV9FgXPDcjFrghufIlZEsY,1094
 optool/logging.py,sha256=GidpPg6pZwcjsUOKMrC1YT-K6YZmZLY3RAB4wSgF4s0,4095
 optool/math.py,sha256=T3dS7zeDNBOh8IIDd85GTnBvZW_trsS0DyJiyacj4Ww,7758
 optool/orthography.py,sha256=KjLnK8KWMW6_d29Nuu8aAmK9NdgH-vscP5UlFMyWyB0,1148
 optool/parallel.py,sha256=W1fXrDSSTL9-SJXO9yN1zYltyahsyObHBV4cvHRNUqo,1814
 optool/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 optool/uom.py,sha256=L5VrhdXebtYaTvDbp1EZXqYYS1lF1M6m0KW3nM8p30o,31001
 optool/util.py,sha256=eKRxJ4Y6sYAzTdnJew-2V9Vfv36nWFRKA2ol2Q8ce7o,833
 optool/fields/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-optool/fields/callables.py,sha256=n74ehwaHXyoivkpPvwzsYMa6A5yNJSUmRDK5yT7-Gt8,4075
+optool/fields/callables.py,sha256=2mwcFPXTsYKsm50W-NzpPuR82R_Z0RNTDJPVat1UVSc,3914
 optool/fields/containers.py,sha256=Bka5Yi1z0sURI29_n6cy3b8qHtT64en9EzjPiSJVHXY,4363
-optool/fields/dataframe.py,sha256=l5BTJfDpRts8M0fKxceF7AASS34RX0XLrkIUOgJVLEY,2229
-optool/fields/misc.py,sha256=kvNqAhNcyTW-98F7w9zdYGNYUfoiQm9YtmCJ5k9E7z0,403
-optool/fields/numeric.py,sha256=oGa6e0UGEaAdmIbb1dErB7D48qnw3hdn7gCb5PHx8PU,7116
-optool/fields/quantities.py,sha256=wHVMIQlTfDDuc8T_tmlttmZiCnGtVaxx2hL2w4spl8M,5055
-optool/fields/series.py,sha256=4uHW_QdMBX_Q27QpqaLEOQCieC1EiMKaPqntht2mbHw,6353
-optool/fields/symbolic.py,sha256=zV_L7xpkrYkAKdmumygV3zFHn_C3xNsDlo5D4v9nk7w,2413
-optool/fields/util.py,sha256=SWRn6rGV_wce127Iq8euSFUK8Z7ev6k4QRyp30AOKvw,6556
+optool/fields/dataframe.py,sha256=Wa4cMQaPDf9W13jYaan15uijPiAuxCv2-F-Fd5jA1Zk,2122
+optool/fields/misc.py,sha256=61UDJlRyd_cDEYrCUhdGOHg8iZRV2_voUxYkHu8Q9wM,829
+optool/fields/numeric.py,sha256=ieibdvnKn1fdMG_LoKjv5F_leb2A-3ry7Wz44i1MP6k,6940
+optool/fields/quantities.py,sha256=gjPz-DXk7Q_QmNR0us4VekWuosbWbsCCA2HJ944qBI4,5143
+optool/fields/series.py,sha256=byV8kM64Klt-c6XVxv9wJJp1S-2KW2EI4Ig5AfPR-tc,6155
+optool/fields/symbolic.py,sha256=qLTVDYhQ1t8FFRlgqiVHiTqjo5-f-VERzTUHF13NpxY,2335
+optool/fields/util.py,sha256=JlD9FPCHPDBYXKKy3YSwwjjbdyAp66Csw4FqP1sMm8Y,5848
 optool/optimization/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-optool/optimization/constraints.py,sha256=WAr14OXt6oImKbcdeG60dDE_djJeTgsm-BSG7EfDbYA,4217
+optool/optimization/constraints.py,sha256=jpjrBnTWsc6rYjrJ9Hm9UFIwuUNBmXg04Wvr-x3JdTw,4206
 optool/optimization/helpers.py,sha256=BS9gHdDxPlWStSD621oKQPWFrhJgfaSbCnRvbmaEjks,4535
 optool/optimization/ode.py,sha256=DmTG8PPNkRuGnKiGieMJ5_qziti1_Ug_UDYIwumlv6Y,8014
-optool/optimization/problem.py,sha256=J79n1IfCBm7eGJ8OmZ3S5sosbPLaeYS8vS0OVCElKQo,21377
+optool/optimization/problem.py,sha256=uh_f-klvjd0Mn1M8mpoLwBuUR2qbzvnWKHtggDTML9U,21135
 optool/optimization/variables.py,sha256=fcIldRQcriB_k-XF15lfst95vD3eOHuXnV5aNCDV3wg,7159
-optool/serialization/__init__.py,sha256=n2Thhy2ICBOBDKctiHdEPlp4zg1GqGp2VmCz65paAJc,2755
-optool/serialization/dataframes.py,sha256=3WOaTWuKXh9FUSPSfsrcQDqyI6dApRPMmJ55ZCUczq8,2457
-optool/serialization/numeric.py,sha256=FCUms6UI3O8Xq4yY4sw5BWoeMiyUXxmpIKkfCCkJhsg,538
-optool/serialization/quantities.py,sha256=P9zAucaiA6GQ9yX0DOXrFTJR6H05JrAwaMhmQ1S3ags,729
-optool-0.2.0.dist-info/LICENSE.txt,sha256=hrghB2ojre3BR1nxeXTWOsbhc7CzFIRaC2r1Ez1gV10,1081
-optool-0.2.0.dist-info/METADATA,sha256=hNptWWmguTHnU9ao9EWAMxJf5mtQBHjEBNW_R8nmKDs,9017
-optool-0.2.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-optool-0.2.0.dist-info/top_level.txt,sha256=rBfjZiBTokmEUSgWAvNw23rTh-MvuILbz6o2LeWprko,7
-optool-0.2.0.dist-info/RECORD,,
+optool/serialization/__init__.py,sha256=K6VhGngQxaxGxkkOH0ZBrOKdF-sdZSQs5jpFAkmFCzs,2862
+optool/serialization/datetime_objects.py,sha256=YaCHFJLYhfbQJeJnchpo2eQPActARuMuYOGlNLW4tfs,1317
+optool/serialization/numpy_objects.py,sha256=ORn0PRNFDEospPZPfnRBftAvzFQ3sqT5F7PMX4yIHuQ,616
+optool/serialization/pandas_objects.py,sha256=layoYfhGtMLJeHZL4nwaeR2kuBqFK-OP1JBFlDPe4_Y,2880
+optool/serialization/pint_objects.py,sha256=xr2vjpCoT1JT--yElo9CjXIqR-VHlsQQhBS1IukdDfk,1217
+optool-0.3.0.dist-info/LICENSE.txt,sha256=hrghB2ojre3BR1nxeXTWOsbhc7CzFIRaC2r1Ez1gV10,1081
+optool-0.3.0.dist-info/METADATA,sha256=sIKTG_EIvqcOr2uBlueuGnIcq38MfUuq85CBrrZIzuY,7771
+optool-0.3.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+optool-0.3.0.dist-info/top_level.txt,sha256=rBfjZiBTokmEUSgWAvNw23rTh-MvuILbz6o2LeWprko,7
+optool-0.3.0.dist-info/RECORD,,
```

