# Comparing `tmp/asimov-0.6.0a2-py2.py3-none-any.whl.zip` & `tmp/asimov-0.6.0a3-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,54 +1,59 @@
-Zip file size: 87066 bytes, number of entries: 52
--rw-r--r--  2.0 unx     2808 b- defN 23-Apr-11 10:03 asimov/__init__.py
--rw-r--r--  2.0 unx    27309 b- defN 23-Apr-11 10:03 asimov/analysis.py
--rw-rw-rw-  2.0 unx      843 b- defN 23-Apr-11 10:03 asimov/asimov.conf
--rw-r--r--  2.0 unx    11152 b- defN 23-Apr-11 10:03 asimov/condor.py
--rw-r--r--  2.0 unx     1617 b- defN 23-Apr-11 10:03 asimov/database.py
--rw-r--r--  2.0 unx    14634 b- defN 23-Apr-11 10:03 asimov/event.py
--rw-r--r--  2.0 unx    12166 b- defN 23-Apr-11 10:03 asimov/git.py
--rw-r--r--  2.0 unx     7226 b- defN 23-Apr-11 10:03 asimov/gitlab.py
--rw-r--r--  2.0 unx     5398 b- defN 23-Apr-11 10:03 asimov/ini.py
--rw-r--r--  2.0 unx     9374 b- defN 23-Apr-11 10:03 asimov/ledger.py
--rw-r--r--  2.0 unx     2051 b- defN 23-Apr-11 10:03 asimov/locutus.py
--rw-r--r--  2.0 unx     7051 b- defN 23-Apr-11 10:03 asimov/logging.py
--rw-r--r--  2.0 unx     1646 b- defN 23-Apr-11 10:03 asimov/mattermost.py
--rw-r--r--  2.0 unx     1586 b- defN 23-Apr-11 10:03 asimov/olivaw.py
--rw-r--r--  2.0 unx    14096 b- defN 23-Apr-11 10:03 asimov/pipeline.py
--rw-r--r--  2.0 unx     4513 b- defN 23-Apr-11 10:03 asimov/review.py
--rw-r--r--  2.0 unx    10087 b- defN 23-Apr-11 10:03 asimov/storage.py
--rw-r--r--  2.0 unx     1156 b- defN 23-Apr-11 10:03 asimov/testing.py
--rw-r--r--  2.0 unx     2134 b- defN 23-Apr-11 10:03 asimov/utils.py
--rw-rw-rw-  2.0 unx      395 b- defN 23-Apr-11 10:03 asimov/cli/__init__.py
--rw-rw-rw-  2.0 unx     4948 b- defN 23-Apr-11 10:03 asimov/cli/application.py
--rw-rw-rw-  2.0 unx     1141 b- defN 23-Apr-11 10:03 asimov/cli/configuration.py
--rw-rw-rw-  2.0 unx     1285 b- defN 23-Apr-11 10:03 asimov/cli/data.py
--rw-rw-rw-  2.0 unx     9010 b- defN 23-Apr-11 10:03 asimov/cli/event.py
--rw-rw-rw-  2.0 unx        0 b- defN 23-Apr-11 10:03 asimov/cli/ledger.py
--rw-rw-rw-  2.0 unx    10312 b- defN 23-Apr-11 10:03 asimov/cli/manage.py
--rw-rw-rw-  2.0 unx    14640 b- defN 23-Apr-11 10:03 asimov/cli/monitor.py
--rw-rw-rw-  2.0 unx     5606 b- defN 23-Apr-11 10:03 asimov/cli/production.py
--rw-rw-rw-  2.0 unx     6016 b- defN 23-Apr-11 10:03 asimov/cli/project.py
--rw-rw-rw-  2.0 unx     5225 b- defN 23-Apr-11 10:03 asimov/cli/report.py
--rw-rw-rw-  2.0 unx     2817 b- defN 23-Apr-11 10:03 asimov/cli/review.py
--rw-rw-rw-  2.0 unx       96 b- defN 23-Apr-11 10:03 asimov/cli/report-theme/body.html
--rw-rw-rw-  2.0 unx      247 b- defN 23-Apr-11 10:03 asimov/cli/report-theme/footer.html
--rw-rw-rw-  2.0 unx     1467 b- defN 23-Apr-11 10:03 asimov/cli/report-theme/head.html
--rw-rw-rw-  2.0 unx      841 b- defN 23-Apr-11 10:03 asimov/cli/report-theme/header.html
--rw-rw-rw-  2.0 unx      220 b- defN 23-Apr-11 10:03 asimov/configs/README.rst
--rw-rw-rw-  2.0 unx     2903 b- defN 23-Apr-11 10:03 asimov/configs/bayeswave.ini
--rw-rw-rw-  2.0 unx    12328 b- defN 23-Apr-11 10:03 asimov/configs/bilby.ini
--rw-rw-rw-  2.0 unx     6238 b- defN 23-Apr-11 10:03 asimov/configs/lalinference.ini
--rw-rw-rw-  2.0 unx     7569 b- defN 23-Apr-11 10:03 asimov/configs/rift.ini
--rw-rw-rw-  2.0 unx      610 b- defN 23-Apr-11 10:03 asimov/pipelines/__init__.py
--rw-rw-rw-  2.0 unx    19216 b- defN 23-Apr-11 10:03 asimov/pipelines/bayeswave.py
--rw-rw-rw-  2.0 unx    12809 b- defN 23-Apr-11 10:03 asimov/pipelines/bilby.py
--rw-rw-rw-  2.0 unx     9489 b- defN 23-Apr-11 10:03 asimov/pipelines/lalinference.py
--rw-rw-rw-  2.0 unx    17635 b- defN 23-Apr-11 10:03 asimov/pipelines/rift.py
--rw-rw-rw-  2.0 unx      755 b- defN 23-Apr-11 10:03 asimov-0.6.0a2.dist-info/LICENSE
--rw-r--r--  2.0 unx     1392 b- defN 23-Apr-11 10:03 asimov-0.6.0a2.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 23-Apr-11 10:03 asimov-0.6.0a2.dist-info/WHEEL
--rw-r--r--  2.0 unx      108 b- defN 23-Apr-11 10:03 asimov-0.6.0a2.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        7 b- defN 23-Apr-11 10:03 asimov-0.6.0a2.dist-info/top_level.txt
--rw-r--r--  2.0 unx        1 b- defN 23-Apr-11 10:03 asimov-0.6.0a2.dist-info/zip-safe
--rw-rw-r--  2.0 unx     4116 b- defN 23-Apr-11 10:03 asimov-0.6.0a2.dist-info/RECORD
-52 files, 296399 bytes uncompressed, 80664 bytes compressed:  72.8%
+Zip file size: 94098 bytes, number of entries: 57
+-rw-r--r--  2.0 unx     2794 b- defN 23-May-23 15:36 asimov/__init__.py
+-rw-r--r--  2.0 unx    32456 b- defN 23-May-23 15:36 asimov/analysis.py
+-rw-rw-rw-  2.0 unx      828 b- defN 23-May-23 15:36 asimov/asimov.conf
+-rw-r--r--  2.0 unx    11115 b- defN 23-May-23 15:36 asimov/condor.py
+-rw-r--r--  2.0 unx     1617 b- defN 23-May-23 15:36 asimov/database.py
+-rw-r--r--  2.0 unx    14506 b- defN 23-May-23 15:36 asimov/event.py
+-rw-r--r--  2.0 unx    12188 b- defN 23-May-23 15:36 asimov/git.py
+-rw-r--r--  2.0 unx     7226 b- defN 23-May-23 15:36 asimov/gitlab.py
+-rw-r--r--  2.0 unx     5398 b- defN 23-May-23 15:36 asimov/ini.py
+-rw-r--r--  2.0 unx     9868 b- defN 23-May-23 15:36 asimov/ledger.py
+-rw-r--r--  2.0 unx     2051 b- defN 23-May-23 15:36 asimov/locutus.py
+-rw-r--r--  2.0 unx     7051 b- defN 23-May-23 15:36 asimov/logging.py
+-rw-r--r--  2.0 unx     1646 b- defN 23-May-23 15:36 asimov/mattermost.py
+-rw-r--r--  2.0 unx     1586 b- defN 23-May-23 15:36 asimov/olivaw.py
+-rw-r--r--  2.0 unx    16426 b- defN 23-May-23 15:36 asimov/pipeline.py
+-rw-r--r--  2.0 unx     4513 b- defN 23-May-23 15:36 asimov/review.py
+-rw-r--r--  2.0 unx    10087 b- defN 23-May-23 15:36 asimov/storage.py
+-rw-r--r--  2.0 unx     1156 b- defN 23-May-23 15:36 asimov/testing.py
+-rw-r--r--  2.0 unx     3050 b- defN 23-May-23 15:36 asimov/utils.py
+-rw-rw-rw-  2.0 unx      395 b- defN 23-May-23 15:36 asimov/cli/__init__.py
+-rw-rw-rw-  2.0 unx     7146 b- defN 23-May-23 15:36 asimov/cli/application.py
+-rw-rw-rw-  2.0 unx     1175 b- defN 23-May-23 15:36 asimov/cli/configuration.py
+-rw-rw-rw-  2.0 unx     1285 b- defN 23-May-23 15:36 asimov/cli/data.py
+-rw-rw-rw-  2.0 unx     9096 b- defN 23-May-23 15:36 asimov/cli/event.py
+-rw-rw-rw-  2.0 unx        0 b- defN 23-May-23 15:36 asimov/cli/ledger.py
+-rw-rw-rw-  2.0 unx    10355 b- defN 23-May-23 15:36 asimov/cli/manage.py
+-rw-rw-rw-  2.0 unx    15477 b- defN 23-May-23 15:36 asimov/cli/monitor.py
+-rw-rw-rw-  2.0 unx     5606 b- defN 23-May-23 15:36 asimov/cli/production.py
+-rw-rw-rw-  2.0 unx     6089 b- defN 23-May-23 15:36 asimov/cli/project.py
+-rw-rw-rw-  2.0 unx     5271 b- defN 23-May-23 15:36 asimov/cli/report.py
+-rw-rw-rw-  2.0 unx     2817 b- defN 23-May-23 15:36 asimov/cli/review.py
+-rw-rw-rw-  2.0 unx       96 b- defN 23-May-23 15:36 asimov/cli/report-theme/body.html
+-rw-rw-rw-  2.0 unx      247 b- defN 23-May-23 15:36 asimov/cli/report-theme/footer.html
+-rw-rw-rw-  2.0 unx     1467 b- defN 23-May-23 15:36 asimov/cli/report-theme/head.html
+-rw-rw-rw-  2.0 unx      841 b- defN 23-May-23 15:36 asimov/cli/report-theme/header.html
+-rw-rw-rw-  2.0 unx      220 b- defN 23-May-23 15:36 asimov/configs/README.rst
+-rw-rw-rw-  2.0 unx     3004 b- defN 23-May-23 15:36 asimov/configs/bayeswave.ini
+-rw-rw-rw-  2.0 unx    14237 b- defN 23-May-23 15:36 asimov/configs/bilby.ini
+-rw-rw-rw-  2.0 unx     6291 b- defN 23-May-23 15:36 asimov/configs/lalinference.ini
+-rw-rw-rw-  2.0 unx     7586 b- defN 23-May-23 15:36 asimov/configs/rift.ini
+-rw-rw-rw-  2.0 unx      772 b- defN 23-May-23 15:36 asimov/pipelines/__init__.py
+-rw-rw-rw-  2.0 unx    19726 b- defN 23-May-23 15:36 asimov/pipelines/bayeswave.py
+-rw-rw-rw-  2.0 unx    13147 b- defN 23-May-23 15:36 asimov/pipelines/bilby.py
+-rw-rw-rw-  2.0 unx     9489 b- defN 23-May-23 15:36 asimov/pipelines/lalinference.py
+-rw-rw-rw-  2.0 unx    10853 b- defN 23-May-23 15:36 asimov/pipelines/pesummary.py
+-rw-rw-rw-  2.0 unx    17646 b- defN 23-May-23 15:36 asimov/pipelines/rift.py
+-rw-rw-rw-  2.0 unx       96 b- defN 23-May-23 15:36 asimov/report-theme/body.html
+-rw-rw-rw-  2.0 unx      247 b- defN 23-May-23 15:36 asimov/report-theme/footer.html
+-rw-rw-rw-  2.0 unx     1467 b- defN 23-May-23 15:36 asimov/report-theme/head.html
+-rw-rw-rw-  2.0 unx      841 b- defN 23-May-23 15:36 asimov/report-theme/header.html
+-rw-rw-rw-  2.0 unx      755 b- defN 23-May-23 15:36 asimov-0.6.0a3.dist-info/LICENSE
+-rw-r--r--  2.0 unx     1392 b- defN 23-May-23 15:36 asimov-0.6.0a3.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 23-May-23 15:36 asimov-0.6.0a3.dist-info/WHEEL
+-rw-r--r--  2.0 unx      108 b- defN 23-May-23 15:36 asimov-0.6.0a3.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        7 b- defN 23-May-23 15:36 asimov-0.6.0a3.dist-info/top_level.txt
+-rw-r--r--  2.0 unx        1 b- defN 23-May-23 15:36 asimov-0.6.0a3.dist-info/zip-safe
+-rw-rw-r--  2.0 unx     4547 b- defN 23-May-23 15:36 asimov-0.6.0a3.dist-info/RECORD
+57 files, 325467 bytes uncompressed, 87018 bytes compressed:  73.3%
```

## zipnote {}

```diff
@@ -126,32 +126,47 @@
 
 Filename: asimov/pipelines/bilby.py
 Comment: 
 
 Filename: asimov/pipelines/lalinference.py
 Comment: 
 
+Filename: asimov/pipelines/pesummary.py
+Comment: 
+
 Filename: asimov/pipelines/rift.py
 Comment: 
 
-Filename: asimov-0.6.0a2.dist-info/LICENSE
+Filename: asimov/report-theme/body.html
+Comment: 
+
+Filename: asimov/report-theme/footer.html
+Comment: 
+
+Filename: asimov/report-theme/head.html
+Comment: 
+
+Filename: asimov/report-theme/header.html
+Comment: 
+
+Filename: asimov-0.6.0a3.dist-info/LICENSE
 Comment: 
 
-Filename: asimov-0.6.0a2.dist-info/METADATA
+Filename: asimov-0.6.0a3.dist-info/METADATA
 Comment: 
 
-Filename: asimov-0.6.0a2.dist-info/WHEEL
+Filename: asimov-0.6.0a3.dist-info/WHEEL
 Comment: 
 
-Filename: asimov-0.6.0a2.dist-info/entry_points.txt
+Filename: asimov-0.6.0a3.dist-info/entry_points.txt
 Comment: 
 
-Filename: asimov-0.6.0a2.dist-info/top_level.txt
+Filename: asimov-0.6.0a3.dist-info/top_level.txt
 Comment: 
 
-Filename: asimov-0.6.0a2.dist-info/zip-safe
+Filename: asimov-0.6.0a3.dist-info/zip-safe
 Comment: 
 
-Filename: asimov-0.6.0a2.dist-info/RECORD
+Filename: asimov-0.6.0a3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## asimov/__init__.py

```diff
@@ -74,15 +74,15 @@
     PRINT_LEVEL = logging.ERROR
 
 ch = logging.StreamHandler()
 print_formatter = logging.Formatter("[%(levelname)s] %(message)s", "%Y-%m-%d %H:%M:%S")
 ch.setFormatter(print_formatter)
 ch.setLevel(PRINT_LEVEL)
 
-logfile = os.path.join("asimov.log")
+logfile = "asimov.log"
 fh = logging.FileHandler(logfile)
 formatter = logging.Formatter(
     "%(asctime)s [%(name)s][%(levelname)s] %(message)s", "%Y-%m-%d %H:%M:%S"
 )
 fh.setFormatter(formatter)
 fh.setLevel(LOGGER_LEVEL)
```

## asimov/analysis.py

```diff
@@ -21,29 +21,43 @@
   on all events, including event and simple analyses.
   This type of analysis is useful for defining a population analysis, for example.
 
 """
 import os
 import configparser
 from copy import deepcopy
-from warnings import warn
 
 from functools import reduce
 import operator
 
 from liquid import Liquid
 
 from asimov import config, logger, LOGGER_LEVEL
 from asimov.pipelines import known_pipelines
 from asimov.utils import update
 from asimov.storage import Store
 
 from .review import Review
 from .ini import RunConfiguration
 
+status_map = {
+    "cancelled": "light",
+    "finished": "success",
+    "uploaded": "success",
+    "processing": "primary",
+    "running": "primary",
+    "stuck": "warning",
+    "restart": "secondary",
+    "ready": "secondary",
+    "wait": "light",
+    "stop": "danger",
+    "manual": "light",
+    "stopped": "light",
+}
+
 
 class Analysis:
     """
     The base class for all other types of analysis.
 
     TODO: Add a check to make sure names cannot conflict
     """
@@ -290,42 +304,201 @@
 
         liq = Liquid(template_file)
         rendered = liq.render(production=self, config=config)
 
         with open(filename, "w") as output_file:
             output_file.write(rendered)
 
+    def build_report(self):
+        if self.pipeline:
+            self.pipeline.build_report()
 
-class SimpleAnalysis(Analysis):
+    def html(self):
+        """
+        An HTML representation of this production.
+        """
+        production = self
+
+        card = ""
+
+        card += f"<div class='asimov-analysis asimov-analysis-{self.status}'>"
+        card += f"<h4>{self.name}"
+
+        if self.comment:
+            card += (
+                f"""  <small class="asimov-comment text-muted">{self.comment}</small>"""
+            )
+        card += "</h4>"
+        if self.status:
+            card += f"""<p class="asimov-status">
+  <span class="badge badge-pill badge-{status_map[self.status]}">{self.status}</span>
+</p>"""
+
+        if self.pipeline:
+            card += f"""<p class="asimov-pipeline-name">{self.pipeline.name}</p>"""
+
+        if self.pipeline:
+            # self.pipeline.collect_pages()
+            card += self.pipeline.html()
+
+        if self.rundir:
+            card += f"""<p class="asimov-rundir"><code>{production.rundir}</code></p>"""
+        else:
+            card += """&nbsp;"""
+
+        if "approximant" in production.meta:
+            card += f"""<p class="asimov-attribute">Waveform approximant:
+   <span class="asimov-approximant">{production.meta['approximant']}</span>
+</p>"""
+
+        card += """&nbsp;"""
+        card += """</div>"""
+
+        if len(self.review) > 0:
+            for review in self.review:
+                card += review.html()
+
+        return card
+
+
+class PostAnalysis(Analysis):
     """
-    A single subject, single pipeline analysis.
+    A post-processing analysis
     """
 
     def __init__(self, subject, name, pipeline, **kwargs):
 
         self.event = self.subject = subject
         self.name = name
 
-        self.logger = logger.getChild("event").getChild(f"{self.name}")
+        self.logger = logger.getChild("postprocessing").getChild(f"{self.name}")
         self.logger.setLevel(LOGGER_LEVEL)
 
         if "status" in kwargs:
             self.status_str = kwargs["status"].lower()
         else:
             self.status_str = "none"
 
         self.meta = deepcopy(self.meta_defaults)
+
+        # Start by adding pipeline defaults
+        if "pipelines" in self.event.ledger.data:
+            if pipeline in self.event.ledger.data["pipelines"]:
+                self.meta = update(
+                    self.meta, deepcopy(self.event.ledger.data["pipelines"][pipeline])
+                )
+
+        self.meta = update(self.meta, deepcopy(self.subject.meta))
+
+        self.meta = update(self.meta, deepcopy(kwargs))
+        self.meta["pipeline"] = pipeline.lower()
+        self.pipeline = pipeline.lower()
+        self.pipeline = known_pipelines[pipeline.lower()](self)
+        # if "needs" in self.meta:
+        #     self._needs = self.meta.pop("needs")
+        # else:
+        #     self._needs = []
+        if "comment" in kwargs:
+            self.comment = kwargs["comment"]
+        else:
+            self.comment = None
+
+    def to_dict(self):
+        """
+        Return this project production as a dictionary.
+
+        Parameters
+        ----------
+        event : bool
+           If set to True the output is designed to be included nested within an event.
+           The event name is not included in the representation, and the production name is provided as a key.
+        """
+        dictionary = {}
+        dictionary = update(dictionary, self.meta)
+        dictionary["name"] = self.name
+        dictionary["status"] = self.status
+        if isinstance(self.pipeline, str):
+            dictionary["pipeline"] = self.pipeline
+        else:
+            dictionary["pipeline"] = self.pipeline.name.lower()
+        dictionary["comment"] = self.comment
+
+        if self.review:
+            dictionary["review"] = self.review.to_dicts()
+
+        dictionary["needs"] = self.dependencies
+
+        if "ledger" in dictionary:
+            dictionary.pop("ledger")
+
+        # dictionary['subjects'] = self.subjects
+        # dictionary['analyses'] = self._analysis_spec
+
+        output = dictionary
+
+        return output
+
+    @classmethod
+    def from_dict(cls, parameters, subject):
+        parameters = deepcopy(parameters)
+        # Check that pars is a dictionary
+        if not {"pipeline", "name"} <= parameters.keys():
+            raise ValueError(
+                f"Some of the required parameters are missing."
+                f"Found {parameters.keys()}"
+            )
+        if "status" not in parameters:
+            parameters["status"] = "ready"
+        if "event" in parameters:
+            parameters.pop("event")
+        pipeline = parameters.pop("pipeline")
+        name = parameters.pop("name")
+        if "comment" not in parameters:
+            parameters["comment"] = None
+
+        return cls(subject, name, pipeline, **parameters)
+
+
+class SimpleAnalysis(Analysis):
+    """
+    A single subject, single pipeline analysis.
+    """
+
+    def __init__(self, subject, name, pipeline, status=None, comment=None, **kwargs):
+
+        self.event = self.subject = subject
+        self.name = name
+
+        self.logger = logger.getChild("event").getChild(f"{self.name}")
+        self.logger.setLevel(LOGGER_LEVEL)
+
+        if status:
+            self.status_str = status.lower()
+        else:
+            self.status_str = "none"
+
+        self.meta = deepcopy(self.meta_defaults)
+
+        # Start by adding pipeline defaults
+        if "pipelines" in self.event.ledger.data:
+            if pipeline in self.event.ledger.data["pipelines"]:
+                self.meta = update(
+                    self.meta, deepcopy(self.event.ledger.data["pipelines"][pipeline])
+                )
+
+        # if "postprocessing" in self.event.ledger.data:
+        #     self.meta["postprocessing"] = deepcopy(
+        #         self.event.ledger.data["postprocessing"]
+        #     )
+
         self.meta = update(self.meta, deepcopy(self.subject.meta))
         if "productions" in self.meta:
             self.meta.pop("productions")
-        # if "needs" in self.meta:
-        #    self.meta.pop("needs")
 
         self.meta = update(self.meta, deepcopy(kwargs))
-        self.meta["pipeline"] = pipeline.lower()
         self.pipeline = pipeline.lower()
         self.pipeline = known_pipelines[pipeline.lower()](self)
         if "needs" in self.meta:
             self._needs = self.meta.pop("needs")
         else:
             self._needs = []
         if "comment" in kwargs:
@@ -340,14 +513,15 @@
         Parameters
         ----------
         event : bool
            If set to True the output is designed to be included nested within an event.
            The event name is not included in the representation, and the production name is provided as a key.
         """
         dictionary = {}
+        dictionary = update(dictionary, self.meta)
         if not event:
             dictionary["event"] = self.event.name
             dictionary["name"] = self.name
 
         dictionary["status"] = self.status
         if isinstance(self.pipeline, str):
             dictionary["pipeline"] = self.pipeline
@@ -401,23 +575,23 @@
 
 
 class SubjectAnalysis(Analysis):
     """
     A single subject analysis which requires results from multiple pipelines.
     """
 
-    def __init__(self, subject, name, pipeline, **kwargs):
+    def __init__(self, subject, name, pipeline, status=None, comment=None, **kwargs):
         self.event = self.subject = subject
         self.name = name
 
         self.logger = logger.getChild("event").getChild(f"{self.name}")
         self.logger.setLevel(LOGGER_LEVEL)
 
-        if "status" in kwargs:
-            self.status_str = kwargs["status"].lower()
+        if status:
+            self.status_str = status.lower()
         else:
             self.status_str = "none"
 
         self.meta = deepcopy(self.meta_defaults)
         self.meta = update(self.meta, deepcopy(self.subject.meta))
         if "productions" in self.meta:
             self.meta.pop("productions")
@@ -444,14 +618,16 @@
         Parameters
         ----------
         event : bool
            If set to True the output is designed to be included nested within an event.
            The event name is not included in the representation, and the production name is provided as a key.
         """
         dictionary = {}
+        dictionary = update(dictionary, self.meta)
+
         if not event:
             dictionary["event"] = self.event.name
             dictionary["name"] = self.name
 
         dictionary["status"] = self.status
         if isinstance(self.pipeline, str):
             dictionary["pipeline"] = self.pipeline
@@ -505,15 +681,17 @@
 
 
 class ProjectAnalysis(Analysis):
     """
     A multi-subject analysis.
     """
 
-    def __init__(self, subjects, analyses, name, pipeline, ledger=None, **kwargs):
+    def __init__(
+            self, subjects, analyses, name, pipeline, comment=None, ledger=None, **kwargs
+    ):
         """ """
         super().__init__()
 
         self.name = name  # if name else "unnamed project analysis"
 
         self.logger = logger.getChild("project analyses").getChild(f"{self.name}")
         self.logger.setLevel(LOGGER_LEVEL)
@@ -598,15 +776,17 @@
         Parameters
         ----------
         event : bool
            If set to True the output is designed to be included nested within an event.
            The event name is not included in the representation, and the production name is provided as a key.
         """
         dictionary = {}
-        dictionary["name"] = self.name
+        dictionary = update(dictionary, self.meta)
+
+        dictionary['name'] = self.name
         dictionary["status"] = self.status
         if isinstance(self.pipeline, str):
             dictionary["pipeline"] = self.pipeline
         else:
             dictionary["pipeline"] = self.pipeline.name.lower()
         dictionary["comment"] = self.comment
 
@@ -630,30 +810,29 @@
 
         dictionary["subjects"] = self.subjects
         dictionary["analyses"] = self._analysis_spec
 
         output = dictionary
 
         return output
-
-
+    
 class GravitationalWaveTransient(SimpleAnalysis):
     """
     A single subject, single pipeline analysis for a gravitational wave transient.
     """
 
     def __init__(self, subject, name, pipeline, **kwargs):
 
         self.category = config.get("general", "calibration_directory")
 
         super().__init__(subject, name, pipeline, **kwargs)
         self._add_missing_parameters()
         self._checks()
 
-        self.psds = self._set_psds()
+        self.psds = self._collect_psds()
         self.xml_psds = self._collect_psds(format="xml")
 
     def _collect_psds(self, format="ascii"):
         """
         Collect the required psds for this production.
         """
         psds = {}
@@ -693,14 +872,25 @@
             psds = {}
 
         for ifo, psd in psds.items():
             self.logger.debug(f"PSD-{ifo}: {psd}")
 
         return psds
 
+    def _check_compatible(self, other_production):
+        """
+        Check that the data settings in two productions are sufficiently compatible
+        that one can be used as a dependency of the other.
+        """
+        compatible = True
+
+        # compatible = self.meta["likelihood"] == other_production.meta["likelihood"]
+        # compatible = self.meta["data"] == other_production.meta["data"]
+        return compatible
+
     def _add_missing_parameters(self):
         for parameter in {"quality", "waveform", "likelihood"}:
             if parameter not in self.meta:
                 self.meta[parameter] = {}
 
         for parameter in {"marginalization"}:
             if parameter not in self.meta["likelihood"]:
@@ -712,29 +902,25 @@
 
     def _checks(self):
         """
         Carry-out a number of data consistency checks on the information from the ledger.
         """
         # Check that the upper frequency is included, otherwise calculate it
 
-        if self.quality:
-            if ("high-frequency" not in self.quality) and (
-                "sample-rate" in self.quality
+        if "quality" in self.meta:
+            if ("maximum frequency" not in self.meta["quality"]) and (
+                "sample rate" in self.meta["likelihood"]
             ):
+                self.meta["quality"]["maximum frequency"] = {}
                 # Account for the PSD roll-off with the 0.875 factor
-                self.meta["quality"]["high-frequency"] = int(
-                    0.875 * self.meta["quality"]["sample-rate"] / 2
-                )
-            elif ("high-frequency" in self.quality) and ("sample-rate" in self.quality):
-                if self.meta["quality"]["high-frequency"] != int(
-                    0.875 * self.meta["quality"]["sample-rate"] / 2
-                ):
-                    warn(
-                        "The upper-cutoff frequency is not equal to 0.875 times the Nyquist frequency."
+                for ifo in self.meta["interferometers"]:
+                    self.meta["quality"]["maximum frequency"][ifo] = int(
+                        0.875 * self.meta["likelihood"]["sample rate"] / 2
                     )
+                self.event.ledger.update_event(self.event)
 
     @property
     def quality(self):
         if "quality" in self.meta:
             return self.meta["quality"]
         else:
             return None
@@ -746,32 +932,14 @@
         """
         ifos = self.meta["interferometers"]
         if len(ifos) == 1:
             return ifos[0]
         else:
             return "".join(ifos[:2])
 
-    def _set_psds(self):
-        """
-        Return the PSDs stored for this transient event.
-        """
-        if "psds" in self.meta and self.quality:
-            if self.quality["sample-rate"] in self.meta["psds"]:
-                self.psds = self.meta["psds"][self.quality["sample-rate"]]
-            else:
-                self.psds = {}
-        else:
-            self.psds = {}
-
-        for ifo, psd in self.psds.items():
-            if self.subject.repository:
-                self.psds[ifo] = os.path.join(self.subject.repository.directory, psd)
-            else:
-                self.psds[ifo] = psd
-
     def get_timefile(self):
         """
         Find this event's time file.
 
         Returns
         -------
         str
@@ -821,11 +989,7 @@
             ini = RunConfiguration(ini_loc)
         except ValueError:
             raise ValueError("Could not open the ini file")
         except configparser.MissingSectionHeaderError:
             raise ValueError("This isn't a valid ini file")
 
         return ini
-
-
-class Production(SimpleAnalysis):
-    pass
```

## asimov/asimov.conf

```diff
@@ -11,37 +11,39 @@
 location = logs
 
 [project]
 
 [report]
 timezone = Europe/London
 
+[asimov start]
+accounting = ligo.dev.o4.cbc.pe.lalinference
+
 [pipelines]
 environment = /cvmfs/oasis.opensciencegrid.org/ligo/sw/conda/envs/igwn-py39
-accounting = ligo.dev.o4.cbc.pe.lalinference
 
 [ledger]
 engine = yamlfile
 location = ledger.yaml
 
 [storage]
 root = ""
 results_store = results/
 
 [rift]
 environment = /cvmfs/oasis.opensciencegrid.org/ligo/sw/conda/envs/igwn-py39
 
 [mattermost]
-webhook_url = https://chat.ligo.org/hooks/i5k56qcs1fnaiqa1i9qrsdexby
 
 [gracedb]
 url = https://gracedb.ligo.org/api/
 
 [pesummary]
 executable = /cvmfs/oasis.opensciencegrid.org/ligo/sw/conda/envs/igwn-py39/bin/summarypages
 
 [condor]
 cache_time = 900
 cron_minute = */15
+accounting = ligo.dev.o4.cbc.pe.bilby
 
 [theme]
 name = report-theme
```

## asimov/condor.py

```diff
@@ -15,15 +15,14 @@
 from asimov import config, logger, LOGGER_LEVEL
 
 UTC = tz.tzutc()
 
 logger = logger.getChild("condor")
 logger.setLevel(LOGGER_LEVEL)
 
-
 def datetime_from_epoch(dt, tzinfo=UTC):
     """Returns the `datetime.datetime` for a given Unix epoch
 
     Parameters
     ----------
     dt : `float`
         a Unix timestamp
@@ -34,21 +33,14 @@
     Returns
     -------
     datetime.datetime
         the datetime that represents the given Unix epoch
     """
     return datetime.datetime.utcfromtimestamp(dt).replace(tzinfo=tzinfo)
 
-
-UTC = tz.tzutc()
-
-logger = logger.getChild("condor")
-logger.setLevel(LOGGER_LEVEL)
-
-
 def submit_job(submit_description):
     """
     Submit a new job to the condor scheduller
     """
 
     hostname_job = htcondor.Submit(submit_description)
 
@@ -280,15 +272,15 @@
 
     The list is automatically pulled from the condor scheduller if it is
     more than 15 minutes old (by default)
     """
 
     def __init__(self):
         self.jobs = {}
-        cache = "_cache_jobs.yaml"
+        cache = os.path.join(".asimov", "_cache_jobs.yaml")
         if not os.path.exists(cache):
             self.refresh()
         else:
             age = -os.stat(cache).st_mtime + datetime.datetime.now().timestamp()
             logger.info(f"Condor cache is {age} seconds old")
             if float(age) < float(config.get("condor", "cache_time")):
                 with open(cache, "r") as f:
@@ -356,9 +348,9 @@
         for datum in retdat:
             if datum.dag:
                 if datum.dag in self.jobs:
                     self.jobs[datum.dag].add_subjob(datum)
                 else:
                     self.jobs[datum.idno] = datum.to_dict()
 
-        with open("_cache_jobs.yaml", "w") as f:
+        with open(os.path.join(".asimov", "_cache_jobs.yaml"), "w") as f:
             f.write(yaml.dump(self.jobs))
```

## asimov/event.py

```diff
@@ -8,14 +8,17 @@
 import subprocess
 
 import networkx as nx
 import yaml
 from ligo.gracedb.rest import GraceDb, HTTPError
 
 from asimov import config, logger, LOGGER_LEVEL
+from asimov.pipelines import known_pipelines
+from asimov.storage import Store
+from asimov.utils import update, diff_dict
 from asimov.analysis import GravitationalWaveTransient
 
 from .git import EventRepo
 
 status_map = {
     "cancelled": "light",
     "finished": "success",
@@ -27,31 +30,14 @@
     "ready": "secondary",
     "wait": "light",
     "stop": "danger",
     "manual": "light",
     "stopped": "light",
 }
 
-
-status_map = {
-    "cancelled": "light",
-    "finished": "success",
-    "uploaded": "success",
-    "processing": "primary",
-    "running": "primary",
-    "stuck": "warning",
-    "restart": "secondary",
-    "ready": "secondary",
-    "wait": "light",
-    "stop": "danger",
-    "manual": "light",
-    "stopped": "light",
-}
-
-
 class DescriptionException(Exception):
     """Exception for event description problems."""
 
     def __init__(self, message, production=None):
         super(DescriptionException, self).__init__(message)
         self.message = message
         self.production = production
@@ -153,15 +139,19 @@
                     Production.from_dict(
                         production,
                         subject=self,
                     )
                 )
         self._check_required()
 
-        if ("interferometers" in self.meta) and ("calibration" in self.meta):
+        if (
+            ("interferometers" in self.meta)
+            and ("calibration" in self.meta)
+            and ("data" in self.meta)
+        ):
             try:
                 self._check_calibration()
             except DescriptionException:
                 pass
 
     @property
     def analyses(self):
```

## asimov/git.py

```diff
@@ -185,15 +185,17 @@
             except IndexError:
                 raise AsimovFileNotFound
 
     def find_coincfile(self, category=config.get("general", "calibration_directory")):
         """
         Find the coinc file for this calibration category in this repository.
         """
-        coinc_file = glob.glob(os.path.join(os.getcwd(), self.directory, category, "*coinc*.xml"))
+        coinc_file = glob.glob(
+            os.path.join(os.getcwd(), self.directory, category, "*coinc*.xml")
+        )
 
         if len(coinc_file) > 0:
             return coinc_file[0]
         else:
             raise AsimovFileNotFound
 
     def find_prods(
```

## asimov/ledger.py

```diff
@@ -7,15 +7,15 @@
 
 import os
 import shutil
 
 import asimov
 import asimov.database
 from asimov import config
-from asimov.analysis import ProjectAnalysis
+from asimov.analysis import ProjectAnalysis, PostAnalysis
 from asimov.event import Event, Production
 from asimov.utils import update, set_directory
 
 
 class Ledger:
     @classmethod
     def create(cls, name=None, engine=None, location=None):
@@ -39,30 +39,33 @@
         elif engine == "gitlab":
             raise NotImplementedError(
                 "This hasn't been ported to the new interface yet. Stay tuned!"
             )
 
 
 class YAMLLedger(Ledger):
-    def __init__(self, location=".asimov/ledger.yml"):
+    def __init__(self, location=None):
+        if not location:
+            location = os.path.join(".asimov", "ledger.yml")
         self.location = location
         with open(location, "r") as ledger_file:
             self.data = yaml.safe_load(ledger_file)
 
         self.data["events"] = [
             update(self.get_defaults(), event, inplace=False)
             for event in self.data["events"]
         ]
 
         self.events = {ev["name"]: ev for ev in self.data["events"]}
         self.data.pop("events")
 
     @classmethod
-    def create(cls, name, location=".asimov/ledger.yml"):
-
+    def create(cls, name, location=None):
+        if not location:
+            location = os.path.join(".asimov", "ledger.yml")
         data = {}
         data["asimov"] = {}
         data["asimov"]["version"] = asimov.__version__
         data["events"] = []
         data["project analyses"] = []
         data["project"] = {}
         data["project"]["name"] = name
@@ -147,17 +150,15 @@
             self.data["project analyses"].append(analysis.to_dict())
         else:
             event.add_production(analysis)
             self.events[event.name] = event.to_dict()
         self.save()
 
     def add_production(self, event, production):
-        event.add_production(production)
-        self.events[event.name] = event.to_dict()
-        self.save()
+        self.add_analysis(production=production, event=event)
 
     def get_defaults(self):
         """
         Gather project-level defaults from the ledger.
 
         At present data, quality, priors, and likelihood settings can all be set at a project level as defaults.
         """
@@ -177,14 +178,26 @@
     @property
     def project_analyses(self):
         return [
             ProjectAnalysis.from_dict(analysis, ledger=self)
             for analysis in self.data["project analyses"]
         ]
 
+    def postprocessing(self, subject):
+        """
+        Return a list of all postprocessing jobs defined in this project.
+        """
+        if "postprocessing" in self.data:
+            return [
+                PostAnalysis.from_dict(analysis, subject=subject, ledger=self)
+                for analysis in self.data["postprocessing"]
+            ]
+        else:
+            return []
+
     def get_event(self, event=None):
         if event:
             return [Event(**self.events[event], ledger=self)]
         else:
             return [
                 Event(**self.events[event], ledger=self) for event in self.events.keys()
             ]
```

## asimov/pipeline.py

```diff
@@ -1,13 +1,14 @@
 """Defines the interface with generic analysis pipelines."""
 import configparser
 import os
 import subprocess
 import time
 import warnings
+from copy import deepcopy
 
 warnings.filterwarnings("ignore", module="htcondor")
 
 
 import htcondor  # NoQA
 
 from asimov import utils  # NoQA
@@ -136,15 +137,19 @@
         """
         Define a hook to run after the DAG has completed execution successfully.
 
         Note, this method should take no arguments, and should be over-written in the
         specific pipeline implementation if required.
         """
         self.production.status = "finished"
-        self.production.meta.pop("job id")
+        # self.production.meta.pop("job id")
+
+        # post_pipeline = PESummary(production=self.production, subject=self.production.subject)
+        # cluster = post_pipeline.submit_dag()
+        # self.production.meta["job id"] = int(cluster)
 
     def collect_assets(self):
         """
         Add the various analysis assets from the run directory to the git repository.
         """
         repo = self.production.event.repository
 
@@ -303,162 +308,239 @@
                     for message in log.split("\n"):
                         report_logs + message
             # with report_config:
             #     report_config + self.
 
 
 class PostPipeline:
-    def __init__(self, production, category=None):
-        self.production = production
+    """This class describes post processing pipelines which are a
+    class of pipelines which can be run on multiple analyses for a
+    subject.
+
+    Postprocessing pipelines take the results of analyses and run
+    additional steps on them, and unlike a normal analysis they are
+    not immutable, and can be re-run each time a new analyis is
+    completed which satisfies the conditions of the postprocessing
+    job.
 
-        self.category = category if category else production.category
-        self.logger = logger
-        self.meta = self.production.meta["postprocessing"][self.name.lower()]
-
-
-class PESummaryPipeline(PostPipeline):
-    """
-    A postprocessing pipeline add-in using PESummary.
     """
 
-    name = "PESummary"
+    style = "multiplex"
 
-    def submit_dag(self, dryrun=False):
+    def __init__(self, subject, **kwargs):
         """
-        Run PESummary on the results of this job.
+        Post processing pipeline factory class.
+
         """
 
-        psds = {ifo: os.path.abspath(psd) for ifo, psd in self.production.psds.items()}
+        if "ledger" in kwargs:
+            self.ledger = kwargs["ledger"]
+        else:
+            self.ledger = None
 
-        calibration = [
-            os.path.abspath(os.path.join(self.production.repository.directory, cal))
-            if not cal[0] == "/"
-            else cal
-            for cal in self.production.meta["data"]["calibration"].values()
-        ]
-        configfile = self.production.event.repository.find_prods(
-            self.production.name, self.category
-        )[0]
-        command = [
-            "--webdir",
-            os.path.join(
-                config.get("project", "root"),
-                config.get("general", "webroot"),
-                self.production.event.name,
-                self.production.name,
-                "pesummary",
-            ),
-            "--labels",
-            self.production.name,
-            "--gw",
-            "--approximant",
-            self.production.meta["waveform"]["approximant"],
-            "--f_low",
-            str(min(self.production.meta["quality"]["minimum frequency"].values())),
-            "--f_ref",
-            str(self.production.meta["waveform"]["reference frequency"]),
-        ]
+        self.subject = subject
+        # self.category = category if category else production.category
+        self.logger = logger
+        # self.meta = self.production.meta["postprocessing"][self.name.lower()]
+        self.meta = kwargs
 
-        if "cosmology" in self.meta:
-            command += [
-                "--cosmology",
-                self.meta["cosmology"],
-            ]
-        if "redshift" in self.meta:
-            command += ["--redshift_method", self.meta["redshift"]]
-        if "skymap samples" in self.meta:
-            command += [
-                "--nsamples_for_skymap",
-                str(
-                    self.meta["skymap samples"]
-                ),  # config.get('pesummary', 'skymap_samples'),
-            ]
-
-        if "evolve spins" in self.meta:
-            if "forwards" in self.meta["evolve spins"]:
-                command += ["--evolve_spins_fowards", "True"]
-            if "backwards" in self.meta["evolve spins"]:
-                command += ["--evolve_spins_backwards", "precession_averaged"]
-
-        if "nrsur" in self.production.meta["waveform"]["approximant"].lower():
-            command += ["--NRSur_fits"]
-
-        if "multiprocess" in self.meta:
-            command += ["--multi_process", str(self.meta["multiprocess"])]
-
-        if "regenerate" in self.meta:
-            command += ["--regenerate", " ".join(self.meta["regenerate posteriors"])]
-
-        # Config file
-        command += [
-            "--config",
-            os.path.join(
-                self.production.event.repository.directory, self.category, configfile
-            ),
-        ]
-        # Samples
-        command += ["--samples"]
-        command += self.production.pipeline.samples(absolute=True)
-        # Calibration information
-        command += ["--calibration"]
-        command += calibration
-        # PSDs
-        command += ["--psd"]
-        for key, value in psds.items():
-            command += [f"{key}:{value}"]
-
-        with utils.set_directory(self.production.rundir):
-            with open(f"{self.production.name}_pesummary.sh", "w") as bash_file:
-                bash_file.write(
-                    f"{config.get('pesummary', 'executable')} " + " ".join(command)
-                )
+        self.outputs = os.path.join(
+            config.get("project", "root"),
+            config.get("general", "webroot"),
+            self.subject.name,
+        )
+        if self.style == "simplex":
+            self.outputs = os.path.join(self.outputs, self.analyses[0].name)
 
-        self.logger.info(
-            f"PE summary command: {config.get('pesummary', 'executable')} {' '.join(command)}",
-            production=self.production,
-            channels=["file"],
+    def __repr__(self):
+        output = ""
+        output += "-------------------------------------------------------" + "\n"
+        output += "Asimov postprocessing pipeline" + "\n"
+        output += f"{self.name}" + "\n"
+        output += (
+            f"""Currently contains: {(chr(10)+"                    ").join(self.current_list)}"""
+            + "\n"
+        )
+        output += (
+            """Designed to contain: """
+            + f"""{(chr(10)+"                     ").join([analysis.name for analysis in self.analyses])}"""
+            + "\n"
         )
+        output += f"""Is fresh?: {self.fresh}""" + "\n"
+        output += "-------------------------------------------------------" + "\n"
+        return output
+
+    def _process_analyses(self):
+        """
+        Process the analysis list for this production.
 
-        if dryrun:
-            print("PESUMMARY COMMAND")
-            print("-----------------")
-            print(command)
-
-        submit_description = {
-            "executable": config.get("pesummary", "executable"),
-            "arguments": " ".join(command),
-            "accounting_group": self.meta["accounting group"],
-            "output": f"{self.production.rundir}/pesummary.out",
-            "error": f"{self.production.rundir}/pesummary.err",
-            "log": f"{self.production.rundir}/pesummary.log",
-            "request_cpus": self.meta["multiprocess"],
-            "getenv": "true",
-            "batch_name": f"PESummary/{self.production.event.name}/{self.production.name}",
-            "request_memory": "8192MB",
-            "should_transfer_files": "YES",
-            "request_disk": "8192MB",
-        }
-
-        if dryrun:
-            print("SUBMIT DESCRIPTION")
-            print("------------------")
-            print(submit_description)
+        The dependencies can be provided either as the name of an analysis,
+        or a query against the analysis's attributes.
 
-        if not dryrun:
-            hostname_job = htcondor.Submit(submit_description)
+        Parameters
+        ----------
+        needs : list
+           A list of all the analyses which this should be applied to
 
+        Returns
+        -------
+        list
+           A list of all the analysis specifiations processed for evaluation.
+        """
+        all_requirements = []
+        post_requirements = []
+        for need in self.meta["analyses"]:
             try:
-                # There should really be a specified submit node, and if there is, use it.
-                schedulers = htcondor.Collector().locate(
-                    htcondor.DaemonTypes.Schedd, config.get("condor", "scheduler")
+                requirement = need.split(":")
+                requirement = [requirement[0].split("."), requirement[1]]
+            except IndexError:
+                requirement = [["name"], need]
+            if requirement[0][0] == "postprocessing" and requirement[0][1] == "name":
+                post_requirements.append(requirement)
+            else:
+                all_requirements.append(requirement)
+
+        analyses = []
+        post = []
+        if self.meta["analyses"]:
+            matches = set(self.subject.analyses)
+            for attribute, match in all_requirements:
+                filtered_analyses = list(
+                    filter(
+                        lambda x: x.matches_filter(attribute, match),
+                        self.subject.analyses,
+                    )
                 )
-                schedd = htcondor.Schedd(schedulers)
-            except:  # NoQA
-                # If you can't find a specified scheduler, use the first one you find
-                schedd = htcondor.Schedd()
-            with schedd.transaction() as txn:
-                cluster_id = hostname_job.queue(txn)
+                matches = set.intersection(matches, set(filtered_analyses))
+                for analysis in matches:
+                    analyses.append(analysis)
+
+            matches = set(self.ledger.postprocessing)
+            for attribute, match in post_requirements:
+                filtered_posts = list(
+                    filter(lambda x: x == match, self.ledger.postprocessing.keys())
+                )
+                matches = set.intersection(
+                    self.ledger.postprocessing, set(filtered_posts)
+                )
+                for analysis in matches:
+                    post.append(analysis)
+        return analyses
+
+    @property
+    def current_list(self):
+        """
+        Return the list of analyses which are included in the current results.
+        """
+        if "current list" in self.meta:
+            return self.meta["current list"]
+        else:
+            return []
+
+    @current_list.setter
+    def current_list(self, data):
+        """
+        Return the list of analyses which are included in the current results.
+        """
+        self.meta["current list"] = [analysis.name for analysis in data]
+        if self.ledger:
+            self.ledger.save()
+
+    @property
+    def fresh(self):
+        """
+        Check if the post-processing job is fresh.
+
+        Tries to work out if any of the samples which should have been
+        included for post-processing are newer than the most recent
+        files produced by this job.  If they are all older then the
+        results are fresh, otherwise they are stale, and the
+        post-processing will need to run again.
+
+        Returns
+        -------
+        bool
+           If the job is fresh True is returned.
+           Otherwise False.
+        """
+        # Check if there are any finished analyses
+        finished = []
+        for analysis in self.analyses:
+            if analysis.finished:
+                finished.append(analysis)
+        if len(finished) > 0:
+            if not all([os.path.exists(result) for result in self.results().values()]):
+                return False
+            try:
+                for analysis in self.analyses:
+                    self._check_ages(
+                        analysis.pipeline.samples(),
+                        self.results().values(),
+                    )
+            except AssertionError:
+                return False
+            return True
+        else:
+            return True
 
+    def _check_ages(self, listA, listB):
+        """
+        Check that the ages of all the samples from listA are younger than all the files from listB.
+        """
+
+        for sample in listA:
+            for comparison in listB:
+                assert os.stat(sample).st_mtime < os.stat(comparison).st_mtime
+
+    @property
+    def analyses(self):
+        """
+        Return a list of productions which this pipeline should apply to.
+
+        Post-processing pipelines can either apply to individual
+        analyses, or to a subset of all the analyses on a given
+        subject.  This property returns a list of all the productions
+        which this pipeline will be applied to.
+
+        Returns
+        -------
+        list
+           A list of all analyses which the pipeline will be applied to.
+        """
+        return self._process_analyses()
+
+    def run(self, dryrun=False):
+        """
+        Run all of the steps required to build and submit this pipeline.
+        """
+        cluster = self.submit_dag(dryrun=dryrun)
+        self.current_list = [
+            analysis for analysis in self.analyses if analysis.finished
+        ]
+        self.meta["job id"] = cluster
+        self.meta["status"] = "running"
+        if self.ledger:
+            self.ledger.save()
+
+    def to_dict(self):
+        """
+        Convert this pipeline into a dictionary.
+        """
+        output = {}
+        output.update(deepcopy(self.meta))
+        output.pop("ledger")
+        output["pipeline"] = self.name
+        return output
+
+    @property
+    def status(self):
+        if "status" in self.meta:
+            return self.meta["status"]
         else:
-            cluster_id = 0
+            return None
 
-        return cluster_id
+    @property
+    def job_id(self):
+        if "job id" in self.meta:
+            return self.meta["job id"]
+        else:
+            return None
```

## asimov/utils.py

```diff
@@ -38,14 +38,16 @@
     """
     if time < 1190000000:
         dir = "/home/cal/public_html/uncertainty/O2C02"
         virgo = "/home/carl-johan.haster/projects/O2/C02_reruns/V_calibrationUncertaintyEnvelope_magnitude5p1percent_phase40mraddeg20microsecond.txt"  # NoQA
     elif time < 1290000000:
         dir = "/home/cal/public_html/uncertainty/O3C01"
         virgo = "/home/cbc/pe/O3/calibrationenvelopes/Virgo/V_O3a_calibrationUncertaintyEnvelope_magnitude5percent_phase35milliradians10microseconds.txt"  # NoQA
+    elif time > 1268306607:
+        raise ValueError("This method cannot be used to add calibration envelope data beyond O3.")
     data_llo = glob.glob(f"{dir}/L1/*LLO*FinalResults.txt")
     times_llo = {
         int(datum.split("GPSTime_")[1].split("_C0")[0]): datum for datum in data_llo
     }
 
     data_lho = glob.glob(f"{dir}/H1/*LHO*FinalResults.txt")
     times_lho = {
@@ -58,18 +60,42 @@
     return {
         "H1": times_lho[keys_lho[np.argmin(np.abs(keys_lho - time))]],
         "L1": times_llo[keys_llo[np.argmin(np.abs(keys_llo - time))]],
         "V1": virgo,
     }
 
 
+
 def update(d, u, inplace=True):
     """Recursively update a dictionary."""
     if not inplace:
         d = deepcopy(d)
         u = deepcopy(u)
     for k, v in u.items():
         if isinstance(v, collections.abc.Mapping):
             d[k] = update(d.get(k, {}), v)
         else:
             d[k] = v
     return d
+
+# The following function adapted from https://stackoverflow.com/a/69908295
+def diff_dict(d1, d2):
+    d1_keys = set(d1.keys())
+    d2_keys = set(d2.keys())
+    shared_keys = d1_keys.intersection(d2_keys)
+    shared_deltas = {o: (d1[o], d2[o]) for o in shared_keys if d1[o] != d2[o]}
+    added_keys = d2_keys - d1_keys
+    added_deltas = {o: (None, d2[o]) for o in added_keys}
+    deltas = {**shared_deltas, **added_deltas}
+    return parse_deltas(deltas)
+
+# The following function adapted from https://stackoverflow.com/a/69908295
+def parse_deltas(deltas: dict):
+    res = {}
+    for k, v in deltas.items():
+        if isinstance(v[0], dict):
+            tmp = diff_dict(v[0], v[1])
+            if tmp:
+                res[k] = tmp
+        else:
+            res[k] = v[1]
+    return res
```

## asimov/cli/application.py

```diff
@@ -7,14 +7,15 @@
 import requests
 import yaml
 
 from asimov import LOGGER_LEVEL, logger
 import asimov.event
 from asimov.analysis import ProjectAnalysis
 from asimov import current_ledger as ledger
+from asimov.ledger import Ledger
 from asimov.utils import update
 
 import sys
 
 if sys.version_info < (3, 10):
     from importlib_metadata import entry_points
 else:
@@ -83,26 +84,75 @@
                 click.echo(
                     click.style("", fg="red")
                     + f" Could not apply {production.name} to {event_o.name} as "
                     + "an analysis already exists with this name"
                 )
                 logger.exception(e)
 
-        elif document['kind'].lower() == "projectanalysis":
+        elif document["kind"].lower() == "postprocessing":
+            # Handle a project analysis
+            logger.info("Found a postprocessing description")
+            document.pop("kind")
+            if event:
+                event_s = event
+
+            if event:
+                try:
+                    event_o = ledger.get_event(event_s)[0]
+                    level = event_o
+                except KeyError as e:
+                    click.echo(
+                        click.style("", fg="red")
+                        + f" Could not apply postprocessing, couldn't find the event {event}"
+                    )
+                    logger.exception(e)
+            else:
+                level = ledger
+            try:
+                if document["name"] in level.data["postprocessing"]:
+                    click.echo(
+                        click.style("", fg="red")
+                        + f" Could not apply postprocessing, as {document['name']} is already in the ledger."
+                    )
+                    logger.error(
+                        f" Could not apply postprocessing, as {document['name']} is already in the ledger."
+                    )
+                else:
+                    if isinstance(level, asimov.event.Event):
+                        level.meta["postprocessing"][document["name"]] = document
+                    elif isinstance(level, Ledger):
+                        level.data["postprocessing"][document["name"]] = document
+                        level.name = "the project"
+                    ledger.save()
+                    click.echo(
+                        click.style("", fg="green")
+                        + f" Successfully added {document['name']} to {level.name}."
+                    )
+                    logger.info(f"Added {document['name']}")
+            except ValueError as e:
+                click.echo(
+                    click.style("", fg="red")
+                    + f" Could not apply {document['name']} to project as "
+                    + "a post-process already exists with this name"
+                )
+                logger.exception(e)
+
+        elif document["kind"].lower() == "projectanalysis":
             # Handle a project analysis
             logger.info("Found a project analysis")
             document.pop("kind")
             analysis = ProjectAnalysis.from_dict(document, ledger=ledger)
 
             try:
                 ledger.add_analysis(analysis)
                 click.echo(
                     click.style("", fg="green")
                     + f" Successfully added {analysis.name} to this project."
                 )
+                ledger.save()
                 logger.info(f"Added {analysis.name}")
             except ValueError as e:
                 click.echo(
                     click.style("", fg="red")
                     + f" Could not apply {analysis.name} to project as "
                     + "an analysis already exists with this name"
                 )
```

## asimov/cli/configuration.py

```diff
@@ -1,9 +1,9 @@
 import click
-
+import os
 from asimov import config
 
 
 @click.group()
 def configuration():
     """Group for all of the configuration-related command line stuff."""
     pass
@@ -32,9 +32,9 @@
 @click.argument("kwargs", nargs=-1, type=click.UNPROCESSED)
 def update(kwargs):
     """Update a configuration setting."""
     key, value = kwargs
     section, key = key.split("/")
     section = section.replace("-", "")
     config.set(section, key, value)
-    with open("asimov.conf", "w") as config_file:
+    with open(os.path.join(".asimov", "asimov.conf"), "w") as config_file:
         config.write(config_file)
```

## asimov/cli/event.py

```diff
@@ -277,15 +277,18 @@
     event = ledger.get_event(event)[0]
     try:
         event._check_calibration()
     except DescriptionException:
         print(event.name)
         time = event.meta["event time"]
         if not calibration[0]:
-            calibrations = find_calibrations(time)
+            try:
+                calibrations = find_calibrations(time)
+            except ValueError:
+                calibrations = {}
         else:
             calibrations = {}
             for cal in calibration:
                 calibrations[cal.split(":")[0]] = cal.split(":")[1]
         print(calibrations)
         update(event.meta["data"]["calibration"], calibrations)
         ledger.update_event(event)
```

## asimov/cli/manage.py

```diff
@@ -181,15 +181,16 @@
                         "The pipeline failed to build a DAG file.",
                     )
                     logger.exception(e)
                     click.echo(
                         click.style("", fg="red")
                         + f" Unable to submit {production.name}"
                     )
-                except ValueError:
+                except ValueError as e:
+                    print("ERROR", e)
                     logger.info("Unable to submit an unbuilt production")
                     click.echo(
                         click.style("", fg="red")
                         + f" Unable to submit {production.name} as it hasn't been built yet."
                     )
                     click.echo("Try running `asimov manage build` first.")
                 try:
```

## asimov/cli/monitor.py

```diff
@@ -1,45 +1,40 @@
 import shutil
 import configparser
-import os
 import sys
 import click
+from copy import deepcopy
+import logging
 
 from asimov import condor, config, logger, LOGGER_LEVEL
 from asimov import current_ledger as ledger
 from asimov.cli import ACTIVE_STATES, manage, report
 
 logger = logger.getChild("cli").getChild("monitor")
 logger.setLevel(LOGGER_LEVEL)
 
-if sys.version_info < (3, 10):
-    from importlib_metadata import entry_points
-else:
-    from importlib.metadata import entry_points
-
 
 @click.option("--dry-run", "-n", "dry_run", is_flag=True)
 @click.command()
 def start(dry_run):
     """Set up a cron job on condor to monitor the project."""
 
     try:
         minute_expression = config.get("condor", "cron_minute")
     except (configparser.NoOptionError, configparser.NoSectionError):
         minute_expression = "*/15"
 
     submit_description = {
         "executable": shutil.which("asimov"),
         "arguments": "monitor --chain",
-        "accounting_group": config.get("pipelines", "accounting"),
-        "output": os.path.join(".asimov", "asimov_cron.out"),
+        "accounting_group": config.get("asimov start", "accounting"),
+        "output": "asimov_cron.out",
         "on_exit_remove": "false",
-        "universe": "local",
-        "error": os.path.join(".asimov", "asimov_cron.err"),
-        "log": os.path.join(".asimov", "asimov_cron.log"),
+        "error": "asimov_cron.err",
+        "log": "asimov_cron.log",
         "request_cpus": "1",
         "cron_minute": minute_expression,
         "getenv": "true",
         "batch_name": f"asimov/monitor/{ledger.data['project']['name']}",
         "request_memory": "8192MB",
         "request_disk": "8192MB",
     }
@@ -108,14 +103,15 @@
         finish = 0
         click.secho(f"{event.name}", bold=True)
         on_deck = [
             production
             for production in event.productions
             if production.status.lower() in ACTIVE_STATES
         ]
+
         for production in on_deck:
 
             logger.debug(f"Available analyses: {event}/{production.name}")
 
             click.echo(
                 "\t- "
                 + click.style(f"{production.name}", bold=True)
@@ -138,23 +134,23 @@
                     click.secho("  \tStopped", fg="red")
                 else:
                     click.echo("\t\t{production.name} --> stopped")
                 continue
 
             # Get the condor jobs
             try:
-                if "job id" in production.meta:
+                if "job id" in production.meta["scheduler"]:
                     if not dry_run:
-                        if production.meta["job id"] in job_list.jobs:
-                            job = job_list.jobs[production.meta["job id"]]
+                        if production.job_id in job_list.jobs:
+                            job = job_list.jobs[production.job_id]
                         else:
                             job = None
                     else:
                         logger.debug(
-                            f"Running analysis: {event}/{production.name}, cluster {production.meta['job id']}"
+                            f"Running analysis: {event}/{production.name}, cluster {production.job_id}"
                         )
                         click.echo("\t\tRunning under condor")
                 else:
                     raise ValueError  # Pass to the exception handler
 
                 if not dry_run:
 
@@ -162,43 +158,31 @@
                         job.status.lower() == "running"
                         and production.status == "processing"
                     ):
                         click.echo(
                             "  \t  "
                             + click.style("", "green")
                             + f" Postprocessing for {production.name} is running"
-                            + f" (condor id: {production.meta['job id']})"
+                            + f" (condor id: {production.job_id})"
                         )
 
-                    elif (
-                        job.status.lower() == "running"
-                        and production.status == "processing"
-                    ):
-                        click.echo(
-                            "  \t  "
-                            + click.style("", "green")
-                            + f" {production.name} is postprocessing (condor id: {production.meta['job id']})"
-                        )
                         production.meta["postprocessing"]["status"] = "running"
 
-                    elif job.status.lower() == "running":
+                    elif job.status.lower() == "idle":
                         click.echo(
                             "  \t  "
                             + click.style("", "green")
-                            + f" {production.name} is running (condor id: {production.meta['job id']})"
+                            + f" {production.name} is in the queue (condor id: {production.job_id})"
                         )
-                        if "profiling" not in production.meta:
-                            production.meta["profiling"] = {}
-                        production.status = "running"
-
+                        
                     elif job.status.lower() == "running":
                         click.echo(
                             "  \t  "
                             + click.style("", "green")
-                            + f" {production.name} is running (condor id: {production.meta['job id']})"
+                            + f" {production.name} is running (condor id: {production.job_id})"
                         )
                         if "profiling" not in production.meta:
                             production.meta["profiling"] = {}
                         production.status = "running"
 
                     elif job.status.lower() == "completed":
                         pipe.after_completion()
@@ -210,15 +194,15 @@
                         job_list.refresh()
 
                     elif job.status.lower() == "held":
                         click.echo(
                             "  \t  "
                             + click.style("", "yellow")
                             + f" {production.name} is held on the scheduler"
-                            + f" (condor id: {production.meta['job id']})"
+                            + f" (condor id: {production.job_id})"
                         )
                         production.status = "stuck"
                         stuck += 1
                     else:
                         running += 1
 
             except (ValueError, AttributeError):
@@ -256,17 +240,17 @@
                             except ValueError as e:
                                 click.echo(e)
                         else:
                             click.echo(
                                 "  \t  "
                                 + click.style("", "green")
                                 + f" {production.name} has finished and post-processing"
-                                + f" is stuck ({production.meta['job id']})"
+                                + f" is stuck ({production.job_id})"
                             )
-                            production.meta["postprocessing"]["status"] = "stuck"
+                            # production.meta["postprocessing"]["status"] = "stuck"
                     elif (
                         pipe.detect_completion()
                         and production.status.lower() == "processing"
                     ):
                         click.echo(
                             "  \t  "
                             + click.style("", "green")
@@ -278,24 +262,24 @@
                     ):
                         # The job has been completed, collect its assets
                         if "profiling" not in production.meta:
                             production.meta["profiling"] = {}
                         try:
                             config.get("condor", "scheduler")
                             production.meta["profiling"] = condor.collect_history(
-                                production.meta["job id"]
+                                production.job_id
                             )
-                            production.meta["job id"] = None
+                            production.job_id = None
                         except (
                             configparser.NoOptionError,
                             configparser.NoSectionError,
                         ):
                             logger.warning(
-                                "Could not collect condor profiling data as no " +
-                                "scheduler was specified in the config file."
+                                "Could not collect condor profiling data as no "
+                                + "scheduler was specified in the config file."
                             )
                         except ValueError as e:
                             logger.error("Could not collect condor profiling data.")
                             logger.exception(e)
                             pass
 
                         finish += 1
@@ -340,18 +324,54 @@
         }
         others = all_productions - set(event.get_all_latest()) - complete
         if len(others) > 0:
             click.echo(
                 "The event also has these analyses which are waiting on other analyses to complete:"
             )
             for production in others:
-                needs = ", ".join(production.meta["needs"])
+                needs = ", ".join(production._needs)
                 click.echo(f"\t{production.name} which needs {needs}")
 
         # Post-monitor hooks
-        discovered_hooks = entry_points(group="asimov.hooks.postmonitor")
-        for hook in discovered_hooks:
-            if hook.name in list(ledger.data["hooks"]["postmonitor"].keys()):
-                hook.load()(ledger).run()
+        if "hooks" in ledger.data:
+            if "postmonitor" in ledger.data["hooks"]:
+                discovered_hooks = entry_points(group="asimov.hooks.postmonitor")
+                for hook in discovered_hooks:
+                    if hook.name in list(ledger.data["hooks"]["postmonitor"].keys()):
+                        try:
+                            hook.load()(deepcopy(ledger)).run()
+                        except Exception:
+                            pass
+
+        if "postprocessing" in ledger.data:
+            if len(ledger.data["postprocessing"]) > 0:
+                click.echo(
+                    "The following post-processing jobs are defined on this subject"
+                )
+            for postprocess in ledger.postprocessing(event):
+                # If the pipeline's not fresh and not currently running, then run it.
+                pipe = postprocess.pipeline
+                if (
+                    not postprocess.pipeline.fresh
+                    and postprocess.pipeline.job_id not in job_list.jobs
+                    and not postprocess.pipeline.status == "running"
+                ):
+                    postprocess.pipeline.run()
+                    ledger.data["postprocessing"][postprocess.name] = postprocess.to_dict()
+                    ledger.save()
+                elif (
+                    pipe.fresh
+                    and pipe.job_id not in job_list.jobs
+                    and pipe.status == "running"
+                ):
+                    postprocess.status = "finished"
+                    ledger.save()
+                elif not pipe.fresh and pipe.job_id not in job_list.jobs:
+                    postprocess.pipeline.run()
+                    postprocess.status = "running"
+                    ledger.save()
+                click.echo(
+                    f"""\t{postprocess.name} ({pipe.name}) - {"fresh" if pipe.fresh else "stale"} - {pipe.status}"""
+                )
 
         if chain:
             ctx.invoke(report.html)
```

## asimov/cli/project.py

```diff
@@ -70,31 +70,39 @@
     config.set("logging", "directory", logs)
 
     # Make the results store
     storage.Store.create(root=results, name=f"{project_name} storage")
     config.set("storage", "directory", results)
 
     # Make the ledger and operative files
-    pathlib.Path('.asimov').mkdir(parents=True, exist_ok=True)
+    pathlib.Path(".asimov").mkdir(parents=True, exist_ok=True)
     config.set("ledger", "engine", "yamlfile")
     config.set("ledger", "location", os.path.join(".asimov", "ledger.yml"))
 
     # Set the default environment
     python_loc = shutil.which("python").split("/")[:-2]
     config.set("pipelines", "environment", os.path.join("/", *python_loc))
+    config.set("rift", "environment", os.path.join("/", *python_loc))
+
+    try:
+        config.set("pesummary", "executable", shutil.which("summarypages"))
+    except Exception:
+        pass
 
     # Set the default condor user
     if not user:
         config.set("condor", "user", getpass.getuser())
     else:
         config.set("condor", "user", user)
 
-    Ledger.create(engine="yamlfile",
-                  name=project_name,
-                  location=os.path.join(".asimov", "ledger.yml"))
+    Ledger.create(
+        engine="yamlfile",
+        name=project_name,
+        location=os.path.join(".asimov", "ledger.yml"),
+    )
 
     with open(os.path.join(".asimov", "asimov.conf"), "w") as config_file:
         config.write(config_file)
 
 
 @click.command()
 @click.argument("name")
@@ -115,22 +123,15 @@
     help="The location to store cloned git repositories.",
 )
 @click.option(
     "--results",
     default="results",
     help="The location where the results store should be created.",
 )
-@click.option(
-    "--user",
-    default=None,
-    help="The user account to be used for accounting purposes. Defaults to the current user if not set.",
-)
-def init(
-    name, root, working="working", checkouts="checkouts", results="results", user=None
-):
+def init(name, root, working="working", checkouts="checkouts", results="results"):
     """
     Roll-out a new project.
     """
     make_project(name, root, working=working, checkouts=checkouts, results=results)
     click.echo(click.style("", fg="green") + " New project created successfully!")
     logger.info(f"A new project was created in {os.getcwd()}")
 
@@ -179,19 +180,19 @@
         os.path.join(location, config.get("storage", "results_store")), results
     )
     config.set("storage", "results_store", results)
 
     # Make the ledger
     if config.get("ledger", "engine") == "yamlfile":
         shutil.copyfile(
-            os.path.join(location, config.get("ledger", "location")), ".asimov/ledger.yml"
+            os.path.join(location, config.get("ledger", "location")), os.path.join(".asimov", "ledger.yml")
         )
     elif config.get("ledger", "engine") == "gitlab":
         raise NotImplementedError(
             "The gitlab interface has been removed from this version of asimov."
         )
 
     config.set("ledger", "engine", "yamlfile")
-    config.set("ledger", "location", ".asimov/ledger.yml")
+    config.set("ledger", "location", os.path.join(".asimov", "ledger.yml"))
 
-    with open("asimov.conf", "w") as config_file:
+    with open(os.path.join(".asimov", "asimov.conf"), "w") as config_file:
         config.write(config_file)
```

## asimov/cli/report.py

```diff
@@ -1,12 +1,14 @@
 """
 Reporting functions
 """
 from datetime import datetime
 
+import os
+
 import click
 import pytz
 import yaml
 from pkg_resources import resource_filename
 
 import otter
 import otter.bootstrap as bt
@@ -38,16 +40,16 @@
     if not webdir:
         webdir = config.get("general", "webroot")
 
     report = otter.Otter(
         f"{webdir}/index.html",
         author="Asimov",
         title="Asimov project report",
-        theme_location=resource_filename(__name__, "theme/"),
-        config_file="asimov.conf",
+        theme_location=resource_filename("asimov.cli", "report-theme"),
+        config_file=os.path.join(".asimov", "asimov.conf"),
     )
     with report:
 
         style = """
 <style>
         body {
         background-color: #f2f2f2;
```

## asimov/configs/bayeswave.ini

```diff
@@ -44,15 +44,17 @@
 cache-files={ {% for ifo in ifos %}'{{ifo}}':'{{data['data files'][ifo]}}',{% endfor %} }
 {%- endif %}
 
 [bayeswave_options]
 ; command line options for BayesWave.  See BayesWave --help
 Dmax=100
 updatedGeocenterPSD=
-Niter = {{ likelihood['iterations'] | default: 2000000}}
+Niter = {{ likelihood['iterations'] | default: 100000}}
+Nchain = {{ likelihood['chains'] | default: 8 }} 
+Nthreads = {{ likelihood['threads'] | default: 4 }}
 cleanOnly=
 bayesLine =
 
 [bayeswave_post_options]
 ; command line options for BayesWavePost.  See BayesWavePost --help
 0noise=
 lite =
```

## asimov/configs/bilby.ini

```diff
@@ -24,14 +24,17 @@
 ## Calibration arguments
 ################################################################################
 calibration-model=CubicSpline
 spline-calibration-envelope-dict={ {% for ifo in ifos %}{{ifo}}:{{data['calibration'][ifo]}},{% endfor %} }
 spline-calibration-nodes=10
 spline-calibration-amplitude-uncertainty-dict=None
 spline-calibration-phase-uncertainty-dict=None
+{%- if priors contains 'calibration' %}
+calibration-prior-boundary={{ priors['calibration']['boundary'] | default: reflective }}
+{%- endif %}
 {%- endif %}
 {%- endif %}
 {%- endif %}
 
 ################################################################################
 ## Data generation arguments
 ################################################################################
@@ -48,27 +51,27 @@
 data-format=None
 channel-dict={ {% for ifo in ifos %}{{data['channels'][ifo]}},{% endfor %} }
 
 ################################################################################
 ## Detector arguments
 ################################################################################
 
-coherence-test=False
+coherence-test={{ likelihood['coherence test'] | default: "False" }}
 detectors={{ ifos }}
 duration={{ data['segment length'] }}
-generation-seed=None
+generation-seed=42
 psd-dict={ {% for ifo in ifos %}{{ifo}}:{{production.psds[ifo]}},{% endfor %} }
 psd-fractional-overlap=0.5
 post-trigger-duration=2.0
 sampling-frequency={{ likelihood['sample rate'] | round }}
 psd-length={{ likelihood['psd length'] | round }}
 psd-maximum-duration=1024
 psd-method=median
 psd-start-time=None
-minimum-frequency={ {% for ifo in ifos %}{{ifo}}:{{quality['minimum frequency'][ifo]}},{% endfor %} }
+minimum-frequency={ {% for ifo in ifos %}{{ifo}}:{{quality['minimum frequency'][ifo]}},{% endfor %}{% if likelihood contains 'start frequency'%} waveform: {{ likelihood['start frequency'] }} {% endif %} }
 maximum-frequency={ {% for ifo in ifos %}{{ifo}}:{{quality['maximum frequency'][ifo]}},{% endfor %} }
 zero-noise=False
 tukey-roll-off=0.4
 resampling-method=lal
 
 ################################################################################
 ## Injection arguments
@@ -88,47 +91,58 @@
 accounting_user = {{ config['condor']['user'] }}
 label={{ production.name }}
 local=False
 local-generation={{ scheduler['local generation'] | default: False }}
 local-plot=False
 outdir={{ production.rundir }}
 periodic-restart-time={{ scheduler['periodic restart time'] | default: 28800 }}
-request-memory={{ scheduler['request memory'] | default: 8.0}}
-request-memory-generation=None
-request-cpus={{ scheduler['request cpus'] | default: 16}}
+request-memory={{ scheduler['request memory'] | default: 4.0}}
+request-memory-generation={{ scheduler['request generation memory'] | default: None }}
+request-cpus={{ scheduler['request cpus'] | default: 1 }}
+request-disk={{ scheduler['request disk'] | default: 1 }}
 scheduler={{ scheduler['type'] | default: "condor" }}
 scheduler-args=None
 scheduler-module=None
 scheduler-env=None
 transfer-files={% if scheduler['osg'] %}True{% else %}{{ scheduler['transfer files'] | default: False }}{% endif %}
 log-directory=None
 online-pe=False
 osg={{ scheduler['osg'] | default: False }}
 desired-sites={{ scheduler['desired sites'] | default: None }}
 
 ################################################################################
 ## Likelihood arguments
 ################################################################################
 
-distance-marginalization={{ likelihood['marginalization']['distance'] | default: "False" }}
+distance-marginalization={{ likelihood['marginalization']['distance'] | default: "True" }}
 {% assign lookup_default = repo_dir+"/C01_offline/"+production.name+".TD.npz" -%}
 distance-marginalization-lookup-table={{ likelihood['marginalization']['distance lookup'] | default: lookup_default }}
 phase-marginalization={{ likelihood['marginalization']['phase'] | default: "False" }}
-time-marginalization={{ likelihood['marginalization']['time'] | default: "True" }}
+time-marginalization={{ likelihood['marginalization']['time'] | default: "False" }}
+calibration-marginalization={{ likelihood['marginalization']['calibration'] | default: "False" }}
 jitter-time=True
+time-reference={{ likelihood['time reference'] | default: "geocent" }}
 reference-frame={% if production.meta['interferometers'] contains "H1" %}H1{% endif %}{% if production.meta['interferometers'] contains "L1" %}L1{% endif %}{% if production.meta['interferometers'] contains "V1" %}V1{% endif %}
-time-reference=geocent
+
 likelihood-type={{ likelihood['type'] | default: "GravitationalWaveTransient" }}
 
-{% if likelihood contains "roq" %}
+{%- if likelihood contains "roq" %}
 roq-folder={{ likelihood['roq']['folder'] | default: "None" }}
 roq-weights={{ likelihood['roq']['weights'] | default: "None" }}
 roq-weight-format={{ likelihood['roq']['weight format'] | default: "None" }}
 roq-scale-factor={{ likelihood['roq']['scale'] | default: 1 }}
-{% endif %}
+roq-linear-matrix={{ likelihood['roq']['linear matrix'] }}
+roq-quadratic-matrix={{ likelihood['roq']['quadratic matrix'] }}
+{%- endif %}
+
+{%- if likelihood contains "relative binning" %}
+fiducial-parameters={{ likelihood['relative binning']['fiducial parameters'] | default: None }}
+update-fiducial-parameters={{ likelihood['relative binning']['update fiducial parameters'] | default: False }}
+epsilon={{ likelihood['relative binning']['epsilon'] | default: 0.025 }}
+{%- endif %}
 
 extra-likelihood-kwargs={{ likelihood['kwargs'] | default: "None" }}
 
 ################################################################################
 ## Output arguments
 ################################################################################
 
@@ -144,33 +158,39 @@
 webdir={{ config['general']['webroot'] }}/{{ production.event.name }}/{{ production.name }}
 summarypages-arguments=None
 result-format=hdf5
 final-result=True
 final-result-nsamples=20000
 
 ################################################################################
+## Executables
+################################################################################
+analysis-executable={{ config['pipelines']['environment'] }}/bin/bilby_pipe_analysis
+
+
+################################################################################
 ## Prior arguments
 ################################################################################
 
 default-prior = {{ priors['default'] | default: "BBHPriorDict" }}
 deltaT=0.2
 {% if production.meta contains "priors" %}
 prior-dict = {
 {%- if priors.keys() contains "geocentric time" %}
 {%- assign p = priors['geocentric time'] %}
-   geocent_time = {{p['type']}}(name="geocentric time", minimum={{p['minimum']}}, maximum={{p['maximum']}}, boundary={{p['boundary'] | default: None}}),
+   geocent_time = {{p['type']}}(name="geocent_time", minimum={{p['minimum']}}, maximum={{p['maximum']}}, boundary={{p['boundary'] | default: None}}),
 {% endif %}
 {%- if priors.keys() contains "chirp mass" %}{% assign p = priors['chirp mass'] %}{% else %}{% assign p = None %}{% endif %}
    chirp_mass = {{p['type'] | default: "bilby.gw.prior.UniformInComponentsChirpMass" }}(name='chirp_mass', minimum={{p['minimum'] | default: 1}}, maximum={{p['maximum'] | default: 100}}, unit='$M_\{\{\odot\}\}$'),
 {%- if priors.keys() contains "mass ratio" %}{% assign p = priors['mass ratio'] %}{% else %}{% assign p = None %}{% endif %}
    mass_ratio = {{p['type'] | default: "bilby.gw.prior.UniformInComponentsMassRatio" }}(name='mass_ratio', minimum={{p['minimum']}}, maximum={{p['maximum']}}),
 {%- if priors.keys() contains "mass 1" %}{% assign p = priors['mass 1'] %}{% else %}{% assign p = None %}{% endif %}
-   mass_1 = {{p['type'] | default: Constraint}}(name='mass_1', minimum={{p['minimum']}}, maximum={{p['maximum']}}),
+   mass_1 = {{p['type'] | default: Constraint}}(name='mass_1', minimum={{p['minimum'] | default: 1}}, maximum={{p['maximum'] | default: 1000}}),
 {%- if priors.keys() contains "mass 2" %}{% assign p = priors['mass 2'] %}{% else %}{% assign p = None %}{% endif %}
-   mass_2 = {{p['type'] | default: Constraint}}(name='mass_2', minimum={{p['minimum'] | default: 1 }}, maximum={{p['maximum'] | default: 100}}),
+   mass_2 = {{p['type'] | default: Constraint}}(name='mass_2', minimum={{p['minimum'] | default: 1 }}, maximum={{p['maximum'] | default: 1000}}),
 {%- if priors.keys() contains "spin 1" %}
 {%- assign p = priors['spin 1'] %}
 {%- else %}
 {%- assign p = None %}
 {% endif %}
    a_1 = {{ p['type'] | default: Uniform}}(name='a_1', minimum={{ p['minimum'] | default: 0}}, maximum={{ p['maximum'] | default: 0.99}}),
 {%- if priors.keys() contains "spin 2" %}
@@ -183,28 +203,35 @@
    tilt_1 = {{ p['type'] | default: Sine}}(name='tilt_1'),
 {%- if priors.keys() contains "tilt 2" %}{% assign p = priors['tilt 2'] %}{% else %}{% assign p = None %}{% endif %}
    tilt_2 = {{ p['type'] | default: Sine}}(name='tilt_2'),
 {%- if priors.keys() contains "phi 12" %}{% assign p = priors['phi 12'] %}{% else %}{% assign p = None %}{% endif %}
    phi_12 = {{ p['type'] | default: Uniform}}(name='phi_12', minimum={{ p['minimum'] | default: 0}}, maximum={{ p['maximum'] | default: "2 * np.pi"}}, boundary={{p['boundary'] | default: "'periodic'"}}),
 {%- if priors.keys() contains "phi jl" %}{% assign p = priors['phi jl'] %}{% else %}{% assign p = None %}{% endif %}
    phi_jl = {{ p['type'] | default: Uniform}}(name='phi_jl', minimum={{ p['minimum'] | default: 0}}, maximum={{ p['maximum'] | default: "2 * np.pi"}}, boundary={{p['boundary'] | default: "'periodic'"}}),
+{%- if priors.keys() contains "lambda 1" %}
+{%- assign p = priors['lambda 1'] %}
+   lambda_1 = Uniform(name='lambda_1', minimum={{ p['minimum'] | default: 0}}, maximum={{ p['maximum'] | default: 5000}}),
+{%- endif %} 
+{%- if priors.keys() contains "lambda 2" %}{% assign p = priors['lambda 2'] %}
+   lambda_2 = Uniform(name='lambda_2', minimum={{ p['minimum'] | default: 0}}, maximum={{ p['maximum'] | default: 5000}}),
+{%- endif %}
 {%- if priors.keys() contains "luminosity distance" %}
 {%- assign p = priors['luminosity distance'] %}
-   luminosity_distance =  {{ p['type'] | default: PowerLaw}}({% for key in p.keys() %}{% if key != "type" %}{{key | replace: " ", "_"}}={{p[key]}},{% endif %} {% endfor %} unit='Mpc'),
+   luminosity_distance =  {{ p['type'] | default: PowerLaw}}(name='luminosity_distance', {% for key in p.keys() %}{% if key != "type" %}{{key | replace: " ", "_"}}={{p[key]}},{% endif %} {% endfor %} unit='Mpc'),
 {%- else %}
 {%- assign p = None %}
-   luminosity_distance =  {{ p['type'] | default: PowerLaw}}(unit='Mpc'),
+   luminosity_distance =  {{ p['type'] | default: PowerLaw}}(name='luminosity_distance', unit='Mpc'),
 {%- endif %}
    dec = Cosine(name='dec'),
    ra = Uniform(name='ra', minimum=0, maximum=2 * np.pi, boundary='periodic'),
    theta_jn = Sine(name='theta_jn'),
    psi = Uniform(name='psi', minimum=0, maximum=np.pi, boundary='periodic'),
    phase = Uniform(name='phase', minimum=0, maximum=2 * np.pi, boundary='periodic')}
 {% endif %}
-enforce-signal-duration=True
+enforce-signal-duration={{ production.meta['waveform']['enforce signal duration'] | default: False }}
 
 ################################################################################
 ## Post processing arguments
 ################################################################################
 
 postprocessing-executable=None
 postprocessing-arguments=None
@@ -212,29 +239,29 @@
 single-postprocessing-arguments=None
 
 ################################################################################
 ## Sampler arguments
 ################################################################################
 
 sampler={{sampler['sampler'] | default: "dynesty" }}
-sampling-seed={{sampler['seed'] | default: None }}
-n-parallel={{ sampler['parallel jobs'] | default: 4 }}
-sampler-kwargs={{ sampler['sampler kwargs'] |  default: "{'nlive': 2000, 'sample': 'rwalk', 'walks': 100, 'nact': 50, 'check_point_delta_t':1800, 'check_point_plot':True}"  }} 
+sampling-seed={{sampler['seed'] | default: 1 }}
+n-parallel={{ sampler['parallel jobs'] | default: 2 }}
+sampler-kwargs={{ sampler['sampler kwargs'] |  default: "{'nlive': 1000, 'naccept': 60, 'check_point_plot': True, 'check_point_delta_t': 1800, 'print_method': 'interval-60', 'sample': 'acceptance-walk'}"  }} 
 
 ################################################################################
 ## Waveform arguments
 ################################################################################
 
-waveform-generator={{ production.meta['waveform']['generator'] | default: "bilby.gw.waveform_generator.WaveformGenerator" }}
+waveform-generator={{ production.meta['waveform']['generator'] | default: "bilby.gw.waveform_generator.LALCBCWaveformGenerator" }}
 reference-frequency={{ production.meta['waveform']['reference frequency'] }}
 waveform-approximant={{ production.meta['waveform']['approximant'] }}
 catch-waveform-errors=True
 pn-spin-order={{ production.meta['waveform']['pn spin order'] | default: -1 }}
 pn-tidal-order={{ production.meta['waveform']['pn tidal order'] | default: -1 }}
 pn-phase-order={{ production.meta['waveform']['pn phase order'] | default: -1 }}
-pn-amplitude-order={{ production.meta['waveform']['pn amplitude order'] | default: 1 }}
+pn-amplitude-order={{ production.meta['waveform']['pn amplitude order'] | default: 0 }}
 numerical-relativity-file={{ production.meta['waveform']['file'] | default: "None" }}
 waveform-arguments-dict={{ production.meta['waveform']['arguments'] | default: "None" }}
 mode-array={{ production.meta['waveform']['mode array'] | default: "None" }}
 frequency-domain-source-model={{ production.meta['likelihood']['frequency domain source model'] | default: "lal_binary_black_hole" }}
 conversion-function={{ production.meta['waveform']['conversion function'] | default: "None" }}
 generation-function={{ production.meta['waveform']['generation function'] | default: "None" }}
```

## asimov/configs/lalinference.ini

```diff
@@ -139,18 +139,19 @@
 [skyarea]
 maxpts=2000
 
 [resultspage]
 skyres=0.5
 deltaLogP = 7.5
 
+{%- if quality contains "statevector" %}
 [statevector]
 state-vector-channel={ {% for ifo in ifos %}{{ifo}}:{{quality['state vector'][ifo]}},{% endfor %} }
 bits=['Bit 0', 'Bit 1', 'Bit 2']
-
+{%- endif %}
 
 [ligo-skymap-from-samples]
 enable-multiresolution=
 
 [ligo-skymap-plot]
 annotate=
 contour= 50 90
```

## asimov/configs/rift.ini

```diff
@@ -52,31 +52,31 @@
 mpirun = %(lalsuite-install)s/bin/mpirun
 
 accounting_group={{ scheduler['accounting group'] }}
 accounting_group_user={{ config['condor']['user'] }}
 
 [datafind]
 url-type=file
-types = { {% for ifo in ifos %}{{ifo}}:{{data['frame types'][ifo]}},{% endfor %} }
+types = { {% for ifo in ifos %}"{{ifo}}":"{{data['frame types'][ifo]}}",{% endfor %} }
 
 [data]
-channels = { {% for ifo in ifos %}{{ifo}}:{{data['channels'][ifo]}},{% endfor %} }
+channels = { {% for ifo in ifos %}"{{ifo}}":"{{data['channels'][ifo]}}",{% endfor %} }
 
 [lalinference]
-flow = { {% for ifo in ifos %}{{ifo}}:{{quality['minimum frequency'][ifo]}},{% endfor %} }
-fhigh = { {% for ifo in ifos %}{{ifo}}:{{quality['maximum frequency'][ifo]}},{% endfor %} }
+flow = { {% for ifo in ifos %}"{{ifo}}":{{quality['minimum frequency'][ifo]}},{% endfor %} }
+fhigh = { {% for ifo in ifos %}"{{ifo}}":{{quality['maximum frequency'][ifo]}},{% endfor %} }
 
 [engine]
 
 fref={{ waveform['reference frequency'] }}
 
 fmin-template={{ likelihood['start frequency'] }}
 
 approx = {{ waveform['approximant'] }}
-amporder = {{ production.meta['priors']['amplitude order'] }}
+amporder = {{ production.meta['waveform']['pn amplitude order'] }}
 
 seglen = {{ data['segment length'] }}
 srate =  {{ likelihood['sample rate'] }}
 
 
 enable-spline-calibration =
 spcal-nodes = 10
@@ -189,15 +189,15 @@
 # 
 l-max={{ waveform['maximum mode'] | default: 2 }}
 
 #
 # Priors
 #
 # * distance prior if this argument is *not* set is dL^2
-ile-distance-prior="pseudo-cosmo"
+ile-distance-prior="pseudo_cosmo"
 
 # maximum runtime for ILE jobs. May need to be longer
 ile-runtime-max-minutes= {{ sampler['ile']['runtime max minues'] | default: 700 }}
 # Number of likelihood evaluations for each instance. Make this larger if your likelihoods are very fast, smaller if you need low latency
 ile-jobs-per-worker= {{ sampler['ile']['jobs per worker'] | default: 20 }}
 
 #
```

## asimov/pipelines/__init__.py

```diff
@@ -6,20 +6,24 @@
     from importlib.metadata import entry_points
 
 from asimov.pipelines.bayeswave import BayesWave
 from asimov.pipelines.bilby import Bilby
 from asimov.pipelines.lalinference import LALInference
 from asimov.pipelines.rift import Rift
 
+from asimov.pipelines.pesummary import PESummary, SummaryCombinePosteriors
+
 discovered_pipelines = entry_points(group="asimov.pipelines")
 
 
 known_pipelines = {
     "bayeswave": BayesWave,
     "bilby": Bilby,
     "rift": Rift,
     "lalinference": LALInference,
+    "pesummary": PESummary,
+    "summarycombineposteriors": SummaryCombinePosteriors,
 }
 
 
 for pipeline in discovered_pipelines:
     known_pipelines[pipeline.name] = pipeline.load()
```

## asimov/pipelines/bayeswave.py

```diff
@@ -131,17 +131,28 @@
 
         command = [
             pipe_cmd,
             # "-l", f"{gps_file}",
             f"--trigger-time={gps_time}",
         ]
 
+        if "copy frames" in self.production.meta["scheduler"]:
+            if self.production.meta["scheduler"]["copy frames"]:
+                command += ["--copy-frames"]
+        
         if "osg" in self.production.meta["scheduler"]:
             if self.production.meta["scheduler"]["osg"]:
-                command += ["--osg-deploy", "--transfer-files"]
+                command += ["--transfer-files"]
+
+                if "copy frames" not in self.production.meta['scheduler']:
+                    command += ["--osg-deploy"]
+                if "copy frames" in self.production.meta['scheduler']:
+                    if not self.production.meta["scheduler"]["copy frames"]:
+                        command += ["--osg-deploy"]
+                
         command += [
             "-r",
             self.production.rundir,
             ini,
         ]
 
         self.logger.info(" ".join(command))
@@ -444,15 +455,15 @@
     def supress_psd(self, ifo, fmin, fmax):
         """
         Suppress portions of a PSD.
         Author: Carl-Johan Haster - August 2020
         (Updated for asimov by Daniel Williams - November 2020
         """
         store = Store(root=config.get("storage", "directory"))
-        sample_rate = self.production.meta["quality"]["sample-rate"]
+        sample_rate = self.production.meta["likelihood"]["sample rate"]
         orig_PSD_file = np.genfromtxt(
             os.path.join(
                 self.production.event.repository.directory,
                 self.category,
                 "psds",
                 str(sample_rate),
                 f"{ifo}-psd.dat",
```

## asimov/pipelines/bilby.py

```diff
@@ -4,18 +4,16 @@
 import os
 import re
 import subprocess
 import configparser
 
 import time
 
-from asimov.utils import set_directory
-
 from .. import config
-from ..pipeline import Pipeline, PipelineException, PipelineLogger, PESummaryPipeline
+from ..pipeline import Pipeline, PipelineException, PipelineLogger
 
 
 class Bilby(Pipeline):
     """
     The Bilby Pipeline.
 
     Parameters
@@ -202,56 +200,54 @@
                 job_label = self.production.name
             dag_filename = f"dag_{job_label}.submit"
             command = [
                 # "ssh", f"{config.get('scheduler', 'server')}",
                 "condor_submit_dag",
                 "-batch-name",
                 f"bilby/{self.production.event.name}/{self.production.name}",
-                os.path.join(self.production.rundir, "submit", dag_filename),
+                os.path.join("submit", dag_filename),
             ]
 
             if dryrun:
                 print(" ".join(command))
             else:
 
                 # with set_directory(self.production.rundir):
                 self.logger.info(f"Working in {os.getcwd()}")
 
                 dagman = subprocess.Popen(
                     command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT
                 )
 
-                with set_directory(self.rundir):
-
-                    dagman = subprocess.Popen(
-                        command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT
-                    )
+                dagman = subprocess.Popen(
+                    command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT
+                )
 
-                    self.logger.info(" ".join(command))
+                self.logger.info(" ".join(command))
 
-                    stdout, stderr = dagman.communicate()
+                stdout, stderr = dagman.communicate()
 
-                    if "submitted to cluster" in str(stdout):
-                        cluster = re.search(
-                            r"submitted to cluster ([\d]+)", str(stdout)
-                        ).groups()[0]
-                        self.logger.info(
-                            f"Submitted successfully. Running with job id {int(cluster)}"
-                        )
-                        self.production.status = "running"
-                        self.production.job_id = int(cluster)
-                        return cluster, PipelineLogger(stdout)
-                    else:
-                        self.logger.error("Could not submit the job to the cluster")
-                        self.logger.info(stdout)
-                        self.logger.error(stderr)
+                if "submitted to cluster" in str(stdout):
+                    cluster = re.search(
+                        r"submitted to cluster ([\d]+)", str(stdout)
+                    ).groups()[0]
+                    self.logger.info(
+                        f"Submitted successfully. Running with job id {int(cluster)}"
+                    )
+                    self.production.status = "running"
+                    self.production.job_id = int(cluster)
+                    return cluster, PipelineLogger(stdout)
+                else:
+                    self.logger.error("Could not submit the job to the cluster")
+                    self.logger.info(stdout)
+                    self.logger.error(stderr)
 
-                        raise PipelineException(
-                            "The DAG file could not be submitted.",
-                        )
+                    raise PipelineException(
+                        "The DAG file could not be submitted.",
+                    )
 
         except FileNotFoundError as error:
             self.logger.exception(error)
             raise PipelineException(
                 "It looks like condor isn't installed on this system.\n"
                 f"""I wanted to run {" ".join(command)}."""
             ) from error
@@ -273,18 +269,19 @@
             rundir = self.production.rundir
         self.logger.info(f"Rundir for samples: {rundir}")
         return glob.glob(
             os.path.join(rundir, "result", "*_merge*_result.hdf5")
         ) + glob.glob(os.path.join(rundir, "result", "*_merge*_result.json"))
 
     def after_completion(self):
-        post_pipeline = PESummaryPipeline(production=self.production)
+        super().after_completion()
+        # post_pipeline = PESummary(production=self.production, subject=self.production.subject)
         self.logger.info("Job has completed. Running PE Summary.")
-        cluster = post_pipeline.submit_dag()
-        self.production.meta["job id"] = int(cluster)
+        # cluster = post_pipeline.submit_dag()
+        # self.production.meta["job id"] = int(cluster)
         self.production.status = "processing"
         self.production.event.update_data()
 
     def collect_logs(self):
         """
         Collect all of the log files which have been produced by this production and
         return their contents as a dictionary.
@@ -351,16 +348,25 @@
         config_parser = configparser.RawConfigParser()
         config_parser.read_string(file_content)
 
         return config_parser
 
     def html(self):
         """Return the HTML representation of this pipeline."""
-        # pages_dir = os.path.join(self.production.event.name, self.production.name)
+        pages_dir = os.path.join(self.production.event.name, self.production.name, "pesummary")
         out = ""
+        if self.production.status in {"uploaded"}:
+            out += """<div class="asimov-pipeline">"""
+            out += (
+                f"""<p><a href="{pages_dir}/home.html">Summary Pages</a></p>"""
+            )
+            out += f"""<img height=200 src="{pages_dir}/plots/{self.production.name}_psd_plot.png"</src>"""
+            out += f"""<img height=200 src="{pages_dir}/plots/{self.production.name}_waveform_time_domain.png"</src>"""
+
+            out += """</div>"""
 
         return out
 
     def resurrect(self):
         """
         Attempt to ressurrect a failed job.
         """
```

## asimov/pipelines/rift.py

```diff
@@ -8,15 +8,15 @@
 
 from ligo.gracedb.rest import HTTPError
 
 from asimov import config, logger
 from asimov.utils import set_directory
 
 from asimov.pipeline import Pipeline, PipelineException, PipelineLogger
-from asimov.pipeline import PESummaryPipeline
+from asimov.pipelines.pesummary import PESummary
 
 
 class Rift(Pipeline):
     """
     The RIFT Pipeline.
 
     Parameters
@@ -58,15 +58,15 @@
             for section_arg in required_args[section]:
                 if section_arg not in section_data:
                     section_data[section_arg] = {}
 
     def after_completion(self):
 
         self.logger.info("Job has completed. Running PE Summary.")
-        post_pipeline = PESummaryPipeline(production=self.production)
+        post_pipeline = PESummary(production=self.production)
         cluster = post_pipeline.submit_dag()
 
         self.production.meta["job id"] = int(cluster)
         self.production.status = "processing"
 
     def before_submit(self):
         pass
@@ -184,15 +184,15 @@
         if self.production.get_meta("user"):
             user = self.production.get_meta("user")
         else:
             user = config.get("condor", "user")
             self.production.set_meta("user", user)
 
         os.environ["LIGO_USER_NAME"] = f"{user}"
-        os.environ["LIGO_ACCOUNTING"] = f"{config.get('pipelines', 'accounting')}"
+        os.environ["LIGO_ACCOUNTING"] = f"{self.production.meta['scheduler']['accounting group']}"
 
         try:
             calibration = config.get("general", "calibration")
         except configparser.NoOptionError:
             calibration = "C01"
 
         try:
```

## Comparing `asimov-0.6.0a2.dist-info/LICENSE` & `asimov-0.6.0a3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `asimov-0.6.0a2.dist-info/METADATA` & `asimov-0.6.0a3.dist-info/METADATA`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: asimov
-Version: 0.6.0a2
+Version: 0.6.0a3
 Summary: A Python package for managing and interacting with data analysis jobs.
 Home-page: https://git.ligo.org/asimov/asimov
 Author: Daniel Williams
 Author-email: daniel.williams@ligo.org
 License: MIT license
 Keywords: pe,ligo,asimov
 Platform: UNKNOWN
```

## Comparing `asimov-0.6.0a2.dist-info/RECORD` & `asimov-0.6.0a3.dist-info/RECORD`

 * *Files 19% similar despite different names*

```diff
@@ -1,52 +1,57 @@
-asimov/__init__.py,sha256=5xYX9iZ42fY31ifmRcuODWo1OPD4aGLKRP5vywFYKjg,2808
-asimov/analysis.py,sha256=cId5XNLXVUQ46RJFEBmsmk6eucfa7XjUwtTQE8NsyGo,27309
-asimov/asimov.conf,sha256=NTs8PEmemE_cZSAEofWXBXNzT0eKSE1c76fHyOkytjI,843
-asimov/condor.py,sha256=7DOCG8eMJ5CdoTcRPH1OSI2_YkIBwHQ1BdGEF5o61eA,11152
+asimov/__init__.py,sha256=5vTrPGGI9Dgj0DA2car7qTH-OlJHZg2eKL1dygE683E,2794
+asimov/analysis.py,sha256=FPrnkjhYeM1Szi3HY86rGCW2Hnn2Rlm5RMDmPSq6ew0,32456
+asimov/asimov.conf,sha256=3bRSX4IbYwf7nygNA5X3YeQVNfW5CdSnRQH5P03xYZw,828
+asimov/condor.py,sha256=CUmpEmC7WIPf5o_U1KLzM7NUF8f29601f1tb2u3uXjE,11115
 asimov/database.py,sha256=H9jgEoS2GLDMXjkK0d8nBY2ds1bwYUIMDbQmFsHst5U,1617
-asimov/event.py,sha256=5M98gbZ5qnWXtmRRy9tOgUKD2XnMJ6ZEdtYQgsieQbI,14634
-asimov/git.py,sha256=CxxUpJzfQApt4d3RiGdA7fSln2kwc1sAUy1Z81UYDW0,12166
+asimov/event.py,sha256=-Iq0X8EcCZxG3kG1yszwWxSlgQ3XS6Z4DuYMVaRXFtg,14506
+asimov/git.py,sha256=lcFDAKG6NTWRs9QDxZdplc73LIBAwOThDJleHesLCGM,12188
 asimov/gitlab.py,sha256=g5Zg4lSgqAadnf0759wkJchDeSPaJuXTru7CB9a6K9w,7226
 asimov/ini.py,sha256=K3mo8257HpvO0ipX3w1ofmgCN9vEMhvVCi_kupkJzAs,5398
-asimov/ledger.py,sha256=vN-LsfLiV1Z2WW5DaVx8rn0TMQGVFW-Aw96AJar7cEQ,9374
+asimov/ledger.py,sha256=eGDiMIh6czga6ny1RWEJTb_rvcOMDJfk9vGvvopBy6s,9868
 asimov/locutus.py,sha256=XfiDYrIWQunbTKBAE8BadeCikVI63O67EeQ1jWH28Pg,2051
 asimov/logging.py,sha256=HCE_giZgeXAFeDv3AaRSJBxPj20vAVmyN3uqcnpUJOM,7051
 asimov/mattermost.py,sha256=Lp-WwwTJor4Do9Gab4L79mBvQzZ2WAonmfZT9JO6RT4,1646
 asimov/olivaw.py,sha256=ihsBqkpjxuvP6E96W2OxBkrmnCuVQyOiS6TCrMdo428,1586
-asimov/pipeline.py,sha256=_IOtfPYvrxycDEYW5YDuJkWlRK8CTlL6SdxkfaNivIY,14096
+asimov/pipeline.py,sha256=GxSPYx7koIDL723_OPWuIvni7QYjWflMFczv_OiScKs,16426
 asimov/review.py,sha256=72Kv9ccK3wMBD_aBDCnX4ozm0BHwTvekz43WNXmeQuQ,4513
 asimov/storage.py,sha256=5IUNyysKFhnhHCj8-J3xX5qDTqnMTsOUKQGmO9_dQsM,10087
 asimov/testing.py,sha256=GHN83ozigC--s0tMH8P_5yaZr-X_elT4-q5sjz0Gu_g,1156
-asimov/utils.py,sha256=gBrWOMNy1hXrSIx4z-3e3t8aBRjzhqbZCItt28D6kK8,2134
+asimov/utils.py,sha256=1F3vvkla1E8XKnpmM1VUdwrYoL329y2ZesPQ2TSBW_U,3050
 asimov/cli/__init__.py,sha256=4fD4aGBcYH0pS-RmtxhijztzZsaXOwfvv8odUJVTxcA,395
-asimov/cli/application.py,sha256=wndJ22V2ay6PBSmwz3MsGyMolShfde7nvg-8wcIXHJo,4948
-asimov/cli/configuration.py,sha256=rXRFzoZaxfTfN9S6y4tT1Bo30cimEey2Lkg0srGeIVk,1141
+asimov/cli/application.py,sha256=YVJT-3L9qyrlvrxIRfHPtZ6OSMjJdFXC9a9pfmyDfsY,7146
+asimov/cli/configuration.py,sha256=LPuWIWX5MdpTEaLo4z6dCTLBsweqGTXqROtiyLjNPK8,1175
 asimov/cli/data.py,sha256=iYGRdyhFs4h7nUMTb1G3wWtqJpxixSNfUQF813pHujE,1285
-asimov/cli/event.py,sha256=6XPVPuUH1BoJmF5zGXYUEH6Ij0W67gWn4Wglp-P39_8,9010
+asimov/cli/event.py,sha256=N0u1U_hCQWOOe8tiPZlUs_JFUQCRKZJ25fRXEcplMYo,9096
 asimov/cli/ledger.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-asimov/cli/manage.py,sha256=6f9N95yxaGzU9IJr2GdqHz5dX3SLdRA3uL8D9LBRCG8,10312
-asimov/cli/monitor.py,sha256=nNBrnPMwkZK8qlwOLBwxbAe5T3C6WIK1AgQi2uP0Vdk,14640
+asimov/cli/manage.py,sha256=nwcPkQFCTMFVITGuk43TJP5rSgvSuxtxkhMlcUwXdo0,10355
+asimov/cli/monitor.py,sha256=8IH9G1wpDdA0gr6kTBRXHK0PYZhlxF7F_5xiOuK_Gow,15477
 asimov/cli/production.py,sha256=8wysaQimcKOhEJFiHdWetk6U6VB37NItUbQ7a-SNccA,5606
-asimov/cli/project.py,sha256=U73sMCOSQ3A0N3mcwEBYMMHfl-lpdt6xSe889Q5-GS8,6016
-asimov/cli/report.py,sha256=Upm1JZFNSnMde-iiMMXYMl-i0GIoUF_dtRN5WtBrL-w,5225
+asimov/cli/project.py,sha256=hJeU5u67wbFCjTDYUhyQmvQJS2-NatDj8-DSYxcWl1c,6089
+asimov/cli/report.py,sha256=yJ0W827EcwJKNu6iPh7hTQWCjpYTcwW1YLzj5nupGnI,5271
 asimov/cli/review.py,sha256=gMT5iGNf4au32mFaoDI70F2yQ3f-we0QaUN3GIlT9zk,2817
 asimov/cli/report-theme/body.html,sha256=_EO2Ce1k6LkoL4t-dhhk7RZ6FPpIlSNO_p0J8EEiycs,96
 asimov/cli/report-theme/footer.html,sha256=DhOISyuw80kVSTy5xy84xIbV-CzLEqYsoZVfwZEiGMc,247
 asimov/cli/report-theme/head.html,sha256=3NZAIpIlraJAx8TsXTfa6VRycLq6JyRiun2TGjdfu64,1467
 asimov/cli/report-theme/header.html,sha256=KRo_10q0HIdXXlM8DYCZqw-JpMf3uFithQRun8qYMJE,841
 asimov/configs/README.rst,sha256=Dyk7Rkf8KZ5hL7dcFOye2EoZjn_dG7RARB7t67u7pk8,220
-asimov/configs/bayeswave.ini,sha256=Lhp7Ud5lLBqiymmNTBRGGAT5bUHaAse7SnMJ-fpgo0M,2903
-asimov/configs/bilby.ini,sha256=j0M2DFysY4tHmG9_uQXRS3LVTOZVEi2vGg2MX9ZhvBo,12328
-asimov/configs/lalinference.ini,sha256=HdvRkaRqYC4Lj9kk0uTvEtV92iWovAD75BIbSE0F0cY,6238
-asimov/configs/rift.ini,sha256=Lkpwgslv_fTK6t-snjYVo1eDkl55a8Ht_gFPHVlgtL8,7569
-asimov/pipelines/__init__.py,sha256=n3EJ7BRwjio1hqdLiDA4g9eAV5k9O_YZXiPjKshTtZs,610
-asimov/pipelines/bayeswave.py,sha256=nUeI8TIaLryd_a2bx4KIOUXATgd1HE4cgRiOMwtpE6E,19216
-asimov/pipelines/bilby.py,sha256=zhvS2LEOR7Fhveg3jkPx8Pl6HiT5eNdf-293haPK-PU,12809
+asimov/configs/bayeswave.ini,sha256=zfyn_fEwEttyQKuCdPX1-6ZpwRH8B_IDUopHVgnqhNU,3004
+asimov/configs/bilby.ini,sha256=pZLVFmaFH60idxhsVdhISKAIQ5tQmaseMQrOpOF2hgw,14237
+asimov/configs/lalinference.ini,sha256=zjejoT2Hw-up8oAG6NQtdibQjY4TdpMV71bwe8jNBMs,6291
+asimov/configs/rift.ini,sha256=ho0rl1-MvkA8CKEn-a43LaxJa_Kp9c1ZyZh5d2iG6p0,7586
+asimov/pipelines/__init__.py,sha256=uDIM-nEUDgF_H8dpGW7Y-QqgKQWoFURIuzE0GZpU2OU,772
+asimov/pipelines/bayeswave.py,sha256=d9ivSs06-jJVy7gvBD2wpiSYJbDJzPCGL8xcsEDx3DQ,19726
+asimov/pipelines/bilby.py,sha256=JYTG88cPwmAmdlkIX20mW2Li7XdDKchA7Tb4cucZZyM,13147
 asimov/pipelines/lalinference.py,sha256=bi3k9fcj4K5Vr2HiNV6xS7RSHuTtJ6FJW9cd1G2eyzA,9489
-asimov/pipelines/rift.py,sha256=HwEijPYMldY9lEL4PHgevZkUgK0HhGc8kNahRRFgnX8,17635
-asimov-0.6.0a2.dist-info/LICENSE,sha256=3OWyNSDNgd5tDbORK72QyJEUBMBVwVUjtcZppOdfrFM,755
-asimov-0.6.0a2.dist-info/METADATA,sha256=ARtGmwLBJ35G6rAICAKfX7PWAUkzQ6PEsoMSEFT5gtw,1392
-asimov-0.6.0a2.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
-asimov-0.6.0a2.dist-info/entry_points.txt,sha256=jbTPZwBFpPW3VKNLnjeKb327DPTChpjY4B1IPC3ZDEc,108
-asimov-0.6.0a2.dist-info/top_level.txt,sha256=_R0fwGHM71D6a2YHrKUPX0Z21P8tfW6qn96Xl0MIVHY,7
-asimov-0.6.0a2.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-asimov-0.6.0a2.dist-info/RECORD,,
+asimov/pipelines/pesummary.py,sha256=fITdkHl4V_uvimQedQizfvbxlFPSDl3Rw4ww-A0GhFQ,10853
+asimov/pipelines/rift.py,sha256=Gif-2lt_gsNsP3GcKKH8v_T_Hb3Y8ImT8pwRfekvy5M,17646
+asimov/report-theme/body.html,sha256=_EO2Ce1k6LkoL4t-dhhk7RZ6FPpIlSNO_p0J8EEiycs,96
+asimov/report-theme/footer.html,sha256=DhOISyuw80kVSTy5xy84xIbV-CzLEqYsoZVfwZEiGMc,247
+asimov/report-theme/head.html,sha256=3NZAIpIlraJAx8TsXTfa6VRycLq6JyRiun2TGjdfu64,1467
+asimov/report-theme/header.html,sha256=KRo_10q0HIdXXlM8DYCZqw-JpMf3uFithQRun8qYMJE,841
+asimov-0.6.0a3.dist-info/LICENSE,sha256=3OWyNSDNgd5tDbORK72QyJEUBMBVwVUjtcZppOdfrFM,755
+asimov-0.6.0a3.dist-info/METADATA,sha256=q8RpEEy3xa7OHfzli5dSmhHRoC-0cM6ScNpvlwjCZA8,1392
+asimov-0.6.0a3.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
+asimov-0.6.0a3.dist-info/entry_points.txt,sha256=jbTPZwBFpPW3VKNLnjeKb327DPTChpjY4B1IPC3ZDEc,108
+asimov-0.6.0a3.dist-info/top_level.txt,sha256=_R0fwGHM71D6a2YHrKUPX0Z21P8tfW6qn96Xl0MIVHY,7
+asimov-0.6.0a3.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+asimov-0.6.0a3.dist-info/RECORD,,
```

